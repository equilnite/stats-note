[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"notes based learned class.various functions website make notes accessible case need support reading material. Take look tool bar top try right clicking math equations see options enlargen math equations. Ask Mr. Chang want explore .notes incomplete, ’ll trying update completely AP test. Give feedback might want see notes.Look rest notes navigation bar left hand side (might click hamburger icon), go sequentially clicking arrows left right hand side.","code":""},{"path":"basic-notation.html","id":"basic-notation","chapter":"2 Basic notation","heading":"2 Basic notation","text":"","code":""},{"path":"basic-notation.html","id":"lists","chapter":"2 Basic notation","heading":"2.1 Lists","text":"time, see \\(x\\) denote list values (.e. \nvariable data table).example, \\(x\\) list numerical values \\(\\) \\(g\\), can\nwrite :\\(x = [, b, c, d, e, f, g]\\), \\(x_i\\) means \\(^{th}\\) value list \\(x\\), \\(x_1 = \\) \\(x_2 = b\\) \\(x_7 = g\\).","code":""},{"path":"basic-notation.html","id":"n","chapter":"2 Basic notation","heading":"2.2 \\(n\\)","text":"regards data table list values, \\(n\\) stands number\nrows data points data table list (learn\nsample size later ), list \\(x\\), \\(n = 7\\).Adding , \\(x_n\\) mean last value list \\(x\\) (since \n\\(n\\) values \\(x\\))","code":""},{"path":"basic-notation.html","id":"summation-sigma","chapter":"2 Basic notation","heading":"2.3 Summation (\\(\\Sigma\\))","text":"also see greek letter \\(\\Sigma\\) formulas. Usage \nsign means using summation notation.want sum numbers 1 7, write ,\\[\\sum_{=1}^7 \\]interpret ,Start \\(= 1\\), evaluate expression, \\(\\).Start \\(= 1\\), evaluate expression, \\(\\).Keep evaluated expression side get ready add \nvalues , soKeep evaluated expression side get ready add \nvalues , \\[1 + \\cdots\\]Now go next numbers get \\(7\\) (number top\n\\(\\Sigma\\)) moving onto \\(= 2\\), end \n\\[1 + 2 + \\cdots\\] \\(= 3\\), end \n\\[1 + 2 + 3 + \\cdots\\] Now go next numbers get \\(7\\) (number top\n\\(\\Sigma\\)) moving onto \\(= 2\\), end \n\\[1 + 2 + \\cdots\\] \\(= 3\\), end \n\\[1 + 2 + 3 + \\cdots\\] get end (reach \\(= 7\\)), \nexpanded form summation. \\[1 + 2 + 3+ 4 + 5 + 6 + 7\\]get end (reach \\(= 7\\)), \nexpanded form summation. \\[1 + 2 + 3+ 4 + 5 + 6 + 7\\]","code":""},{"path":"basic-notation.html","id":"other-notation","chapter":"2 Basic notation","heading":"2.4 Other notation","text":"\\(\\bar x\\): “line” top \\(x\\) means mean \\(x\\). \n\\(\\bar \\), asking mean \\(\\).\nPronounced “x bar”\nPronounced “x bar”\\(\\hat p\\): “hat” top \\(p\\) means estimate \\(p\\). \n\\(\\hat x\\), asking estimate \\(x\\).\nPronounced “p hat”\nPronounced “p hat”","code":""},{"path":"basic-notation.html","id":"other-commonly-used-symbols","chapter":"2 Basic notation","heading":"2.5 Other commonly used symbols","text":"\\(p\\): proportion, probability, p-value\\(p\\): proportion, probability, p-value\\(\\bar x\\): sample mean\\(\\bar x\\): sample mean\\(s_x\\): sample standard deviation (x), \\(s_y\\) sample\nstandard deviation \\(y\\)\\(s_x\\): sample standard deviation (x), \\(s_y\\) sample\nstandard deviation \\(y\\)\\(\\mu\\): population mean (true mean)\\(\\mu\\): population mean (true mean)\\(\\sigma\\): population standard deviation (true standard deviation)\\(\\sigma\\): population standard deviation (true standard deviation)\\(N\\): population size\\(N\\): population size","code":""},{"path":"categorical-data-visualizations.html","id":"categorical-data-visualizations","chapter":"3 Categorical Data Visualizations","heading":"3 Categorical Data Visualizations","text":"","code":""},{"path":"categorical-data-visualizations.html","id":"bar-plots","chapter":"3 Categorical Data Visualizations","heading":"3.1 Bar plots","text":"Represent number proportion unique value. numbers\nproportions represented rectangular bars heights\nproportional values represent. can plot \nvertically horizontally (.e. categories x-axis categories\ny-axis)Following data table:Count number values per category (make frequency table).\nNote: table missing totalPlot frequencies height barsIf needed (need proportions y-axis instead, calculate \nrelative frequency table frequency table first). Note: ,\none missing total","code":""},{"path":"categorical-data-visualizations.html","id":"stacked-bar-plots-and-side-by-side-bar-graphs","chapter":"3 Categorical Data Visualizations","heading":"3.2 Stacked Bar Plots and Side-by-Side Bar Graphs","text":"Stacked bar plots show two categorical variables, one \nx-axis/y-axis, legend (colours). call \nvariable x-axis “groups” variable legend \n“categories.”constructing bar plots, first want determine \nvariable goes (choice given choice ). \ncalculate relative frequencies per groupFor example, two-way table detailing hair eye\ncolour statistics studentsSo want eye colour groups, calculate relative\nfrequencies column (use total column divide whole\ncolumn ), group/column add 1.numbers bar heights. bar(s) representing\nbrown eyes:black hair .3091black hair .3091brown hair .5409brown hair .5409red hair 0.1182red hair 0.1182blond hair 0.0318blond hair 0.0318Here’s corresponding side--side bar plot. Note heights \nbars segmented bar graph.hand, want eye colour groups, \ncalculate relative frequencies row (use total row \ndivide whole row ), group/row add 1.numbers bar heights. bar(s) representing\nblack hair:brown eyes 0.6296296brown eyes 0.6296296blue eyes 0.1851852blue eyes 0.1851852hazel eyes 0.1388889hazel eyes 0.1388889green eyes 0.0462963green eyes 0.0462963","code":""},{"path":"categorical-data-visualizations.html","id":"mosaic-plots","chapter":"3 Categorical Data Visualizations","heading":"3.3 Mosaic Plots","text":"Mosaic plots almost stacked bar plots. \ndifference widths bars change according \nproportion points group. mosaic plot, x-axis \nalso measure proportion observations/data points within \ngroupings (.e. x-axis reflects marginal distribution \nvariable x-axis).Following \nsteps \nside--side stacked bar charts find heights, now add \nadditional step plotting.Find widths bars finding marginal distriubtion \nvariable x-axis (groups)group, find probability trait. \nprevious example, table:Using eye colours groups (vertical bars), find:\\(P(Brown) \\approx .3716\\)\\(P(Brown) \\approx .3716\\)\\(P(Blue) \\approx .3632\\)\\(P(Blue) \\approx .3632\\)\\(P(Hazel) \\approx .1571\\)\\(P(Hazel) \\approx .1571\\)\\(P(Green) \\approx .1081\\)\\(P(Green) \\approx .1081\\)plot mosaic plot, thing, except now, \nbars differ widths according numbers just\ncalculated.","code":""},{"path":"quantative-data-visualizations.html","id":"quantative-data-visualizations","chapter":"4 Quantative Data Visualizations","heading":"4 Quantative Data Visualizations","text":"","code":""},{"path":"quantative-data-visualizations.html","id":"dot-plots","chapter":"4 Quantative Data Visualizations","heading":"4.1 Dot Plots","text":"Dot plots discrete quantitative variables , \nuseful situations small range number \ncan actually see data distribution varies across values.Dot plots simple, draw number line plot points \nnumber number see data.Take data example:Now count value figure many dots need \nvalue number line plot graph","code":"## Bin width defaults to 1/30 of the range of the data. Pick better value with `binwidth`."},{"path":"quantative-data-visualizations.html","id":"stemplots","chapter":"4 Quantative Data Visualizations","heading":"4.2 Stemplots","text":"Using data example,stem plot looks like :stem plot, need determine common “stem” numbers\n’re plotting. integer numbers 10 200,\nstems everything tens , ’ll \nstems 1-20. take stems, just write “leaves”\nnext stem belong.Note also add key show stem + leaf means.\nstem leaves give information decimals data, \nsee , need give example like (shown \nexample stemplot):Key: 1|2 = 12Here’s another example (sorted convenience)","code":"## 1 | 2: represents 12\n##  leaf unit: 1\n##             n: 20\n##     0 | 1\n##     1 | 8\n##     2 | \n##     3 | 36689\n##     4 | 4\n##     5 | 2339\n##     6 | 0228\n##     7 | \n##     8 | 19\n##     9 | 35## 1 | 2: represents 1.2\n##  leaf unit: 0.1\n##             n: 50\n##    3 | 3\n##    3 | 579999\n##    4 | 00334\n##    4 | 556677788899\n##    5 | 00112233444\n##    5 | 667888999\n##    6 | 234\n##    6 | 5\n##    7 | 12"},{"path":"quantative-data-visualizations.html","id":"boxplots","chapter":"4 Quantative Data Visualizations","heading":"4.3 Boxplots","text":"Also known box--whisker plotBoxplots primarily made five number summary data.\nfive number summary made :Minimum (min)Minimum (min)First Quartile (\\(Q_1\\))First Quartile (\\(Q_1\\))MedianMedianThird Quartile (\\(Q_3\\))Third Quartile (\\(Q_3\\))Maximum (max)Maximum (max)make simple boxplot, use first quartile, median, third\nquartile make “box” use minimum maximum make\n“whiskers.”simple list numbers:five number summary :detailed , box plot looks like:last detail can calculate outliers using 1.5 IQR rule\nshow boxplot. either direction (left right), \nsee outliers direction, extend whisker \nsmallest /largest point outlier. plot \noutliers individual points.Look example data:Five number summary:numbers calculated 1.5 IQR rule :12 outlier. means draw right whisker 6 \nplot 12 individually number line. Like :","code":"##    Min. 1st Qu.  Median 3rd Qu.    Max. \n##       1       2       4       6       7##    Min. 1st Qu.  Median 3rd Qu.    Max. \n##     -12      -5      -3       0      12## [1] -12.5   7.5"},{"path":"quantative-data-visualizations.html","id":"histograms","chapter":"4 Quantative Data Visualizations","heading":"4.4 Histograms","text":"histogram similar bar plot, except \nhistograms made quantitative data bars continuous \nsense gap bars. make histogram, select\nappropriate equal intervals make don’t \nmany bars don’t bars. goal \nhistograms, many visualizations, able see \nshape characteristics distribution question. \nmany bars bars, won’t able see much important\ninformation (especially think situations many data\npoints precise decimal measurements).Decide intervals (e.g. 5’s, 10’s, 100’s)Decide intervals (e.g. 5’s, 10’s, 100’s)Within intervals, count number observations \nbelong “bin”. , count observations \ncount left end inclusive right end inclusive. \nintervals 5, something like counting \npoints \\(0 \\leq x < 5\\), \\(5 \\leq x < 10\\), .Within intervals, count number observations \nbelong “bin”. , count observations \ncount left end inclusive right end inclusive. \nintervals 5, something like counting \npoints \\(0 \\leq x < 5\\), \\(5 \\leq x < 10\\), .Plot bars.Plot bars.Example:Consider example data set:data set summary statistics:knowledge, let’s make 7 “bins”, let’s every\n0.2, starting 1.5 2.9. something build \nintuition.Now, count values:Now, just put together. bin, bar bars’\nheights correspond number individuals bin., just like bar graphs, can instead relative frequencies\n(’ll see time!!!)histogram like , keep mind bars always\nadd 1 (100%).","code":"##        x        \n##  Min.   :1.522  \n##  1st Qu.:1.912  \n##  Median :2.022  \n##  Mean   :2.110  \n##  3rd Qu.:2.224  \n##  Max.   :2.704## [1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n##         1         4         6         5         1         2         1## [1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n##      0.05      0.20      0.30      0.25      0.05      0.10      0.05"},{"path":"sampling.html","id":"sampling","chapter":"5 Sampling","heading":"5 Sampling","text":"question certain population (entire group \nindividuals). Ideally ask (take census). \ncontacting every member population often isn’t practical: \ntake much time cost much money. Instead, put \nquestion sample, subset individuals population\nactually collect data, chosen represent entire\npopulation.want identify population, ask , \nquestion want know ? group people \nquestion/problem address?identifying sample, ask , group work done\nactually address?","code":""},{"path":"sampling.html","id":"bias","chapter":"5 Sampling","heading":"5.1 Bias","text":"collect data, possibility data becoming\nsystematically pushed towards specific outcome. example, \nwant learn GPA average school take sample \nstudents class, ’s quite possible sample \nrepresentative school. probably result GPA average\nhigher lower actual GPA average school. \nseveral ways can happen. main way learn :","code":""},{"path":"sampling.html","id":"response-bias","chapter":"5 Sampling","heading":"5.1.1 Response Bias","text":"Response bias created something causes people’s responses systematically pushed one direction.","code":""},{"path":"sampling.html","id":"non-response-bias","chapter":"5 Sampling","heading":"5.1.2 Non-response Bias","text":"Nonresponse occurs individual chosen sample can’t contacted refuses participate","code":""},{"path":"sampling.html","id":"undercoverage","chapter":"5 Sampling","heading":"5.1.3 Undercoverage","text":"Undercoverage occurs members population chosen sample.","code":""},{"path":"sampling.html","id":"voluntary-response-sample","chapter":"5 Sampling","heading":"5.1.4 Voluntary Response Sample","text":"Voluntary response samples consists people choose responding general invitation, causing chosen sample representative population","code":""},{"path":"sampling.html","id":"convenience-sample","chapter":"5 Sampling","heading":"5.1.5 Convenience Sample","text":"Choosing individuals population easy reach results convenience sample.","code":""},{"path":"sampling.html","id":"sampling-methods","chapter":"5 Sampling","heading":"5.2 Sampling Methods","text":"","code":""},{"path":"sampling.html","id":"simple-random-sample-srs","chapter":"5 Sampling","heading":"5.2.1 Simple Random Sample (SRS)","text":"SRS, individual (words) subgroup individuals chance chosen population words song.","code":""},{"path":"sampling.html","id":"stratified-random-sample","chapter":"5 Sampling","heading":"5.2.2 Stratified Random Sample","text":"stratified random sample, first determine strata within population. can think strata subgroups people divide based type/status things sampling. goal random sample take SRS strata (normally, want amount people stratum).","code":""},{"path":"sampling.html","id":"random-cluster-sample","chapter":"5 Sampling","heading":"5.2.3 Random Cluster Sample","text":"random cluster sample, first determine clusters within population. can think clusters subgroups people divide based location things sampling (way distinguish clusters besides ). goal random sample take select random clusters sample people chosen clusters.","code":""},{"path":"sampling.html","id":"systematic-random-sample","chapter":"5 Sampling","heading":"5.2.4 Systematic Random Sample","text":"systematic random sample, randomly choose interval (n) /starting point select individuals select every nth individual.","code":""},{"path":"experimental-design.html","id":"experimental-design","chapter":"6 Experimental Design","heading":"6 Experimental Design","text":"sample survey aims gather information population without disturbing population process. Sample surveys one kind observational study, observes individuals measures variables interest attempt influence responses. Unfortunately, means many variables controlled establish causation explanatory response variables. response variable measures outcome study. explanatory variable may help explain predict changes response variable. lack control, certain variables explanatory variable influencing response instead. referred confounding variables.experiment deliberately imposes treatment (specific condition applied) individuals (experimental units subjects experimental units human) measure responses. goal understand cause effect, experiments source fully convincing data. reason, distinction observational study experiment one important statistics.","code":""},{"path":"experimental-design.html","id":"experiment-principles","chapter":"6 Experimental Design","heading":"6.1 Experiment Principles","text":"general, quality experiments (internal validity) can judged based degree four things: comparison, randomization, control, replication. Stronger internal validity gives us better cause-effect link experiment. Whenever describing evaluating design experiment, need sure discuss four !Comparison\nUse design compares two treatments.\nRandomization\nUse chance assign experimental units treatments. helps create roughly equivalent groups experimental units balancing effects variables among treatment groups.\nControl\nKeep variables might affect response groups.\nReplication\nUse enough experimental units group difference effects treatments can distinguished chance differences groups.\nlogic randomized comparative experiment depends ability treat subjects every way except actual treatments compared. Good experiments, therefore, require careful attention details ensure subjects really treated identically.response dummy treatment called placebo effect. Subjects given placebo treatment control placebo effect.Whenever possible, experiments human subjects double-blind, neither subjects interact measure response variable know treatment subject received.","code":""},{"path":"experimental-design.html","id":"experiment-designs","chapter":"6 Experimental Design","heading":"6.2 Experiment Designs","text":"","code":""},{"path":"experimental-design.html","id":"completely-randomized-design","chapter":"6 Experimental Design","heading":"6.2.1 Completely Randomized Design","text":"completely randomized design, experimental units assigned treatments completely chance. similar () simple random sample (SRS), cases ignore variables.\n’s difference: SRS, ’re picking people (sample) study, ignoring rest. completely randomized experiment, however, already sample (people experiment), ’re randomly deciding ’re going study person (, treatment ’re going get). complete randomization, randomization assignment, selection, people study.","code":""},{"path":"experimental-design.html","id":"randomized-block-design","chapter":"6 Experimental Design","heading":"6.2.2 Randomized Block Design","text":"randomized block design, experimental units first assigned blocks according different types/status experimental units experiment. similar stratified random sampling, however, taking sample. reference stratified random sampling wrong describing experiment design.experimental unit assigned block, experiment carried block, completely randomized design carried within block.Afterwards, compare analyze results block finally combine results analyze differences blocks.","code":""},{"path":"experimental-design.html","id":"matched-pairs-design","chapter":"6 Experimental Design","heading":"6.2.3 Matched Pairs Design","text":"matched pairs design special case randomized block design uses blocks size 2. kind design, “matched pairs.” words, need two extremely similar individuals make block. cases, single person block person recieves treatments randomized order (similar person ?).","code":""},{"path":"experimental-design.html","id":"statistical-significance","chapter":"6 Experimental Design","heading":"6.3 Statistical Significance","text":"observed effect large rarely occur chance said statistically significant.","code":""},{"path":"experimental-design.html","id":"scope-of-inference","chapter":"6 Experimental Design","heading":"6.4 Scope of Inference","text":"scope inference refers type inferences (conclusions) can drawn study. types inferences can make (inferences population inferences cause--effect) determined two factors design study.","code":""},{"path":"probability.html","id":"probability","chapter":"7 Probability","heading":"7 Probability","text":"typically know probability chance. chance represent?Probability number 0 1 describes proportion times outcome occur long series repetitions. observe definition probability law large numbers says observe repetitions chance process, proportion times specific outcome occurs approaches single value.random processes, know randomness predictable long run. intuition tries tell us random phenomena also predictable short run. However, probability allow us make short-run predictions. Even can tell things long run, probability tell us certainties future outcomes. , past outcomes influence likelihood individual outcomes occurring future.know certain process supposed act understanding probability, can simulate . simulation imitation chance behavior, based model accurately reflects situation. means described, rather choose way represent situation using box tickets, coin flip, random number generator, etc.describe simulation, want describe following:Describe one simulation situation, using physical medium /random number generator. e.g. flip coin, choose \\(n\\) unique random numbers \\(\\) \\(b\\), choose slips paper box.Describe one simulation situation, using physical medium /random number generator. e.g. flip coin, choose \\(n\\) unique random numbers \\(\\) \\(b\\), choose slips paper box.described procedure many many times.described procedure many many times.Estimate probability question.Estimate probability question.","code":""},{"path":"probability.html","id":"probability-rules","chapter":"7 Probability","heading":"7.1 Probability Rules","text":"sample space \\(\\Omega\\) chance process set possible outcomes.probability model description chance process consists two parts: sample space \\(\\Omega\\) probability outcome.event collection outcomes chance process. , event subset sample space. Events usually designated capital letters, like \\(\\), \\(B\\), \\(C\\), .valid probability model always satisfy following rules:event , \\(0 \\leq P() \\leq 1\\)event , \\(0 \\leq P() \\leq 1\\)\\(\\Omega\\) sample space probability model, \\(P(\\Omega)=1\\)\\(\\Omega\\) sample space probability model, \\(P(\\Omega)=1\\)case equally likely outcomes, \\(P()=\\frac{\\text{number outcomes corresponding event }}{\\text{total number outcomes sample space}}\\)case equally likely outcomes, \\(P()=\\frac{\\text{number outcomes corresponding event }}{\\text{total number outcomes sample space}}\\)also following rules calculate things:\\(P(|B)\\) : probability event \\(\\) occurring, given event \\(B\\) already occurred. (conditional probability)Independent Events: Two events \\(\\) \\(B\\) independent occurrence one effect likelihood occurring (like successive coin flips).independent events,\n\\[P()=P(|B) \\text{ } P(B)=P(B|)\\]Mutually Exclusive (Disjoint) Events: Events \\(\\) \\(B\\) mutually exclusive ’s impossible occur. one flip coin, heads tails mutually exclusive events.mutually exclusive events,\\[P(\\cap B)=0\\]","code":""},{"path":"probability.html","id":"complement-rule","chapter":"7 Probability","heading":"7.1.1 Complement Rule","text":"complement event \\(\\) event \\(\\) doesn’t happen. event sometimes written \\(AC\\). one flip coin, heads tails complementary events (assuming coin won’t land side).complementary events,\\[P()+P(^c)=1 \\text{ } P(^c)=1-P()\\]","code":""},{"path":"probability.html","id":"the-addition-rule","chapter":"7 Probability","heading":"7.1.2 The Addition Rule","text":"\\[P(\\cup B)=P() + P(B)-P(\\cap B)\\]mutually exclusive events, know \\(P(\\cap B)=0\\), can also following simplified rule:\\[P(\\cup B)=P() + P(B)\\]","code":""},{"path":"probability.html","id":"the-multiplication-rule","chapter":"7 Probability","heading":"7.1.3 The Multiplication Rule","text":"\\[P(\\cap B)=P()\\cdot P(B|)\\]independent events \\(\\) \\(B\\), know \\(P(B|) = P(B)\\), can also simplified rule:\\[P(\\cap B)=P()\\cdot P(B)\\]","code":""},{"path":"density-curves.html","id":"density-curves","chapter":"8 Density Curves","heading":"8 Density Curves","text":"density curve curve always horizontal axis, area exactly 1 underneath .density curve describes overall pattern distribution. area curve interval values horizontal axis proportion observations fall interval.data take many, many different values, show distinct shape, can approximate distribution smooth curve called density curve. curves aren’t always bell-shaped.smooth curve (straight line) shows distribution, can use area curve estimate proportion distribution lies given interval (total area curve represents 100%).’ll focus density curves either straight lines, symmetric, skewed. can look density curve estimate mean median?-Remember right skewed data, \\(mean > median\\)-Remember left skewed data, \\(mean < median\\)-Remember symmetric data, \\(mean \\cong median\\)","code":""},{"path":"density-curves.html","id":"percentiles","chapter":"8 Density Curves","heading":"8.1 Percentiles","text":"\\(p^{th}\\) percentile represented given value. \\(p\\)% values less equal given value.","code":""},{"path":"density-curves.html","id":"cumulative-relative-frequency-graphs","chapter":"8 Density Curves","heading":"8.2 Cumulative Relative Frequency Graphs","text":"Cumulative Relative Frequency graphs represent cumulative relative frequency values increase (percentage values less equal x-value). always range 0 1 y-axis (0% 100%).","code":""},{"path":"density-curves.html","id":"normal-distributions","chapter":"8 Density Curves","heading":"8.3 Normal Distributions","text":"early 19th Century, mathematicians Gauss Adrian noticed many, many natural processes (like dimensions weights plants animals) many human processes (manufacturing, scores tests) follow (approximately, exactly) nice bell-shaped curve Abraham de Moivre, 18th century statistician consultant gamblers, developed. bell-shaped curve now called normal curve, equation\\[P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\]\\(P(x)\\) represents height curve, \\(\\mu\\) mean (\\(x\\)’s), \\(\\sigma\\) standard deviation. mean 0 standard deviation 1, graph looks like :Notice inflection points (curve changes concavity, like cubic function) exactly one standard deviation mean, time graph gets 3 standard deviationss direction, height curve basically zero.called Standard Normal Distribution, \\(N(0,1)\\). Remember use approximate real life, always need check approximation appropriate given situation.","code":""},{"path":"density-curves.html","id":"normal-characteristics","chapter":"8 Density Curves","heading":"8.3.1 Characteristics of the Normal Distribution","text":"Normal distributions following characteristics:always unimodal, bell-shaped, symmetric meanThey always unimodal, bell-shaped, symmetric meanThe x-axis represents different values variable, probability single value x zero (true real-life situations, remember approximation).x-axis represents different values variable, probability single value x zero (true real-life situations, remember approximation).probability range x-values shown graph area curve range. shaded area Standard Normal graph represents probability \\(x\\) least 1.5 standard deviations mean (10%).probability range x-values shown graph area curve range. shaded area Standard Normal graph represents probability \\(x\\) least 1.5 standard deviations mean (10%).total area curve 1, 100%total area curve 1, 100%graph can narrow broad, depending standard deviation, never skewed.graph can narrow broad, depending standard deviation, never skewed.","code":""},{"path":"density-curves.html","id":"empirical-rule","chapter":"8 Density Curves","heading":"8.3.2 Empirical Rule","text":"Empirical Rule (68-95-99.7 Rule) tells us :68% distribution within 1 standard deviation meanabout 68% distribution within 1 standard deviation meanabout 95% distribution within 2 standard deviation meanabout 95% distribution within 2 standard deviation meanabout 99.7% distribution within 3 standard deviation meanabout 99.7% distribution within 3 standard deviation mean","code":""},{"path":"density-curves.html","id":"assessing-for-normality","chapter":"8 Density Curves","heading":"8.3.3 Assessing for Normality","text":"assessing normality, think makes normal distribution…. normal distribution. Refer characteristics normal distribution remind need look .Thus check normality:Graph data using dot plot, histogram, boxplot, stem plot. Whichever one convenient show distribution data best.Graph data using dot plot, histogram, boxplot, stem plot. Whichever one convenient show distribution data best.Compare mean median distribution. Normal curves symmetric, data also symmetric. mean median almost ?Compare mean median distribution. Normal curves symmetric, data also symmetric. mean median almost ?Check see data follows empirical rule. 68% data -1 1 standard deviations? 95% 99.7%?Check see data follows empirical rule. 68% data -1 1 standard deviations? 95% 99.7%?Look normal probability plot given. (Know interpret read, make). plot fairly linear (straight line fit points), evidence distribution approximately normalLook normal probability plot given. (Know interpret read, make). plot fairly linear (straight line fit points), evidence distribution approximately normal","code":""},{"path":"density-curves.html","id":"z-score","chapter":"8 Density Curves","heading":"8.3.4 Standardized Score","text":"\\(x\\) observation known mean standard deviation, standardized score \\(x\\) :\n\\[z = \\frac{value - mean}{standard~deviation}\\]\nstandardized score, often called z-score, observation number standard deviations mean.","code":""},{"path":"density-curves.html","id":"calculating-probabilities-and-percentiles","chapter":"8 Density Curves","heading":"8.3.5 Calculating Probabilities and Percentiles","text":"Using calculus, calculate area (therefore probability) section normal distribution. don’t want use calculus every time want calculate normal probability. , use table (inside front cover book also Table ) many Standard Normal probabilities calculated us. need convert problem Standard Normal, can use table. make conversion using z-scores. z-score, can use Standard Normal table.","code":""},{"path":"density-curves.html","id":"using-your-calculator","chapter":"8 Density Curves","heading":"8.3.6 Using your Calculator","text":"Remember access calculator functions related distributions TI-83/84 series, go :two functions interest chapter :using calculator calculate Normal probabilities need make sure write least following information ensure full credit:name distributionThe name distributionThe parameters distribution (normal distributions, mean s.d.)parameters distribution (normal distributions, mean s.d.)calculate test statistic (normal distributions, z-score)calculate test statistic (normal distributions, z-score)probability findingThe probability findingThe answer context problemThe answer context problem","code":"2nd -> vars (distr)normalcdf(lowerbound, upperbound [, mu, sigma])\n\ninvNorm(area to the left of value [, mu, sigma])"},{"path":"density-curves.html","id":"example","chapter":"8 Density Curves","heading":"8.3.6.1 Example","text":"Suppose time need spend BART get Downtown Berkeley SF Airport approximately normally distributed \\(\\mu = 54\\) minutes, \\(\\sigma = 4.6\\) minutes. words, BART times Berkeley SFO \\(\\sim N(54, 4.6)\\).Now find probability (\\(X\\) represents amount time required single randomly selected BART trip Berkeley SFO):\\(P(X<53)\\)\\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X<53) &\\approx P(z < -.22) \\underset{\\text{Table }}{=} 0.4129\n\\end{aligned}\n\\]Using calculator:\\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X<53) &\\approx P(z < -0.22) \\\\\n&= \\texttt{normalcdf(-1000, -0.22)} \\approx 0.4129\n\\end{aligned}\n\\]\\(P(X>60)\\)\\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X>60) &\\approx P(z > 1.30) \\\\\n&= 1 - P(z < 1.30) \\underset{\\text{Table }}{=} 1 - 0.9032  = 0.0968\n\\end{aligned}\n\\]Using calculator:\\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X>60) &\\approx P(z > 1.30) \\\\\n&= \\texttt{normalcdf(1.30, 1000)} \\approx 0.0968\n\\end{aligned}\n\\]lowest 5% travel times many minutes?\\[\n\\begin{aligned}\n0.05 & \\underset{\\text{Table }}{\\approx} P(z<-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\]Using calculator:\\[\n\\begin{aligned}\n\\texttt{invNorm(0.05)}& \\approx P(z<-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\]","code":""},{"path":"random-variables.html","id":"random-variables","chapter":"9 Random Variables","heading":"9 Random Variables","text":"random variable variable whose value numerical outcome\nrandom phenomenon.Random variables can discrete continuous. discrete random\nvariable \\(X\\) countable, smaller, number possible outcomes,\ncontinuous random variable \\(X\\) can take infinite\nnumber (theoretically) different values.probability distribution random variable \\(X\\) gives us \npossible values \\(X\\), corresponding probabilities.\nProbability distributions typically given tables, histograms\n(probability y-axis, instead frequency), density\ncurves (like Normal curve).","code":""},{"path":"random-variables.html","id":"discrete-random-variables","chapter":"9 Random Variables","heading":"9.1 Discrete Random Variables","text":"discrete random variable describes process specific,\npredefined outcomes. example, can finding probability \npeople blue, brown, black eyes. can finding \nprobability people earning salaries ranges <$30,000,\n$30,000 - $50,000, $50,000 - $ 70,000, $70,000 - $ 100,000,\n>$100,000.discrete random variable \\(X\\) whose probability\ndistribution \\[\n\\begin{aligned}\n    \\textbf{Value:}& ~~~~ x_1 ~~~~ x_2 ~~~~ x_3 ~~~~ \\cdots \\\\\n    \\textbf{Probability:}& ~~~~ p_1 ~~~~ p_2 ~~~~ p_3 ~~~~\\cdots\n\\end{aligned}\n\\]know following mean standard deviation \\(X\\):\\[\\mu_X = E(X) = \\sum x_i P(x_i)\\]\\[\\sigma_X = \\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\]\\[\n\\begin{aligned}\n\\sigma_X &= \\sqrt{\\frac{\\sum(x_i - \\mu_X)^2}{n}} \\\\\n&=\\sqrt{E((X - \\bar x)^2)} \\\\\n&=\\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\\\\n\\end{aligned}\n\\]words, remember define standard deviation root\nmean square squared differences mean. Since \ntaking mean squared differences mean \nformulas (use definition mean step 1 2 \ndefinition mean discrete variables step 2 3), two\nformulas equivalent.variance random variable \\(X\\) :\\[\nVar(X) = \\sigma_X^2\n\\]","code":""},{"path":"random-variables.html","id":"binomial-random-variables","chapter":"9 Random Variables","heading":"9.1.1 Binomial Random Variables","text":"Binomial random variables parameters \\(n\\) \\(p\\), can \nwritten \\(B(n, p)\\). Remember, Normal random variables parameters \ncan written \\(N(\\mu,\\sigma)\\).pdf Binomial Random Variable (.e. binomial formula) :\\[\n\\begin{aligned}\n    P(X=k) &= {n \\choose k} p^k (1-p)^{n-k}\\\\\n    \\text{} k &= 0, 1, 2, 3, \\cdots, n\n\\end{aligned}\n\\]apply formula graphing calculator:\n\\(\\texttt{2nd -> vars (distr) -> binompdf}\\)Usage: \\(\\texttt{binompdf(n, p[,x])}\\)cdf Binomial Random Variable : \\[\n\\begin{aligned}\n    P(X\\leq k) &= \\sum_{= 0}^n {n \\choose } p^(1-p)^{n-}\n\\end{aligned}\n\\]graphing calculator: \\(\\texttt{2nd -> vars (distr) -> binomcdf}\\)Usage: \\(\\texttt{binomcdf(n, p[,x])}\\)mean standard deviation binomial random variable given\n:\\[\n\\begin{aligned}\n    \\mu_X &= n p \\\\\n    \\sigma_X &= \\sqrt{n p q} \\\\\n    &\\text{, } q = 1-p.\n\\end{aligned}\n\\]","code":""},{"path":"random-variables.html","id":"binomial-setting","chapter":"9 Random Variables","heading":"9.1.1.1 Binomial setting","text":"can identify binomial setting know:Binary? possible outcomes trial can classified \n“success” “failure” (case rolling 7 ).Binary? possible outcomes trial can classified \n“success” “failure” (case rolling 7 ).Independent? Trials must independent; , knowing \nresult one trial must tells us anything result \nanother trial.Independent? Trials must independent; , knowing \nresult one trial must tells us anything result \nanother trial.Number? number trials n chance process must \nfixed advance. (5, 100).Number? number trials n chance process must \nfixed advance. (5, 100).? probability p success trial\n(1/6).? probability p success trial\n(1/6).","code":""},{"path":"random-variables.html","id":"ten-per-cond","chapter":"9 Random Variables","heading":"9.1.1.2 10% Condition","text":"second condition often perfectly met, case \nSRS population. Imagine choosing 10 students class \n15 females 15 males—choose people, remaining population\nchanges, changes probability next person chosen \nmale female.lack complete independence, can see consequence \nnegligible long sample small relative population\nsampling. choosing 10 people \nschool 3,300 students, change probability person \nperson small enough ignore.general rule sample needs less \n\\(\\frac{1}{10}\\), 10%, population. refer 10%\ncondition.\\[n \\leq (.10) N\\]","code":""},{"path":"random-variables.html","id":"normal-approximation-to-the-binomial-distribution","chapter":"9 Random Variables","heading":"9.1.1.3 Normal Approximation to the Binomial Distribution","text":"Remember \\(n\\) gets large, binomial random variable \\(X\\) can take\ndifferent values, can become tedious continue\ntreat X discrete random variable. \\(n\\) get larger, can\ntreat \\(X\\) continuous random variable, specifically:\\(n\\) gets larger, binomial distribution gets closer normal\ndistribution. However, use normal distribution \napproximate binomial distribution, check following\ncondition:","code":""},{"path":"random-variables.html","id":"large-counts-cond","chapter":"9 Random Variables","heading":"9.1.1.3.1 Large Counts condition","text":"can use Normal distribution model binomial distribution. know:\n\\[np \\geq 10 \\text{ }n(1-p) \\geq 10\\],\nwords, expected number successes failures (respectively) greater equal 10.","code":""},{"path":"random-variables.html","id":"doing-a-normal-approximation","chapter":"9 Random Variables","heading":"9.1.1.3.2 Doing a normal approximation","text":"First verify conditions Binomial setting Large\nCounts Condition. Since know binomial setting, \nknow distribution want use \\(Normal(np, \\sqrt{npq})\\) proceed calculations according distribution.","code":""},{"path":"random-variables.html","id":"geometric-random-variables","chapter":"9 Random Variables","heading":"9.1.2 Geometric Random Variables","text":"\\(X \\sim G(p)\\), \\(X\\) geometric distribution\nparameter \\(p\\)pdf geometric random variable :\\[\n\\begin{aligned}\n    P(X=x) &= (1-p)^{x-1}p\\\\\n    \\text{} x &= 1, 2, 3, \\cdots\n\\end{aligned}\n\\]graphing calculator: \\(\\texttt{2nd -> vars (distr) -> geometpdf}\\)Usage: \\(\\texttt{geometpdf(p, x)}\\)cdf Geometric Variable :\\[\n\\begin{aligned}\n    P(X\\leq x) &= \\sum_{=1}^x(1-p)^{-1}p\n\\end{aligned}\n\\]graphing calculator: \\(\\texttt{2nd -> vars (distr) -> geometcdf}\\)Usage: \\(\\texttt{geometcdf(p, x)}\\)mean standard deviation geometric random variable given\n:\\[\n\\begin{aligned}\n    \\mu_X &= \\frac{1}{p} \\\\\n    \\sigma_X &= \\frac{\\sqrt{q}}{p} \\\\\n    &\\text{, } q = 1-p.\n\\end{aligned}\n\\]","code":""},{"path":"random-variables.html","id":"the-geometric-setting","chapter":"9 Random Variables","heading":"9.1.2.1 The Geometric Setting","text":"geometric setting similar binomial setting, except \n.geometric setting defined series observations 4\nconditions met:Binary? Possible outcomes trial can classified \n“success” “failure”Binary? Possible outcomes trial can classified \n“success” “failure”Independent? Trials must independent, , knowing \nresult one trial must effect result \ntrial.Independent? Trials must independent, , knowing \nresult one trial must effect result \ntrial.Trials? goal count number trials \nfirst success occurs.Trials? goal count number trials \nfirst success occurs.Success? trial, probability p success must \n.Success? trial, probability p success must \n.","code":""},{"path":"random-variables.html","id":"operations-with-random-variables","chapter":"9 Random Variables","heading":"9.2 Operations with Random Variables","text":"","code":""},{"path":"random-variables.html","id":"constants","chapter":"9 Random Variables","heading":"9.2.1 Constants","text":"add constant \\(\\) /multiply constant \\(b\\) random\nvariable \\(X\\), perform linear transformation form\\[\n+ bX\n\\]mean transformed variable :\\[\n\\mu_{+bX}=+\\mu_{bX}=+b\\mu_X\n\\]standard deviation transformed variable :\\[\n\\sigma_{+bX}=\\sigma_{bX}=b\\sigma_X\n\\]","code":""},{"path":"random-variables.html","id":"random-variables-1","chapter":"9 Random Variables","heading":"9.2.2 Random Variables","text":"general, can describe mean standard deviation sum\ndifference independent random variables formulas:\\[\\mu_{X \\pm Y} = \\mu_X \\pm \\mu_Y\\]random variables \\(X\\) \\(Y\\) independent, \\[ \\sigma_{X \\pm Y}^2 = \\sigma_X^2 + \\sigma_Y^2 \\]","code":""},{"path":"sampling-distributions.html","id":"sampling-distributions","chapter":"10 Sampling Distributions","heading":"10 Sampling Distributions","text":"Sampling Distribution\nsampling distribution distribution values taken sample statistic (sample mean sample proportion) possible samples given size \\(n\\) population.\nUnbiased Estimator\nstatistic used estimate parameter mean sampling distribution equal true value parameter estimated.\n\nstatistics unbiased estimators data collect always follow random process representative population.\nVariability Statistic\nvariability statistic described spread sampling distribution.\n\nspread determined mainly size random sample. Larger samples give smaller spreads. spread sampling distribution depend much size\npopulation, long population least 10 times larger sample (10% Condition).\n","code":""},{"path":"sampling-distributions.html","id":"sampling-distribution-of-hat-p","chapter":"10 Sampling Distributions","heading":"10.1 Sampling Distribution of \\(\\hat p\\)","text":"Choose SRS size \\(n\\) population size \\(N\\) proportion \\(p\\) successes. Let \\(\\hat p\\) sample proportion success. :mean sampling distribution \\(\\hat p\\) \\[\\mu_{\\hat p} = p\\]mean sampling distribution \\(\\hat p\\) \\[\\mu_{\\hat p} = p\\]standard deviation sampling distribution \\(\\hat p\\) \\[\\sigma_{\\hat p} = \\frac{\\sigma_p}{\\sqrt{n}} = \\sqrt{\\frac{p(1-p)}{n}}\\] long 10% condition satisfied.standard deviation sampling distribution \\(\\hat p\\) \\[\\sigma_{\\hat p} = \\frac{\\sigma_p}{\\sqrt{n}} = \\sqrt{\\frac{p(1-p)}{n}}\\] long 10% condition satisfied.\\(n\\) increases, sampling distribution \\(\\hat p\\) becomes approximately Normal. perform calculations, check Large Counts condition satisfied:\\(n\\) increases, sampling distribution \\(\\hat p\\) becomes approximately Normal. perform calculations, check Large Counts condition satisfied:\\[\\begin{equation}\nnp \\geq 10 \\text{ }n(1-p) \\geq 10\n\\end{equation}\\]","code":""},{"path":"sampling-distributions.html","id":"sampling-distribution-of-bar-x","chapter":"10 Sampling Distributions","heading":"10.2 Sampling Distribution of \\(\\bar x\\)","text":"Suppose \\(\\bar x\\) mean SRS size \\(n\\) drawn large population mean \\(\\mu_X\\) standard deviation \\(\\sigma_X\\). :mean sampling distribution \\(\\bar x\\) \\[\\mu_{\\bar x} = \\mu_X\\]mean sampling distribution \\(\\bar x\\) \\[\\mu_{\\bar x} = \\mu_X\\]standard deviation sampling distribution \\(\\bar x\\) \\[\\sigma_{\\bar x} = \\frac{\\sigma_X}{\\sqrt{n}}\\] long 10% condition satisfied.standard deviation sampling distribution \\(\\bar x\\) \\[\\sigma_{\\bar x} = \\frac{\\sigma_X}{\\sqrt{n}}\\] long 10% condition satisfied.* facts mean standard deviation \\(\\bar x\\) true matter shape population distribution .","code":""},{"path":"sampling-distributions.html","id":"large-cond","chapter":"10 Sampling Distributions","heading":"10.2.1 Normal/Large Condition for \\(\\bar x\\)","text":"population distribution Normal, sampling distribution \\(\\bar x\\). true matter sample size \\(n\\) .population distribution Normal, sampling distribution \\(\\bar x\\) approximately Normal cases \\(n \\geq 30\\). fact justified Central Limit Theorem.","code":""},{"path":"sampling-distributions.html","id":"clt","chapter":"10 Sampling Distributions","heading":"10.2.2 Central Limit Theorem (CLT)","text":"population mean \\(\\mu\\) standard deviation \\(\\sigma\\) take sufficiently large random samples population replacement, distribution sample means approximately normal distributed.* allows us use Normal probability calculations answer questions sample means/proportions many observations even population distribution Normal","code":""},{"path":"sampling-distributions.html","id":"when-do-we-have-normality","chapter":"10 Sampling Distributions","heading":"10.2.3 When do we have normality?","text":"","code":""},{"path":"sampling-distributions.html","id":"prop-diff-samp-dist","chapter":"10 Sampling Distributions","heading":"10.3 Sampling Distribution of \\(\\hat p_1 - \\hat p_2\\)","text":"Choose SRS size \\(n_1\\) Population 1 proportion successes \\(p_1\\) independent SRS size \\(n_2\\) Population 2 proportion successes \\(p_2\\).Shape:\\(n_1p_1\\), \\(n_1(1-p_1)\\), \\(n_2p_2\\), \\(n_2(1-p_2\\)) least 10, sampling distribution \\(p_1-p_2\\) approximately Normal.Center:mean sampling distribution \\[\\mu_{\\hat p_1-\\hat p_2} =p_1-p_2\\].Spread:standard deviation \\(\\hat p_1-\\hat p_2\\) \\[s_{\\hat p_1-\\hat p_2}=\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\] long sample 10% population (10% condition)","code":""},{"path":"sampling-distributions.html","id":"mean-diff-samp-dist","chapter":"10 Sampling Distributions","heading":"10.4 Sampling Distribution of \\(\\bar x_1 - \\bar x_2\\)","text":"Choose SRS size \\(n_1\\) Population 1 mean \\(\\mu_1\\) standard deviation \\(\\sigma_1\\) independent SRS size \\(n_2\\) Population 2 mean \\(\\mu_2\\) standard deviation \\(\\sigma_2\\).Shape:population distributions Normal, sampling distribution \\(\\bar x_1- \\bar x_2\\) approximately Normal. cases, sampling distribution approximately Normal sample sizes large enough (\\(n_1 \\geq 30\\) \\(n_2 \\geq 30\\))Center:mean sampling distribution \n\\[\\mu_{\\bar x_1- \\bar x_2} = \\mu_1- \\mu_2\\].Spread:long sample 10% population (10% condition), standard deviation sampling distribution \\(\\bar x_1- \\bar x_2\\) \n\\[\\sigma_{\\bar x_1- \\bar x_2} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\]","code":""},{"path":"confidence-intervals.html","id":"confidence-intervals","chapter":"11 Confidence Intervals","heading":"11 Confidence Intervals","text":"Confidence Interval interval plausible values population parameter used estimate parameter.interval calculated data form\n\\[\\text{point estimate} ± \\text{margin error}\\]Margin errors calculated confidence level (\\(C\\)) data form\\[= m = \\text{critical value} \\cdot \\text{standard error statistic}\\]difference point estimate, value statistics sample provides estimate population parameter, true parameter value less margin error \\(C\\)% samples.","code":""},{"path":"confidence-intervals.html","id":"interpreting-confidence-intervals","chapter":"11 Confidence Intervals","heading":"11.1 Interpreting Confidence Intervals","text":"can say something form:[\\(C\\)]% confident interval [lower bound] [upper bound] captures [true parameter context problem].say “95% chance interval contains population (proportion mean)” single interval either capture real value, either probability 0 1","code":""},{"path":"confidence-intervals.html","id":"interpreting-confidence-level","chapter":"11 Confidence Intervals","heading":"11.2 Interpreting Confidence Level","text":"take many, many samples create many, many intervals using method, [\\(C\\)]% capture [true parameter context problem].words, can say : “95% intervals created manner capture true population (proportion mean)”","code":""},{"path":"confidence-intervals.html","id":"changing-confidence-levels","chapter":"11 Confidence Intervals","heading":"11.3 Changing Confidence Levels","text":"increase confidence level, saying intervals manner capture true parameter. means increase confidence level means confidence intervals need increase width.increase size sample, precision statistic increases reduction sampling variability (standard deviation statistic decreases n increases). means can now guess/estimate smaller range compared smaller sample size. , increase size samples, confidence intervals smaller widths compared smaller sample size.","code":""},{"path":"confidence-intervals.html","id":"conditions-for-confidence-intervals","chapter":"11 Confidence Intervals","heading":"11.4 Conditions for Confidence Intervals","text":"","code":""},{"path":"confidence-intervals.html","id":"random","chapter":"11 Confidence Intervals","heading":"1. Random","text":"data must come random sample, random assignment experiment.order infer larger population, must random, unbiased sample population. sample random, need think whether ’s least unbiased /representative. , may able treat random sample, must state ’re . ’re told sample random, can write something like “’m going assume can treat random sample”.case experiments, can perform statistical tests data experimental groups, even don’t random sample larger population. case, must make sure internal validity experiment (well-conducted experiment control, replication, random assignment treatments). , results valid subjects, may may able generalize larger population. depends whether subjects random sample , least representative , larger population.","code":""},{"path":"confidence-intervals.html","id":"large","chapter":"11 Confidence Intervals","heading":"2. Large","text":"must check following know sample large enough us know approximately normal distribution.","code":""},{"path":"confidence-intervals.html","id":"proportions","chapter":"11 Confidence Intervals","heading":"Proportions","text":"proportions, remember, population never anywhere near normal (’s always two bars, yes ). case check Large Counts Condition.ensures sample proportion can take enough different values (make enough bars histogram) create approximately normal sampling distribution.","code":""},{"path":"confidence-intervals.html","id":"sample-means","chapter":"11 Confidence Intervals","heading":"Sample means","text":"case sample means, sampling distribution approximately normal least one following conditions met.Normal/Large ConditionNormal/Large ConditionWhen \\(n<30\\), check graph sample data show strong skew outliers. Otherwise,\\(n<30\\), check graph sample data show strong skew outliers. Otherwise,","code":""},{"path":"confidence-intervals.html","id":"sampling-independence","chapter":"11 Confidence Intervals","heading":"3. Sampling Independence","text":"Observations sample must independent .random samples population, observations never independent population changes every person sample remove . However, effect small enough ignore long population ’re sampling least 10 times large sample. needs stated!experiments, situations ’re sampling population (rolls dice example), just need think whether outcome one observation effect outcome another observation. Often don’t know , need state ’re assuming ; “’m going assume can treat observations independent.”","code":""},{"path":"confidence-intervals.html","id":"the-four-step-process","chapter":"11 Confidence Intervals","heading":"11.5 The Four-Step Process","text":"STATE exactly ’re doingBe sure specific , especially regard defining actual numbers analyzing. need demonstrate understanding population ’re inferring, procedure ’re using, parameter interest. ’re conducting statistical test (’ll cover next), also need write hypotheses . example: “want estimate true proportion CA voters support repeal Prop 8 95% confidence”PLAN method use necessary conditions met.conditions met, can’t sure ’re met, need explain needs true order conditions met.calculationsPerform calculations necessary method chose use. Show work!CONCLUDE context situation giving complete explanation value(s) calculated (c).“95% confident true proportion CA voters support repeal Prop 8 42% 51%. 95% intervals created like one capture true population proportion CA voters support repeal Prop 8.”","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"12 Hypothesis Testing","heading":"12 Hypothesis Testing","text":"","code":""},{"path":"hypothesis-testing.html","id":"significance-tests","chapter":"12 Hypothesis Testing","heading":"12.1 Significance Tests","text":"Confidence intervals one two common types statistical inference. Use confidence interval goal estimate population parameter. second common type inference, called significance tests, different goal: assess evidence provided data claim concerning population.significance test formal procedure comparing observed data claim (also called hypothesis) whose truth want assess. claim statement parameter, like population proportion \\(p\\) population mean \\(\\mu\\). express results significance test terms probability measures well data claim agree.","code":""},{"path":"hypothesis-testing.html","id":"hypotheses","chapter":"12 Hypothesis Testing","heading":"12.1.1 Hypotheses","text":"significance test starts careful statement claims want compare.\\(H_0\\): claim weigh evidence statistical test called null hypothesis. Often null hypothesis statement “difference.”\nsignificance test, null hypothesis form:\nsignificance test, null hypothesis form:\\[H_0 : \\text{parameter} = \\text{null value} \\]\\(H_a\\): claim population trying find evidence alternative hypothesis.\nalternative hypothesis one forms:\nalternative hypothesis one forms:\\[\n\\begin{aligned}\nH_0 &: \\text{parameter} > \\text{null value}\\\\\nH_0 &: \\text{parameter} < \\text{null value}\\\\\nH_0 &: \\text{parameter} \\= \\text{null value}\n\\end{aligned}\n\\]determine correct form \\(H_a\\), read problem carefully.alternative hypothesis one-sided states parameter larger null hypothesis value states parameter smaller null value.two-sided states parameter different null hypothesis value (either larger smaller).hypotheses express hopes suspicions see data. cheating look data first frame hypotheses fit data show (.e. “p-hacking”).Hypotheses always refer population, sample. sure state \\(H_0\\) \\(H_a\\) terms population parameters.never correct write hypothesis sample statistic, \\(\\hat p=0.64\\) \\(\\bar x=85\\).","code":""},{"path":"hypothesis-testing.html","id":"interpreting-p-values","chapter":"12 Hypothesis Testing","heading":"12.1.2 Interpreting P-values","text":"null hypothesis \\(H_0\\) states claim seeking evidence . probability measures strength evidence null hypothesis called P-value.\nprobability, computed assuming \\(H_0\\) true, statistic take value extreme extreme one actually observed called P-value test.\nSmall P-values evidence \\(H_0\\) say observed result unlikely occur \\(H_0\\) true.\nLarge P-values fail give convincing evidence \\(H_0\\) say observed result likely occur chance \\(H_0\\) true.","code":""},{"path":"hypothesis-testing.html","id":"drawing-a-conclusion","chapter":"12 Hypothesis Testing","heading":"12.1.3 Drawing a Conclusion","text":"final step performing significance test draw conclusion competing claims testing. make one two decisions based strength evidence null hypothesis (favor alternative hypothesis):\\[\\text{reject } H_0 \\text{ fail reject } H_0\\]Note: fail--reject \\(H_0\\) decision significance test doesn’t mean \\(H_0\\) true. reason, never “accept \\(H_0\\)” use language implying believe \\(H_0\\) true.rule small P-value require order reject \\(H_0\\). can compare P-value fixed value regard decisive, called significance level. write \\(\\alpha\\), Greek letter alpha.P-value smaller alpha, say data statistically significant level . case, reject null hypothesis \\(H_0\\) conclude convincing evidence favor alternative hypothesis \\(H_a\\).nutshell, conclusion significance test comes \\[\n\\begin{aligned}\n\\text{P-value} &<  \\text{reject } H_0 \\rightarrow \\text{convincing evidence } H_a \\\\\n\\text{P-value} &\\geq \\text{fail reject } H_0  \\rightarrow \\text{convincing evidence } H_a\n\\end{aligned}\n\\]","code":""},{"path":"hypothesis-testing.html","id":"example-1","chapter":"12 Hypothesis Testing","heading":"12.1.4 Example","text":"(“gender discrimination” CYU)\\\nFactinate.com claims 84% teenagers think highly mother. investigate claim, school psychologist selects random sample 150 teenagers finds 135 think highly mother. data provide convincing evidence true proportion teens think highly mother greater 0.84?State appropriate hypotheses performing significance test. sure define parameter interest.Let \\(p =\\) proportion teenagers think highly mother.\\(H_0: p = 0.85\\) (assuming “difference” Factinate.com’s claim)\\(H_0: p > 0.85\\) (evidence got \\(\\frac{135}{150}\\), greater 0.85; also problem asks, “data provide convincing evidence true proportion teens think highly mother greater 0.84?”)school psychologist performed significance test obtained P-value 0.0225. Interpret P-value.Assuming \\(H_0\\) true (.e. \\(p = 0.84\\)), 0.0225 probability getting sample proportion \\(\\frac{135}{150}\\) greater purely chance.can also include context , context answer already infers knowledge interpreting context long-term frequency. really need interpretation.conclusion make \\(\\alpha = 0.05\\) level?Since \\(\\text{p-value } = 0.0225<0.05=\\alpha\\), reject null hypothesis proportion teenagers think highly mother 0.85. convincing evidence true proportion teens think highly mother greater 0.84.","code":""},{"path":"hypothesis-testing.html","id":"type-i-and-ii-error","chapter":"12 Hypothesis Testing","heading":"12.2 Type I and II Error","text":"draw conclusion significance test, hope conclusion correct. sometimes wrong. two types mistakes can make.reject \\(H_0\\) \\(H_a\\) true, committed Type error.reject \\(H_0\\) \\(H_a\\) true, committed Type error.fail reject \\(H_0\\) \\(H_a\\) true, committed Type II error.fail reject \\(H_0\\) \\(H_a\\) true, committed Type II error.can sum table:considering consequences results inference task, want consider Type II errors. cases, ’d prefer Type errors Type II. instances, ’s opposite.","code":""},{"path":"hypothesis-testing.html","id":"type-i-error","chapter":"12 Hypothesis Testing","heading":"12.2.1 Type I Error","text":"probability Type Error \\(\\alpha\\)\n\\[P(\\text{Type Error})=\\alpha\\]","code":""},{"path":"hypothesis-testing.html","id":"type-ii-errors-and-power","chapter":"12 Hypothesis Testing","heading":"12.2.2 Type II Errors and Power","text":"significance test makes Type II error fails reject null hypothesis \\(H_0\\) really false. many values parameter make alternative hypothesis \\(H_a\\) true, concentrate one value. probability making Type II error depends several factors, including actual value parameter. high probability Type II error specific alternative parameter value means test sensitive enough usually detect alternative.power test specific alternative probability test reject \\(H_0\\) chosen significance level specified alternative value parameter true.significance level test probability reaching wrong conclusion null hypothesis true.power test detect specific alternative probability reaching right conclusion alternative true.power test alternative 1 minus probability Type II error alternative; , \\(power = 1-\\beta\\), probability making Type II error.\\[\n\\begin{aligned}\nP(\\text{Type II Error}) &= \\beta \\\\\nP(\\text{Type II Error}) &= 1 - \\text{power} \\\\\n\\text{power} &= 1 - \\beta\n\\end{aligned}\n\\]* Keep mind expected know calculate power \\(\\beta\\) without one . .e. always given power \\(\\beta\\) problem.","code":""},{"path":"hypothesis-testing.html","id":"how-power-looks-like","chapter":"12 Hypothesis Testing","heading":"12.2.2.1 How power looks like","text":"diagram illustrates can visualize power one-sided test checking see parameter larger null parameter. null parameter 0 alternative parameter 3. respective distributions centered 0 3. think power, first look \\(\\alpha\\) value, gives us rejection region (shaded black) statistic get reject null hypothesis.Power probability reject null hypothesis given alternative parameter actually true. , power represented yellow area, starts x (“cut-” value rejection based alpha) rejection region starts.\\(\\beta\\) probability Type II error, reject null hypothesis actually wrong. represented green area.See interactive version .","code":""},{"path":"inference-for-proportions.html","id":"inference-for-proportions","chapter":"13 Inference for Proportions","heading":"13 Inference for Proportions","text":"","code":""},{"path":"inference-for-proportions.html","id":"conditions-for-inference","chapter":"13 Inference for Proportions","heading":"13.1 Conditions for Inference","text":"","code":""},{"path":"inference-for-proportions.html","id":"random-1","chapter":"13 Inference for Proportions","heading":"1. Random","text":"data must come random sample, random assignment experiment.","code":""},{"path":"inference-for-proportions.html","id":"large-1","chapter":"13 Inference for Proportions","heading":"2. Large","text":"must check following know sample large enough us know approximately normal distribution.proportions, remember, population never anywhere near normal (’s always two bars, yes ). case check Large Counts Condition.ensures sample proportion can take enough different values (make enough bars histogram) create approximately normal sampling distribution.","code":""},{"path":"inference-for-proportions.html","id":"sampling-independence-1","chapter":"13 Inference for Proportions","heading":"3. Sampling Independence","text":"Observations sample must independent .random samples population, observations never independent population changes every person sample remove . However, effect small enough ignore long population ’re sampling least 10 times large sample. needs stated!","code":""},{"path":"inference-for-proportions.html","id":"one-sample-z-procedures","chapter":"13 Inference for Proportions","heading":"13.2 One-Sample z-procedures","text":"","code":""},{"path":"inference-for-proportions.html","id":"one-sample-z-interval","chapter":"13 Inference for Proportions","heading":"13.2.1 One-Sample z-interval","text":"\\(C\\)% confidence interval unknown population proportion \\(p\\) conditions met calculated following:\\[\\hat p \\pm z^* \\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\]\\(z^*\\) critical value standard Normal curve \\(C\\)% area \\(-z^*\\) \\(z^*\\).standard error sample proportion :\n\\[SE_{\\hat p}=\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\]","code":""},{"path":"inference-for-proportions.html","id":"one-sample-z-test","chapter":"13 Inference for Proportions","heading":"13.2.2 One-Sample z-test","text":"Given null hypothesis \\(H_0: p=p_0\\), observed sample proportion \\(\\hat p\\) sample size \\(n\\) conditions met. null sampling distribution \\(\\hat p\\), calculated assuming \\(H_0\\) true, following:\\[\\mu_{\\hat p} = p_0\\]\n\\[\\sigma_{\\hat p} = \\frac{\\sigma_{p_0}}{\\sqrt n} = \\sqrt{\\frac{p_0(1- p_0)}{n}}\\]z-statistic standardized score \\(\\hat p\\) mean standard deviation null distribution:\\[z=\\frac{\\hat p - p_0}{\\sqrt{\\frac{p_0(1- p_0)}{n}}}\\]p-value, calculated using z-statistic, based direction alternative hypothesis, \\(H_a\\). \\(Z\\) represents standard normal distribution.\\[\n\\text{p-value}= \\begin{cases}\nP(Z<z) & \\text{ } H_a:p<p_0 \\\\\nP(Z>z) & \\text{ } H_a:p>p_0 \\\\\n2\\cdot P(Z<-|z|) & \\text{ } H_a:p\\=p_0\n\\end{cases}\n\\]","code":""},{"path":"inference-for-proportions.html","id":"two-sample-z-procedures","chapter":"13 Inference for Proportions","heading":"13.3 Two-Sample z-procedures","text":"Two-Sample z-procedures justified knowledge sampling distribution \\(\\hat p_1 - \\hat p_2\\).conditions two-sample z-procedures two learn:Random: data come two independent random samples two groups randomized experiment.Random: data come two independent random samples two groups randomized experiment.Large Counts: counts “successes” “failures” sample group \\(n_1\\hat p_1\\), \\(n_1(1-\\hat p_1)\\), \\(n_2\\hat p_2\\), \\(n_2(1-\\hat p_2)\\) least 10.Large Counts: counts “successes” “failures” sample group \\(n_1\\hat p_1\\), \\(n_1(1-\\hat p_1)\\), \\(n_2\\hat p_2\\), \\(n_2(1-\\hat p_2)\\) least 10.10%: sampling without replacement, check \\(n_1\\leq(.10)N_1\\) \\(n2\\leq(.10)N_2\\).10%: sampling without replacement, check \\(n_1\\leq(.10)N_1\\) \\(n2\\leq(.10)N_2\\).","code":""},{"path":"inference-for-proportions.html","id":"two-sample-z-interval","chapter":"13 Inference for Proportions","heading":"13.3.1 Two-Sample z-interval","text":"conditions met, approximate \\(C\\)% confidence interval \\(p_1-p_2\\) :\\[(\\hat p_1-\\hat p_2)\\pm z^*\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1}+\\frac{\\hat p_2(1-\\hat p_2)}{n_2}}\\]\\(z^*\\) critical value standard Normal curve \\(C\\)% area \\(-z^*\\) \\(z^*\\)","code":""},{"path":"inference-for-proportions.html","id":"two-sample-z-test","chapter":"13 Inference for Proportions","heading":"13.3.2 Two-Sample z-test","text":"null hypothesis general form\\[ H_0:p_1-p_2=p_0 \\]\n\\(p_0\\) hypothesized difference. ’ll restrict situations \\(p_0 =0\\). null hypothesis says difference two parameters:\n\\[H_0:p_1-p_2=0~~\\text{}~~H_0:p_1=p_2\\]\nalternative hypothesis says kind difference expect:\n\\[H_a:p_1-p_2>0~~\\text{}~~Ha:p_1-p_2<0~~\\text{}~~Ha:p_1-p_2\\=0  \\]Suppose conditions met. test hypotheses \\(H_0: p_1 - p_2 = 0\\), first find pooled proportion \\(\\hat p_c\\) successes samples combined.\\[\\hat p_c = \\frac{X_1 + X_2}{n_1 + n_2}\\]compute z statistic:\n\\[ z = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\frac{\\hat p_c(1-\\hat p_c)}{n_1} +\\frac{\\hat p_c(1-\\hat p_c)}{n_2}}}\\]","code":""},{"path":"inference-for-means.html","id":"inference-for-means","chapter":"14 Inference for Means","heading":"14 Inference for Means","text":"","code":""},{"path":"inference-for-means.html","id":"conditions-for-inference-1","chapter":"14 Inference for Means","heading":"14.1 Conditions for Inference","text":"","code":""},{"path":"inference-for-means.html","id":"random-2","chapter":"14 Inference for Means","heading":"1. Random","text":"data must come random sample, random assignment experiment.","code":""},{"path":"inference-for-means.html","id":"large-2","chapter":"14 Inference for Means","heading":"2. Large","text":"must check following know sample large enough us know approximately normal distribution.case sample means, sampling distribution approximately normal least one following conditions met.Normal/Large ConditionNormal/Large ConditionWhen \\(n<30\\), check graph sample data show strong skew outliers. Otherwise,\\(n<30\\), check graph sample data show strong skew outliers. Otherwise,","code":""},{"path":"inference-for-means.html","id":"sampling-independence-2","chapter":"14 Inference for Means","heading":"3. Sampling Independence","text":"Observations sample must independent .random samples population, observations never independent population changes every person sample remove . However, effect small enough ignore long population ’re sampling least 10 times large sample. needs stated!","code":""},{"path":"inference-for-means.html","id":"one-sample-t-procedures","chapter":"14 Inference for Means","heading":"14.2 One-Sample t-procedures","text":"","code":""},{"path":"inference-for-means.html","id":"one-sample-t-interval","chapter":"14 Inference for Means","heading":"14.2.1 One-Sample t-interval","text":"\\(C\\)% confidence interval unknown population mean \\(\\mu\\) conditions met calculated following:\\[\\bar x \\pm t^* \\frac{s_x}{\\sqrt{n}}\\]\\(t^*\\) critical value Student’s \\(t\\) distribution corresponding degrees freedom \\(df = n - 1\\) \\(C\\)% area \\(-t^*\\) \\(t^*\\).standard error sample mean :\n\\[SE_{\\bar x}=\\frac{s_x}{\\sqrt{n}}\\]","code":""},{"path":"inference-for-means.html","id":"one-sample-t-test","chapter":"14 Inference for Means","heading":"14.2.2 One-Sample t-test","text":"Given null hypothesis \\(H_0: \\mu=\\mu_0\\), observed sample mean \\(\\bar x\\) sample standard deviation \\(s_x\\) sample size \\(n\\) conditions met. null sampling distribution \\(bar x\\), calculated assuming \\(H_0\\) true, following:\\[\\mu_{\\bar x} = \\mu_0\\]\n\\[\\sigma_{\\bar x} = \\frac{\\sigma_x}{\\sqrt n} \\approx \\frac{s_x}{\\sqrt n}\\]use \\(s_x\\) place \\(sigma_x\\) population standard deviation usually unknown can use sample standard deviation unbiased estimator population standard deviation.t-statistic standardized score \\(\\bar x\\) mean standard deviation null distribution:\\[t=\\frac{\\bar x - \\mu_0}{\\frac{s_x}{\\sqrt n}}\\]p-value, calculated using t-statistic, based direction alternative hypothesis, \\(H_a\\). \\(t_{n-1}\\) represents \\(t\\) distribution \\(df = n-1\\) degrees freedom.\\[\n\\text{p-value}= \\begin{cases}\nP(t_{n-1}<t) & \\text{ } H_a:\\mu<\\mu_0 \\\\\nP(t_{n-1}>t) & \\text{ } H_a:\\mu>\\mu_0 \\\\\n2\\cdot P(t_{n-1}<-|t|) & \\text{ } H_a: \\mu\\=\\mu_0\n\\end{cases}\n\\]","code":""},{"path":"inference-for-means.html","id":"paired-t-test","chapter":"14 Inference for Means","heading":"14.2.2.1 Paired t-test","text":"paired t-test used data compared related matched way, whereas 2-sample t-test used data compared independent. scenarios paired t-test appropriate 2-sample t-test:Matched pair design:--studies: group individuals measured twice, receiving treatment intervention, paired t-test used determine whether significant difference measurements.--studies: group individuals measured twice, receiving treatment intervention, paired t-test used determine whether significant difference measurements.Matched case-control studies: cases controls matched based certain criteria (age, sex, race), paired t-test can used compare measurements two groups.Matched case-control studies: cases controls matched based certain criteria (age, sex, race), paired t-test can used compare measurements two groups.Repeated measures: group individuals measured multiple time points, paired t-test can used compare measurements within individual.scenarios, paired t-test can powerful 2-sample t-test takes account relationship correlation data compared. contrast, 2-sample t-test assumes data compared independent, may case situations.Given null hypothesis \\(H_0: \\mu_d=\\mu_0\\) (\\(\\mu_d\\) represents true mean difference) observed sample mean \\(\\bar x\\) (differences) sample standard deviation \\(s_x\\) (differences) sample size \\(n\\) conditions met. null sampling distribution \\(\\bar x\\), calculated assuming \\(H_0\\) true, null distribution t-statistic one-sample t-test.","code":""},{"path":"inference-for-means.html","id":"conditions","chapter":"14 Inference for Means","heading":"14.2.2.1.1 Conditions","text":"1. RandomThe data must come random sample, random assignment experiment. latter likely apply paired test.2. LargeYou must check following know sample large enough us know approximately normal distribution.case paired data, ’ll checking sampling distribution differences. sampling distribution approximately normal least one following conditions met.Normal/Large ConditionNormal/Large ConditionWhen \\(n<30\\), check graph sample data show strong skew outliers. Otherwise,\\(n<30\\), check graph sample data show strong skew outliers. Otherwise,3. Sampling IndependenceObservations sample must independent .random samples population, observations never independent population changes every person sample remove . However, effect small enough ignore long population ’re sampling least 10 times large sample. needs stated!However, since ’ll likely working experiments, independence apply, state . can say something like “since experiment, check independence”.","code":""},{"path":"inference-for-means.html","id":"two-sample-t-procedures","chapter":"14 Inference for Means","heading":"14.3 Two-Sample t-procedures","text":"Two-Sample t-procedures justified knowledge sampling distribution \\(\\bar x_1 - \\bar x_2\\).conditions two-sample t-procedures two learn:Random: data come two independent random samples two groups randomized experiment.Random: data come two independent random samples two groups randomized experiment.Normal/Large Sample: population distributions (true distributions responses two treatments) Normal sample sizes large (\\(n_1 \\geq 30\\) \\(n_2 \\geq 30\\)). either population (treatment) distribution unknown shape corresponding sample size less 30, use graph sample data assess Normality population (treatment) distribution. use two-sample t procedures graph shows strong skewness outliers.Normal/Large Sample: population distributions (true distributions responses two treatments) Normal sample sizes large (\\(n_1 \\geq 30\\) \\(n_2 \\geq 30\\)). either population (treatment) distribution unknown shape corresponding sample size less 30, use graph sample data assess Normality population (treatment) distribution. use two-sample t procedures graph shows strong skewness outliers.10%: sampling without replacement, check \\(n_1 \\leq (.10)N_1\\) \\(n_2 \\leq (.10)N_2\\).10%: sampling without replacement, check \\(n_1 \\leq (.10)N_1\\) \\(n_2 \\leq (.10)N_2\\).","code":""},{"path":"inference-for-means.html","id":"calculating-degrees-of-freedom","chapter":"14 Inference for Means","heading":"14.3.1 Calculating degrees of freedom","text":"two options calculating degrees freedom:Option 1 (Technology): Use \\(t\\) distribution degrees freedom calculated data formula . Note \\(df\\) given formula usually whole number.\n\\[df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{1}{n_1-1}\\left( \\frac{s_1^2}{n_1}\\right)^2 + \\frac{1}{n_2-1}\\left( \\frac{s_2^2}{n_2}\\right)^2}\\]Option 2 (Conservative): Use \\(t\\) distribution degrees freedom equal smaller \\(n_1 – 1\\) \\(n_2 – 1\\). option, resulting confidence interval margin error large larger needed desired confidence level. significance test using option gives P-value equal greater true P-value. sample sizes increase, confidence levels P-values Option 2 become accurate.","code":""},{"path":"inference-for-means.html","id":"two-sample-t-interval","chapter":"14 Inference for Means","heading":"14.3.2 Two-Sample t-interval","text":"","code":""},{"path":"inference-for-means.html","id":"two-sample-t-test","chapter":"14 Inference for Means","heading":"14.3.3 Two-Sample t-test","text":"","code":""},{"path":"inference-for-means.html","id":"the-students-t-distribution","chapter":"14 Inference for Means","heading":"14.4 The Student’s \\(t\\) distribution","text":"Draw SRS size \\(n\\) large population Normal distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\). statistic\n\\[t=\\frac{\\bar x - \\mu}{s_x / \\sqrt{n}}\\]\n\\(t\\)-distribution degrees freedom \\(df = n – 1\\), denoted \\(t_{n-1}\\). population distribution isn’t Normal, statistic approximately \\(t_{n-1}\\) sample size large enough.\\(df\\) increases, \\(t\\)-distribution approaches \\(N(0,1)\\) (standard normal) (See Figure 14.1). happens \\(s_x\\) estimates \\(\\sigma\\) accurately \\(n\\) increases. using \\(s_x\\) place \\(\\sigma\\) causes little extra variation sample large enough.","code":""},{"path":"inference-for-means.html","id":"why-a-t-distribution","chapter":"14 Inference for Means","heading":"14.4.1 Why a \\(t\\)-distribution?","text":"’re conducting inference population proportion, ’s one parameter (\\(p\\)) don’t know. sampling distributions cases follow Normal curve well (long conditions met), allowing us use \\(z\\)-procedures. However, ’re conducting inference population mean, additional uncertainty created fact two parameters don’t know: population mean \\(\\mu\\) population standard deviation \\(\\sigma\\). Since now , get different sampling distribution quite normal, especially small \\(n\\).result, use ’s called Student’s \\(t\\) distribution (Guinness Beer brewer developed ), slightly conservative version normal distribution. \\(t\\) distribution still symmetric single peak 0, much area tails. statistic \\(t\\) interpretation standardized \\(z\\) statistic: says many standard deviations \\(\\sigma\\) \\(x\\) distribution’s mean \\(\\mu\\).\nFigure 14.1: t-distribution df increases, standard normal curve red\n","code":""},{"path":"chi2-tests.html","id":"chi2-tests","chapter":"15 \\(\\chi^2\\) Tests$","heading":"15 \\(\\chi^2\\) Tests$","text":"","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"16 Linear Regression","heading":"16 Linear Regression","text":"","code":""},{"path":"ap-exam-formula-sheet.html","id":"ap-exam-formula-sheet","chapter":"A AP Exam Formula Sheet","heading":"A AP Exam Formula Sheet","text":"original AP Statistics Formula Sheet developed Collegeboard.","code":""},{"path":"ap-exam-formula-sheet.html","id":"descriptive-statistics","chapter":"A AP Exam Formula Sheet","heading":"A.1 Descriptive Statistics","text":"\\[\n\\begin{aligned}\n\\bar x &= \\frac{1}{n}\\sum x_i = \\frac{\\sum x_i}{n} \\\\\\\\\ns_x &= \\sqrt{\\frac{1}{n-1}\\sum (x_i - \\bar x)^2} = \\sqrt{\\frac{\\sum(x_i -\\bar x)^2}{n - 1}}\\\\\\\\\n\\hat y &= + bx \\\\\\\\\n\\bar y &= + b \\bar x\\\\\\\\\nr &= \\frac{1}{n-1}\\sum \\left( \\frac{x_i - \\bar x}{s_x}\\right) \\left( \\frac{y_i -\\bar y}{s_y}\\right)\\\\\\\\\nb &= r \\frac{s_y}{s_x}\n\\end{aligned}\n\\]","code":""},{"path":"ap-exam-formula-sheet.html","id":"probability-and-distributions","chapter":"A AP Exam Formula Sheet","heading":"A.2 Probability and Distributions","text":"\\[\n\\begin{aligned}\nP(\\cup B) &= P() + P(B) - P(\\cap B) \\\\\\\\\nP(| B) &= \\frac{P(\\cap B)}{P(B)}\n\\end{aligned}\n\\]","code":""},{"path":"ap-exam-formula-sheet.html","id":"sampling-distributions-and-inferential-statistics","chapter":"A AP Exam Formula Sheet","heading":"A.3 Sampling Distributions and Inferential Statistics","text":"\\[\\text{Standardized test statistic:}~~~~ \\frac{\\text{statistic} - \\text{parameter}} {\\text{standard error statistic}}\\]\\[\\text{Confidence interval:}~~~~ \\text{statistic} \\pm (\\text{critical value}) (\\text{standard error statistic})\\]\\[\\text{Chi-square statistic:}~~~~ \\chi^2 = \\sum \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\\]","code":""},{"path":"ap-exam-formula-sheet.html","id":"sampling-distributions-for-proportions","chapter":"A AP Exam Formula Sheet","heading":"A.3.1 Sampling distributions for proportions:","text":"","code":""},{"path":"ap-exam-formula-sheet.html","id":"sampling-distributions-for-means","chapter":"A AP Exam Formula Sheet","heading":"A.3.2 Sampling distributions for means:","text":"","code":""},{"path":"ap-exam-formula-sheet.html","id":"sampling-distributions-for-simple-linear-regression","chapter":"A AP Exam Formula Sheet","heading":"A.3.3 Sampling distributions for simple linear regression:","text":"*Standard deviation measurement variability theoretical population. Standard error estimate standard deviation. standard deviation statistic assumed known, standard deviation used instead standard error.","code":""},{"path":"ap-exam-formula-sheet.html","id":"table-a","chapter":"A AP Exam Formula Sheet","heading":"A.4 Table A Standard normal probabilities","text":"Table entry \\(z\\) probability lying \\(z\\).Table entry \\(z\\) probability lying \\(z\\).","code":""},{"path":"ap-exam-formula-sheet.html","id":"table-b","chapter":"A AP Exam Formula Sheet","heading":"A.5 Table B \\(t\\) distribution critical values","text":"Table entry \\(p\\) \\(C\\) point \\(t*\\) probability \\(p\\) lying probability \\(C\\) lying \\(-t*\\) \\(t*\\).","code":""},{"path":"ap-exam-formula-sheet.html","id":"table-c","chapter":"A AP Exam Formula Sheet","heading":"A.6 Table C \\(\\chi^2\\) critical values","text":"Table entry \\(p\\) point (\\(\\chi^2\\)) probability \\(p\\) lying .","code":"## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'"},{"path":"calculator-applet.html","id":"calculator-applet","chapter":"B Calculator applet","heading":"B Calculator applet","text":"meant something similar calculator works. behavior working well yet, ’s want use now. Things can calculator commands like:normalcdfnormalcdfinvNorminvNormgeometcdfgeometcdfgeometpdfgeometpdfinvTinvTtcdftcdfand basic arithmetic (though instead ^ exponents, use **)basic arithmetic (though instead ^ exponents, use **)»  ","code":""}]
