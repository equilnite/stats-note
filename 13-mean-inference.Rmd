# Inference for Means

## Conditions for Inference

### 1. Random {.unlisted .unnumbered}

The data must come from a random sample, or random assignment in an experiment.

### 2. Large {.unlisted .unnumbered}

You must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.

In the case of sample means, the sampling distribution is approximately normal if at least one of the following conditions are met. 

- [*Normal/Large Condition*](#large-cond)

- When $n<30$, check if the graph of the sample data does not show any strong skew or outliers. Otherwise, 

### 3. Sampling Independence {.unlisted .unnumbered}

Observations in our sample must be independent of each other.

In random samples from a population, observations are never independent because the population changes with every person we sample and remove from it.  However, this effect is small enough to ignore as long as the population from which we’re sampling is [at least 10 times as large as our sample](#ten-per-cond).  This needs to be stated!

## One-Sample t-interval

::: {.highlight}
A $C$\% confidence interval for the unknown population mean $\mu$ when all conditions are met is calculated with the following: 

$$\bar x \pm t^* \frac{s_x}{\sqrt{n}}$$
:::

where $t^*$ is the critical value for the Student's $t$ distribution corresponding with degrees of freedom $df = n - 1$ with $C$\% of its area between $-t^*$ and $t^*$.

::: {.highlight}
The standard error of the sample mean is:
$$SE_{\bar x}=\frac{s_x}{\sqrt{n}}$$
:::

## One-Sample t-test

::: {.highlight}

Given the null hypothesis $H_0: \mu=\mu_0$, and an observed sample mean $\bar x$ and sample standard deviation $s_x$ from a sample of size $n$ when all conditions are met. The null sampling distribution of $bar x$, calculated assuming $H_0$ is true, has the following:

$$\mu_{\bar x} = \mu_0$$
$$\sigma_{\bar x} = \frac{\sigma_x}{\sqrt n} \approx \frac{s_x}{\sqrt n}$$

Where we use $s_x$ in place of $sigma_x$ because the population standard deviation is usually unknown and we can use the sample standard deviation because it is an unbiased estimator of the population standard deviation.

The **t-statistic** is the standardized score of $\bar x$ under the mean and standard deviation of the null distribution:

$$t=\frac{\bar x - \mu_0}{\frac{s_x}{\sqrt n}}$$

The p-value, calculated using the t-statistic, is based off the direction of the alternative hypothesis, $H_a$. $t_{n-1}$ represents the $t$ distribution with $df = n-1$ degrees of freedom.

$$
\text{p-value}= \begin{cases}
P(t_{n-1}<t) & \text{ if } H_a:\mu<\mu_0 \\ 
P(t_{n-1}>t) & \text{ if } H_a:\mu>\mu_0 \\ 
2\cdot P(t_{n-1}<-|t|) & \text{ if } H_a: \mu\not =\mu_0 
\end{cases}
$$
:::

### Paired t-test

A paired t-test is used when the data being compared are related or matched in some way, whereas a 2-sample t-test is used when the data being compared are independent. Here are some scenarios where a paired t-test would be more appropriate than a 2-sample t-test:

Matched pair design:

- Before-and-after studies: When the same group of individuals is measured twice, such as before and after receiving a treatment or intervention, a paired t-test is used to determine whether there is a significant difference in the measurements.

- Matched case-control studies: When cases and controls are matched based on certain criteria (such as age, sex, or race), a paired t-test can be used to compare the measurements of the two groups.

Repeated measures: When a group of individuals is measured at multiple time points, a paired t-test can be used to compare the measurements within each individual.

In these scenarios, a paired t-test can be more powerful than a 2-sample t-test because it takes into account the relationship or correlation between the data being compared. By contrast, a 2-sample t-test assumes that the data being compared are independent, which may not be the case in these situations.

::: {.highlight}
Given the null hypothesis $H_0: \mu_d=\mu_0$ (where $\mu_d$ represents the true mean difference) and an observed sample mean $\bar x$ (of the differences) and sample standard deviation $s_x$ (of the differences) from a sample of size $n$ when all conditions are met. The null sampling distribution of $\bar x$, calculated assuming $H_0$ is true, has the same null distribution and t-statistic as a [one-sample t-test][One-Sample t-test].
:::

#### Conditions

::: {.highlight}
##### 1. Random {.unlisted .unnumbered}

The data must come from a random sample, or **random assignment in an experiment**. The latter of which will likely apply to a paired test.

##### 2. Large {.unlisted .unnumbered}

You must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.

In the case of paired data, you'll be only **checking the sampling distribution of the differences**. The sampling distribution is approximately normal if at least one of the following conditions are met. 

- [*Normal/Large Condition*](#large-cond)

- When $n<30$, check if the graph of the sample data does not show any strong skew or outliers. Otherwise, 

##### 3. Sampling Independence {.unlisted .unnumbered}

Observations in our sample must be independent of each other.

In random samples from a population, observations are never independent because the population changes with every person we sample and remove from it.  However, this effect is small enough to ignore as long as the population from which we’re sampling is [at least 10 times as large as our sample](#ten-per-cond).  This needs to be stated!

However, **since we'll be likely working with experiments, independence will not apply,** so you will not have to state it. You can say something like *"since we have an experiment, we do not have to check independence"*.
:::

## The Student's $t$ distribution

::: {.highlight}
Draw an SRS of size $n$ from a large population that has a Normal distribution with mean $\mu$ and standard deviation $\sigma$. The statistic 
$$t=\frac{\bar x - \mu}{s_x / \sqrt{n}}$$
has a $t$-distribution with degrees of freedom $df =  n – 1$, denoted as $t_{n-1}$. When the population distribution isn’t Normal, this statistic will be approximately $t_{n-1}$ if the sample size is large enough. 

As the $df$ increases, the $t$-distribution approaches $N(0,1)$ (standard normal) (See Figure \@ref(fig:t-dist-to-norm)). This happens because $s_x$ estimates $\sigma$ more accurately as $n$ increases. So using $s_x$ in place of $\sigma$ causes little extra variation when the sample is large enough.
:::

### Why a $t$-distribution?

When we’re conducting inference for a population proportion, there’s only one parameter ($p$) that we don’t know.  The sampling distributions in these cases follow a Normal curve very well (as long as conditions are met), allowing us to use $z$-procedures.  However, when we’re conducting inference for a population mean, there is additional uncertainty created by the fact that there are two parameters we don’t know: the population mean $\mu$ and the population standard deviation $\sigma$.  Since we now have, we get a different sampling distribution that is not quite normal, especially at small $n$.  

As a result, we use what’s called the Student's $t$ distribution (a Guinness Beer brewer developed this), which is a slightly more **conservative** version of a normal distribution.  The $t$ distribution is still symmetric with a single peak at 0, but with much more area in the tails. The statistic $t$ has the same interpretation as any standardized $z$ statistic: it says many standard deviations $\sigma$ that $x$ is from the distribution's mean $\mu$. 

```{r t-dist-to-norm, echo=FALSE, fig.cap="The t-distribution as df increases, with the standard normal curve in red"}

colors <- c("1" = "#E69F00", "2" = "#56B4E9", "3" = "#009E73", "5" = "#F0E442", "10" = "#0072B2", "Standard Normal" = "#D55E00")

colornames <- names(colors)



t.dists.list <- list(
function(x) dt(x, df = 1),
function(x) dt(x, df = 2),
function(x) dt(x, df = 3),
function(x) dt(x, df = 5),
function(x) dt(x, df = 10)
)

ggplot() + 
    stat_function(aes(colour = colornames[6]),
                  fun = dnorm, xlim = c(-4,4), size = 1.5) + 
    stat_function(aes(colour = colornames[1]),
                  fun = t.dists.list[[1]], xlim = c(-4,4), size = 1.5) + 
    stat_function(aes(colour = colornames[2]),
                  fun = t.dists.list[[2]], xlim = c(-4,4), size = 1.5) + 
    stat_function(aes(colour = colornames[3]),
                  fun = t.dists.list[[3]], xlim = c(-4,4), size = 1.5) + 
    stat_function(aes(colour = colornames[4]),
                  fun = t.dists.list[[4]], xlim = c(-4,4), size = 1.5) + 
    stat_function(aes(colour = colornames[5]),
                  fun = t.dists.list[[5]], xlim = c(-4,4), size = 1.5) +
    theme_void() +
    scale_color_manual(values = colors) +
    labs(title = "t-distribution and df", color = "df") +
    theme(plot.title = element_text(hjust = 0.5))


    
```
