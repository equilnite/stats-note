[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AP Statistics Notes",
    "section": "",
    "text": "1 Introduction\nThese are my notes based off what we learned in class. The numbers next to the sections are not related to the textbook / the chapter numbers. Please take a look at what we are learning to know which page to look at.\nThere are various functions on this website that make your notes accessible in the case that you need more support in reading the material. Take a look at the tool bar on the top and try right clicking on math equations to see options to enlarge math equations. Ask Mr. Chang if you want to explore more.\nThese notes are incomplete, but I’ll be trying to update them completely before the AP test. Give me any feedback that you might want to see on these notes.\nLook at the rest of these notes through the navigation bar on the left hand side (might have to click on the hamburger icon), or go through these sequentially by clicking the arrows on the left and right hand side."
  },
  {
    "objectID": "notation.html#lists",
    "href": "notation.html#lists",
    "title": "2  Basic Notation",
    "section": "2.1 Lists",
    "text": "2.1 Lists\nMost of the time, you will see \\(x\\) denote a list of values (i.e. a variable in a data table).\nFor example, if \\(x\\) was the list of numerical values \\(a\\) to \\(g\\), we can write it as:\n\\(x = [a, b, c, d, e, f, g]\\)\nThen, \\(x_i\\) means the \\(i^{th}\\) value in the list of \\(x\\), so\n\\(x_1 = a\\) and \\(x_2 = b\\) and \\(x_7 = g\\)."
  },
  {
    "objectID": "notation.html#n",
    "href": "notation.html#n",
    "title": "2  Basic Notation",
    "section": "2.2 \\(n\\)",
    "text": "2.2 \\(n\\)\nIn regards to a data table or list of values, \\(n\\) stands for the number of rows or data points that are in the data table or list (we will learn this as the sample size later on)\nSo, for list \\(x\\), \\(n = 7\\).\nAdding on, \\(x_n\\) would mean the last value in the list \\(x\\) (since there are only \\(n\\) values in \\(x\\))"
  },
  {
    "objectID": "notation.html#summation-sigma",
    "href": "notation.html#summation-sigma",
    "title": "2  Basic Notation",
    "section": "2.3 Summation (\\(\\Sigma\\))",
    "text": "2.3 Summation (\\(\\Sigma\\))\nYou will also see the greek letter \\(\\Sigma\\) in formulas. Usage of this sign means that we are using summation notation.\nIf we want the sum of all numbers from 1 to 7, we would write it as,\n\\[\\sum_{i=1}^7 i\\]\nWe interpret this as,\n\nStart from \\(i = 1\\), evaluate the expression, which is \\(i\\).\nKeep our evaluated expression to the side and get ready to add the other values to it, so\n\n\\[1 + \\cdots\\]\n\nNow go the next numbers until we get to \\(7\\) (the number on the top of the \\(\\Sigma\\)) So moving onto \\(i = 2\\), we end up with \\[1 + 2 + \\cdots\\] And with \\(i = 3\\), we end up with \\[1 + 2 + 3 + \\cdots\\] \nWhen we get to the end of it (when we reach \\(i = 7\\)), we have the expanded form of the summation. \\[1 + 2 + 3+ 4 + 5 + 6 + 7\\]"
  },
  {
    "objectID": "notation.html#other-notation",
    "href": "notation.html#other-notation",
    "title": "2  Basic Notation",
    "section": "2.4 Other notation",
    "text": "2.4 Other notation\n\n\\(\\bar x\\): The “line” on top of \\(x\\) means the mean of \\(x\\). If we had \\(\\bar a\\), I would be asking for the mean of \\(a\\).\n\nPronounced “x bar”\n\n\\(\\hat p\\): The “hat” on top of \\(p\\) means the estimate of \\(p\\). If we had \\(\\hat x\\), I would be asking for the estimate of \\(x\\).\n\nPronounced “p hat”"
  },
  {
    "objectID": "notation.html#other-commonly-used-symbols",
    "href": "notation.html#other-commonly-used-symbols",
    "title": "2  Basic Notation",
    "section": "2.5 Other commonly used symbols",
    "text": "2.5 Other commonly used symbols\n\n\\(N\\): population size\n\\(p\\): proportion, probability, or p-value\n\\(\\mu\\): population mean (true mean)\n\\(\\bar x\\): sample mean\n\\(\\sigma\\): population standard deviation (true standard deviation)\n\\(s_x\\): sample standard deviation (of x), so \\(s_y\\) is the sample standard deviation of \\(y\\)\n\\(SE_{\\bar x}\\): Standard error of the sample mean (of x), the estimate of the standard deviation of the sample mean."
  },
  {
    "objectID": "categorical-displays.html#bar-plots",
    "href": "categorical-displays.html#bar-plots",
    "title": "3  Categorical Data Visualizations",
    "section": "3.1 Bar plots",
    "text": "3.1 Bar plots\n\n\n\n\n\n\nBar Plots\n\n\n\nRepresent the number or proportion of each unique value. These numbers or proportions are represented with rectangular bars with heights proportional to the values that they represent.\nYou can plot these vertically or horizontally (i.e. categories on the x-axis or categories on the y-axis)\n\n\nFollowing data from this table:\n\n\n\n\n  \n\n\n\n\nCount up the number of values per category (make a frequency table). Note: This table is missing the total\n\n\n\n\n\n  \n\n\n\n\nPlot the frequencies with them as the height of the bars\n\n\n\n\n\n\nIf needed (if you need proportions for the y-axis instead, calculate the relative frequency table for the frequency table first). Note: again, this one is missing the total"
  },
  {
    "objectID": "categorical-displays.html#stacked-bar-plots-and-side-by-side-bar-graphs",
    "href": "categorical-displays.html#stacked-bar-plots-and-side-by-side-bar-graphs",
    "title": "3  Categorical Data Visualizations",
    "section": "3.2 Stacked Bar Plots and Side-by-Side Bar Graphs",
    "text": "3.2 Stacked Bar Plots and Side-by-Side Bar Graphs\n\n\n\n\n\n\nMultivariate Bar Plots\n\n\n\nStacked bar plots show two categorical variables, one on the x-axis/y-axis, and the other as the legend (colours). We will call the variable on the x-axis as the “groups” and the variable on the legend as the “categories.”\nSide-by-Side bar graphs are similar in concept.\n\n\nWhen constructing these bar plots, we first want to determine which variable goes where (your choice or given choice to you). Then you calculate relative frequencies per group\nFor example, here I have a two-way table detailing the hair and eye colour of some statistics students\n\n\n\n\n  \n\n\n\nSo if I want eye colour to be my groups, I would calculate the relative frequencies by column (use the total of the column and divide the whole column by it), so each group/column will add up to 1.\n\n\n\n\n  \n\n\n\nThese numbers will be my bar heights. So for the bar(s) representing brown eyes:\n\nblack hair will be .3091\nbrown hair will be .5409\nred hair will be 0.1182\nblond hair will be 0.0318\n\n\n\n\n\n\nHere’s the corresponding side-by-side bar plot. Note that the heights of the bars are the same as the segmented bar graph.\n\n\n\n\n\nOn the other hand, if I want my eye colour to be my groups, I would calculate the relative frequencies by row (use the total of the row and divide the whole row by it), so each group/row will add up to 1.\n\n\n\n\n  \n\n\n\nThese numbers will be my bar heights. So for the bar(s) representing black hair:\n\nbrown eyes will be 0.6296296\nblue eyes will be 0.1851852\nhazel eyes will be 0.1388889\ngreen eyes will be 0.0462963"
  },
  {
    "objectID": "categorical-displays.html#mosaic-plots",
    "href": "categorical-displays.html#mosaic-plots",
    "title": "3  Categorical Data Visualizations",
    "section": "3.3 Mosaic Plots",
    "text": "3.3 Mosaic Plots\n\n\n\n\n\n\nMosaic Plots\n\n\n\nMosaic plots are the almost the same as stacked bar plots. The only difference is that the widths of the bars change according to the proportion of points in each group. In a mosaic plot, the x-axis will also measure the proportion of observations/data points within the groupings (i.e. the x-axis reflects the marginal distribution of the variable on the x-axis).\n\n\nFollowing the same steps as the side-by-side and stacked bar charts to find the heights, we now add an additional step before plotting.\n\n\n\n\n\n\nTip\n\n\n\nFind the widths of the bars by finding the marginal distribution of the variable on the x-axis (the groups)\n\n\n\nFor each group, find the probability of having that trait. So for our previous example, we had this table:\n\n\n\n\n\n  \n\n\n\nUsing our eye colours as the groups (vertical bars), we will find:\n\n\\(P(Brown) \\approx .3716\\)\n\\(P(Blue) \\approx .3632\\)\n\\(P(Hazel) \\approx .1571\\)\n\\(P(Green) \\approx .1081\\)\n\nWhen we plot our mosaic plot, we do the same thing, except now, we have our bars differ in widths according to the numbers that we just calculated."
  },
  {
    "objectID": "quantitative-displays.html#dot-plots",
    "href": "quantitative-displays.html#dot-plots",
    "title": "4  Quantative Data Visualizations",
    "section": "4.1 Dot Plots",
    "text": "4.1 Dot Plots\n\n\n\n\n\n\nDot Plots\n\n\n\nDots Plots represent one quantitative variable by marking a dot for each value observed. They are mainly for discrete quantitative variables only and they are mainly useful in situations when you have a small range of number so that you can actually see how the data distribution varies across values.\n\n\nDot plots are simple, you draw a number line and then plot points above the number for each of the number that you see in the data.\nTake this data for example:\n\n\n\n\n  \n\n\n\nNow count up each value to figure out how many dots you need at each value on the number line then plot your graph"
  },
  {
    "objectID": "quantitative-displays.html#stemplots",
    "href": "quantitative-displays.html#stemplots",
    "title": "4  Quantative Data Visualizations",
    "section": "4.2 Stemplots",
    "text": "4.2 Stemplots\n\n\n\n\n\n\nStemplots\n\n\n\nIn a stem plot, you need to determine a common “stem” of all the numbers that you’re plotting. So if you have integer numbers from 10 to 200, your stems will be everything from the tens and so on, so you’ll have stems from 1-20. Once you take the stems, you just write the “leaves” next to the stem that they belong.\nThese are also known as stem-and-leaf plots.\n\n\nUsing this data as an example,\n\n\n\n\n  \n\n\n\nA stem plot looks like this:\n\n\n1 | 2: represents 12\n leaf unit: 1\n            n: 20\n    0 | 1\n    1 | 8\n    2 | \n    3 | 36689\n    4 | 4\n    5 | 2339\n    6 | 0228\n    7 | \n    8 | 19\n    9 | 35\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that you also have to add a key to show what a stem + leaf means. The stem and leaves give no information on the decimals in the data, so as you see above, you need to give an example like (as shown in the example stemplot):\nKey: 1\\|2 = 12\n\n\nHere’s another example (sorted for convenience)\n\n\n\n\n  \n\n\n\n1 | 2: represents 1.2\n leaf unit: 0.1\n            n: 50\n   3 | 3\n   3 | 579999\n   4 | 00334\n   4 | 556677788899\n   5 | 00112233444\n   5 | 667888999\n   6 | 234\n   6 | 5\n   7 | 12"
  },
  {
    "objectID": "quantitative-displays.html#boxplots",
    "href": "quantitative-displays.html#boxplots",
    "title": "4  Quantative Data Visualizations",
    "section": "4.3 Boxplots",
    "text": "4.3 Boxplots\n\n\n\n\n\n\nBoxplots\n\n\n\nBoxplots are primarily made of the five number summary of the data. The five number summary is made up of the:\n\nMinimum (min)\nFirst Quartile (\\(Q_1\\))\nMedian\nThird Quartile (\\(Q_3\\))\nMaximum (max)\n\nAlso known as a box-and-whisker plot.\n\n\nTo make a simple boxplot, you use the first quartile, median, and third quartile to make the “box” and then use the minimum and maximum to make the “whiskers.”\nFor this simple list of numbers:\n\n\n\n\n  \n\n\n\nOur five number summary is:\n\n\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n      1       2       4       6       7 \n\n\nAs detailed above, our box plot then looks like:\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe last detail is that we can calculate outliers using the 1.5 IQR rule and show them on the boxplot. For either direction (left or right), if we see outliers in that direction, we only extend the whisker to the smallest and/or largest point that is not an outlier. Then we plot any outliers as individual points.\n\n\nLook at this example data:\n\n\n\n\n  \n\n\n\nFive number summary:\n\n\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n    -12      -5      -3       0      12 \n\n\nOur numbers calculated by the 1.5 IQR rule are:\n\n\n[1] -12.5   7.5\n\n\nSo our 12 is an outlier. which means we draw our right whisker to 6 and plot the 12 individually on the number line. Like so:"
  },
  {
    "objectID": "quantitative-displays.html#histograms",
    "href": "quantitative-displays.html#histograms",
    "title": "4  Quantative Data Visualizations",
    "section": "4.4 Histograms",
    "text": "4.4 Histograms\n\n\n\n\n\n\nHistograms\n\n\n\nA histogram is similar to a bar plot, except that histograms are made for quantitative data and bars are continuous in the sense that there is no gap between bars. To make a histogram, select an appropriate equal intervals that make it so that you don’t have too many bars and that you don’t have too few bars.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYour goal with histograms, as with many other visualizations, is to be able to see the shape and characteristics of the distribution in question. If you have too many bars or too few bars, you won’t be able to see much important information (especially think of situations when you have many data points with very precise decimal measurements).\n\n\n\nDecide on your intervals (e.g. by 5’s, by 10’s, by 100’s)\nWithin your intervals, count up the number of observations that belong in that “bin”. When you do so, count up observations so that you count the left end inclusive and the right end inclusive. So if you did intervals of 5, you would do something like counting up points \\(0 \\leq x &lt; 5\\), \\(5 \\leq x &lt; 10\\), and so on.\nPlot your bars.\n\nExample:\nConsider this example data set:\nOur data has this set of summary statistics:\n\n\n       x        \n Min.   :1.522  \n 1st Qu.:1.912  \n Median :2.022  \n Mean   :2.110  \n 3rd Qu.:2.224  \n Max.   :2.704  \n\n\nWith this knowledge, let’s make our 7 “bins”, so let’s do these by every 0.2, starting at 1.5 to 2.9. This will be something that you build by intuition.\nNow, count up our values:\n\n\n[1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n        1         4         6         5         1         2         1 \n\n\nNow, we just put it together. For each bin, we have a bar and the bars’ heights correspond to the number of individuals in each bin.\n\n\n\n\n\nAgain, just like bar graphs, we can instead do the relative frequencies (this is what you’ll see most of the time!!!)\n\n\n[1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n     0.05      0.20      0.30      0.25      0.05      0.10      0.05 \n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you have a histogram like this, keep in mind that the bars always add up to 1 (or 100%)."
  },
  {
    "objectID": "sampling.html#bias",
    "href": "sampling.html#bias",
    "title": "5  Sampling",
    "section": "5.1 Bias",
    "text": "5.1 Bias\nWhen we collect data, there is the possibility of the data becoming systematically pushed towards a specific outcome. For example, if we want to learn about the GPA average in the school and take a sample of students only from a class, it’s quite possible that the sample is not representative of the school. We will probably result in a GPA average that is higher or lower than the actual GPA average in the school.\nThere are several ways that this can happen. Based off a scenario, you should be able to identify the bias that is occurring. The main ways that we learn when this occurs is when:\n\n\n\n\n\n\nResponse Bias\n\n\n\nResponse bias is created when something causes people’s responses to be systematically pushed in one direction.\n\n\n\n\n\n\n\n\nSelf-reported response bias\n\n\n\nWhen individuals inaccurately report their own traits, typically to avoid embarrassment or other situation where they feel more inclined to not tell the truth.\n\n\n\n\n\n\n\n\nQuestion wording bias\n\n\n\nWhen survey questions are confusing or leading to favor or disfavor certain answers from a respondent.\n\n\n\n\n\n\n\n\nNon-response Bias\n\n\n\nNonresponse occurs when an individual chosen for the sample can’t be contacted or refuses to participate.\n\n\n\n\n\n\n\n\nUndercoverage\n\n\n\nUndercoverage occurs when some members of the population cannot be chosen in the sample or specific individuals have a reduced chance of being chosen to be in the sample.\n\n\n\n\n\n\n\n\nVoluntary Response Sample\n\n\n\nVoluntary response samples consists of people who choose themselves by responding to general invitation, causing the chosen sample to not be representative of the population.\n\n\n\n\n\n\n\n\nConvenience Sample\n\n\n\nChoosing individuals from a population who are easy to reach results in a convenience sample."
  },
  {
    "objectID": "sampling.html#random-sampling-methods",
    "href": "sampling.html#random-sampling-methods",
    "title": "5  Sampling",
    "section": "5.2 Random Sampling Methods",
    "text": "5.2 Random Sampling Methods\nIt’s important to avoid bias in our data so that we have conclusive results. A big idea in avoiding bias is to have a proper sampling method. A good sampling method always keeps the population in mind. Your sample should be representative of the population, which means that each individual that is part of your population should have an equal chance of being selected for your sample.\n\n\n\n\n\n\nSimple Random Sample (SRS)\n\n\n\nIn a SRS, each individual (the words) and each subgroup of individuals has the same chance of being chosen from the population of all words in the song.\n\n\n\n\n\n\n\n\nStratified Random Sample\n\n\n\nIn a stratified random sample, we first determine strata within our population. You can think of strata as subgroups of people that we divide based on the type/status of the things that we are sampling. The goal of the random sample is then to take an SRS from each strata (normally, you want the same amount of people from each stratum).\n\n\n\n\n\n\n\n\nRandom Cluster Sample\n\n\n\nIn a random cluster sample, we first determine clusters within our population. You can think of clusters as subgroups of people that we divide based on the location of the things that we are sampling (there should be no way to distinguish between the clusters besides where they are). The goal of the random sample is then to take select random clusters and then sample all people in the chosen clusters.\n\n\n\n\n\n\n\n\nSystematic Random Sample\n\n\n\nIn a systematic random sample, we randomly choose an interval (n) and/or a starting point at which to select individuals and then we select every nth individual.\n\n\n\n5.2.1 Box analogy\nAlthough not as descriptive as the above definitions, here’s how you can think about each sampling method:\nImagine that you have a box of balls, all of different weights and different colors (red, white, blue). Balls of similar colors tend to be more similar in weight compared to balls of different colors. We want to study the weights of the balls by randomly selecting some balls.\n\nWe can take a SRS from this box of balls by randomly picking out some balls. Use these randomly picked out balls as your sample.\nWe can take a Stratified Random Sample by first separating the balls into 3 separate boxes. One box with just the red balls, another with just the white balls, and a last one with just the blue balls. Lastly, take an SRS from each of the boxes by randomly taking out some balls from each of the boxes. Use these randomly picked out balls as your sample.\nWe can take a Cluster Sample by separating the balls into several different boxes. We determine out boxes by just taking maybe picking out 10 balls from the box (just take balls out top to bottom) and placing 10 balls in each box. Or we could dump out the balls and just section off the balls into different boxes. Take an SRS of the boxes by randomly selecting some boxes. Take out all of the balls from the boxes that you selected to be part of your sample.\nWe can take a Systematic Random Sample by taking out all the balls in the box and lining them up. Randomly pick one of the balls. From that ball onwards, pick every 5th ball to be part of your sample.\n\n\n\n5.2.2 Benefits of Methods\nAn SRS is an overall good method, giving us a random selection of multiple trees, but we can’t always trust it because it’s just left up to chance. There’s a chance that we select only individuals of a certain type, which is bad since we only have one chance to sample in real life.\nTo guarantee a good spread of people from all over, we would normally prefer a Systematic Random Sample, because by counting people off, you’ll be almost ensuring that you get a random spread of people from all of the population.\nIf we know that there are specific types of people within our population, then a better idea is to take a Stratified Random Sample. By first splitting people off in strata (again, this people types of people), we then are more capable of getting a good spread of people from each type of person.\nLastly, if all we want to do is save as much money and resources as possible, then we do a Random Cluster Sample, which does not take as much time and effort compared to the rest of the sampling methods."
  },
  {
    "objectID": "sampling.html#describing-sampling",
    "href": "sampling.html#describing-sampling",
    "title": "5  Sampling",
    "section": "5.3 Describing Sampling",
    "text": "5.3 Describing Sampling\n\n\n\n\n\n\nTip\n\n\n\nHere’s a general framework for how describing SRS’s should look like:\n\nAssign each individual in the population a number \\(1\\) to \\(N\\) (the population size).\nUse a random number generator to obtain \\(n\\) (sample size) unique numbers.\nSample the individuals whose numbers were generated.\n\n\n\nHere’s some examples of how this can work.\n\nImagine we have a forest of 1000 trees and I want to find out how old they are on average (and we don’t have the resources to visit each tree)\n\nTo take an SRS of 50 trees, we can\n\n\nNumber each tree with a number 1 to 1000.\nGenerate 50 unique random integers from 1 to 1000.\nSelect the trees corresponding to the numbers and record their age.\n\n\nTo take a random cluster sample of 50 trees, we can\n\n\nUse a map to split up the trees by plots of land (assume that each plot has 5 trees.\nNumber each plot of land with a number 1 to 200.\nGenerate 10 unique random integers from 1 to 200.\nSelect the plots corresponding to the numbers and record all the ages from the trees in the plots."
  },
  {
    "objectID": "experimental-design.html#experiment-principles",
    "href": "experimental-design.html#experiment-principles",
    "title": "6  Experimental Design",
    "section": "6.1 Experiment Principles",
    "text": "6.1 Experiment Principles\nIn general, the quality of experiments (their internal validity) can be judged based on the degree to which they have four things: comparison, randomization, control, and replication. Stronger internal validity gives us a better cause-effect link in our experiment. Whenever you are describing or evaluating the design of an experiment, you need to be sure to discuss all four of these!\n\n\n\n\n\n\nPrinciples of Experimental Design\n\n\n\nThese four determine how much internal validity we have in our experiment.\n\nComparison\n\nUse a design that compares two or more treatments.\n\nRandomization\n\nUse chance to assign experimental units to treatments. Doing so helps create roughly equivalent groups of experimental units by balancing the effects of other variables among treatment groups.\n\nControl\n\nKeep other variables that might affect the response the same for all groups.\n\nReplication\n\nUse enough experimental units in each group so that difference in the effects of the treatments can be distinguished from chance differences between the groups.\n\n\n\n\nThe logic of a randomized comparative experiment depends on our ability to treat all the subjects the same in every way except for the actual treatments being compared. Good experiments, therefore, require careful attention to details to ensure that all subjects really are treated identically.\n\n6.1.1 Placebos\n\n\n\n\n\n\nPlacebo Effect\n\n\n\nThe response to a dummy treatment is called the placebo effect. Subjects are given a placebo treatment to control for the placebo effect.\n\nFor example, If I tell someone that I am giving them an energy drink (when it in fact has doesn’t actually provide “energy”), and they feel like they have energy after, they have fallen for the placebo effect.\n\n\n\nIt’s well known that someone’s mental state can easily affect their physical state, so it’s important to control for the placebo effect. Typically, this applies to medicine settings, where you might give a pill with the actual medicine and a placebo (a pill with everything but the actual medicine) and conduct it in a blind.\n\n\n\n\n\n\nBlinding\n\n\n\nConducting an experiment in a blind means that you give treatments to patients without allowing them to know which treatment they are taking.\nHowever, Whenever possible, experiments with human subjects take it a bit further and conduct their experiment in a double-blind, where neither the subjects nor those who interact with them and measure the response variable know which treatment a subject received."
  },
  {
    "objectID": "experimental-design.html#experiment-designs",
    "href": "experimental-design.html#experiment-designs",
    "title": "6  Experimental Design",
    "section": "6.2 Experiment Designs",
    "text": "6.2 Experiment Designs\n\n6.2.1 Completely Randomized Design\nIn a completely randomized design, the experimental units are assigned to the treatments completely by chance. This is similar to (but NOT the same as) a simple random sample (SRS), because in both cases we ignore other variables. Here’s the difference: In an SRS, we’re picking some people (our sample) to study, and ignoring the rest. In a completely randomized experiment, however, we already have our sample (the people in our experiment), and we’re randomly deciding how we’re going to study each person (or, which treatment they’re going to get). So in complete randomization, the randomization is in the assignment, not in the selection, of people in our study.\n\n\n6.2.2 Randomized Block Design\nIn a randomized block design, the experimental units are first assigned to blocks according to the different types/status of the experimental units in the experiment. This is similar to stratified random sampling, however, we are not taking a sample. Any reference to stratified random sampling is wrong when describing an experiment design.\nAfter each experimental unit is assigned to their block, the experiment is carried out in each block, where a completely randomized design is carried out within the block.\nAfterwards, you compare and analyze results from each block and finally combine all results and analyze the differences between blocks.\n\n\n6.2.3 Matched Pairs Design\nA matched pairs design is a special case of a randomized block design that uses blocks of size 2. In this kind of design, you have to have “matched pairs.” In other words, you need to have two extremely similar individuals that make up each block. In some cases, you have a single person for each block and that person recieves both treatments in randomized order (because who is more similar to a person than themselves?)."
  },
  {
    "objectID": "experimental-design.html#inference",
    "href": "experimental-design.html#inference",
    "title": "6  Experimental Design",
    "section": "6.3 Inference",
    "text": "6.3 Inference\nThe main purpose of experiments is to be able to infer something about what we did. Does A actually affect B? Is it true for anyone else other than the people we experimented on?\n\n\n\n\n\n\nStatistical Significance\n\n\n\nAn observed effect so large that it would rarely occur by chance is said to be statistically significant. If we test something according to a single assumption that we make and find out that the data that we collect doesn’t really match up with that claim (if the chance of seeing data like the one we obtained is too low), then we’d say that it is statistically significant evidence.\n\n\n\n6.3.1 Scope of Inference\n\n\n\n\n\n\nScope of Inference\n\n\n\nThe scope of inference refers to the type of inferences (conclusions) that can be drawn from a study. The types of inferences we can make (inferences about the population and inferences about cause-and-effect) are determined by two factors in the design of the study.\n\n\n\n\n\n\n\n\n\n\nWere individuals randomly assigned to groups?\n\n\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nWere individuals randomly selected from a population?\n\n\nYes\n\n\nCan make inferences about the populationCan make inferences about cause and effect (Rare in the real world)\n\n\nCan make inferences about the populationCannot make inferences about cause and effect (Some observational studies)\n\n\n\n\nNo\n\n\nCannot make inferences about the populationCan make Inferences about cause and effect:   (Most experiments)\n\n\nCannot make inferences about the populationCannot make Inferences about cause and effect (Some observational studies)"
  },
  {
    "objectID": "probability.html#probability-rules",
    "href": "probability.html#probability-rules",
    "title": "7  Probability",
    "section": "7.1 Probability Rules",
    "text": "7.1 Probability Rules\nThe sample space \\(\\Omega\\) of a chance process is the set of all possible outcomes.\nA probability model is a description of some chance process that consists of two parts: a sample space \\(\\Omega\\) and a probability for each outcome.\nAn event is any collection of outcomes from some chance process. That is, an event is a subset of the sample space. Events are usually designated by capital letters, like \\(A\\), \\(B\\), \\(C\\), and so on.\n\n\n\n\n\n\nProbability Model\n\n\n\n\nFor any event A, \\(0 \\leq P(A) \\leq 1\\)\nIf \\(\\Omega\\) is the sample space in the probability model, \\(P(\\Omega)=1\\)\nIn the case of equally likely outcomes, \\(P(A)=\\frac{\\text{number of outcomes corresponding to event A}}{\\text{total number of outcomes in sample space}}\\)\n\n\n\nWe also have the following rules to calculate other things:\n\\(P(A|B)\\) : the probability of event \\(A\\) occurring, given that event \\(B\\) has already occurred. (conditional probability)\n\n\n\n\n\n\nIndependent Events\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if the occurrence of one has no effect on the likelihood of the other occurring (like successive coin flips).\nFor independent events, \\[P(A)=P(A|B) \\text{ and } P(B)=P(B|A)\\]\n\n\n\n\n\n\n\n\nMutually Exclusive (or Disjoint)\n\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive if it’s impossible for both to occur. In one flip of a coin, heads and tails are mutually exclusive events.\nFor mutually exclusive events,\n\\[P(A\\cap B)=0\\]\n\n\n\n7.1.1 Complement Rule\nThe complement to event \\(A\\) is the event that \\(A\\) doesn’t happen. This event is sometimes written \\(AC\\). In one flip of a coin, heads and tails are complementary events (assuming the coin won’t land on its side).\n\n\n\n\n\n\nComplementary Rule\n\n\n\nFor complementary events,\n\\[P(A)+P(A^c)=1 \\text{ or } P(A^c)=1-P(A)\\]\n\n\n\n\n7.1.2 The Addition Rule\n\n\n\n\n\n\nAddition Rule\n\n\n\n\\[P(A \\cup B)=P(A) + P(B)-P(A \\cap B)\\]\nFor mutually exclusive events, we know that \\(P(A\\cap B)=0\\), so we can also have the following simplified rule:\n\\[P(A \\cup B)=P(A) + P(B)\\]\n\n\n\n\n7.1.3 The Multiplication Rule\n\n\n\n\n\n\nMultiplication Rule\n\n\n\n\\[P(A\\cap B)=P(A)\\cdot P(B|A)\\]\nFor independent events \\(A\\) and \\(B\\), we know that \\(P(B|A) = P(B)\\), so we can also have the simplified rule:\n\\[P(A\\cap B)=P(A)\\cdot P(B)\\]"
  },
  {
    "objectID": "density-curves.html#percentiles",
    "href": "density-curves.html#percentiles",
    "title": "8  Density Curves",
    "section": "8.1 Percentiles",
    "text": "8.1 Percentiles\n\n\n\n\n\n\nPercentile\n\n\n\nThe \\(p^{th}\\) percentile is represented by a given value. \\(p\\)% of values are less than or equal to the given value."
  },
  {
    "objectID": "density-curves.html#cumulative-relative-frequency-graphs",
    "href": "density-curves.html#cumulative-relative-frequency-graphs",
    "title": "8  Density Curves",
    "section": "8.2 Cumulative Relative Frequency Graphs",
    "text": "8.2 Cumulative Relative Frequency Graphs\n\n\n\n\n\n\nCumulative Relative Frequency Graphs\n\n\n\nCumulative Relative Frequency graphs represent the cumulative relative frequency as the values increase (what percentage of values are less than or equal to each x-value). These always range from 0 to 1 on the y-axis (or 0% to 100%)."
  },
  {
    "objectID": "density-curves.html#normal-distributions",
    "href": "density-curves.html#normal-distributions",
    "title": "8  Density Curves",
    "section": "8.3 Normal Distributions",
    "text": "8.3 Normal Distributions\nIn the early 19th Century, the mathematicians Gauss and Adrian both noticed that many, many natural processes (like dimensions and weights of plants and animals) and many human processes (manufacturing, scores on tests) follow (approximately, not exactly) a very nice bell-shaped curve that Abraham de Moivre, an 18th century statistician and consultant to gamblers, had developed. This bell-shaped curve is now called the normal curve, and has the equation\n\\[P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\]\nWhere \\(P(x)\\) represents the height of the curve, \\(\\mu\\) is the mean (of all the \\(x\\)’s), and \\(\\sigma\\) is the standard deviation. If the mean is 0 and the standard deviation is 1, the graph looks like this:\n\n\n\n\n\n\n\n\n\n\n\nInflection Points\n\n\n\nNotice that the inflection points (where the curve changes its concavity, like in a cubic function) are at exactly one standard deviation from the mean, and by the time the graph gets to 3 standard deviationss in each direction, the height of the curve is basically zero.\n\n\n\n\n\n\n\n\nCheck Your Conditions\n\n\n\nThis is called the Standard Normal Distribution, or \\(N(0,1)\\). Remember that we use this to approximate real life, and we always need to check that this approximation is appropriate in a given situation.\n\n\n\n8.3.1 Characteristics of the Normal Distribution\n\n\n\n\n\n\nNormal Distribution Characteristics\n\n\n\n\nThey are always unimodal, bell-shaped, and symmetric about the mean\nThe x-axis represents different values of the variable, and the probability of any single value of x is zero (not true in real-life situations, but remember that this is an approximation).\nThe probability of a range of x-values is shown on the graph by the area under the curve for that range. The shaded area in the Standard Normal graph above represents the probability that \\(x\\) will be at least 1.5 standard deviations below the mean (about 10%).\nThe total area under the curve is 1, or 100%\nThe graph can be narrow or broad, depending on the standard deviation, but never skewed.\n\n\n\n\n\n8.3.2 Empirical Rule\n\n\n\n\n\n\nEmpirical Rule (68-95-99.7 Rule)\n\n\n\n\nabout 68% of the distribution is within 1 standard deviation of the mean\nabout 95% of the distribution is within 2 standard deviation of the mean\nabout 99.7% of the distribution is within 3 standard deviation of the mean\n\n\n\n\n\n\n\n\n\n\n8.3.3 Assessing for Normality\nWhen assessing normality, you have to think about what makes a normal distribution…. a normal distribution. Refer to the characteristics of a normal distribution to remind yourself what you need to look for.\n\n\n\n\n\n\nChecking Normality…\n\n\n\n\nGraph your data using a dot plot, histogram, boxplot, or stem plot. Whichever one is most convenient and would show the distribution of data the best.\nCompare the mean and median of the distribution. Normal curves are symmetric, so our data should also be symmetric. Are the mean and median almost the same?\nCheck to see if the data follows the empirical rule. Is there 68% of the data between -1 and 1 standard deviations? What about 95% and 99.7%?\nLook at the normal probability plot if given. (Know how to interpret and read, not make). If the plot is fairly linear (does a straight line fit through the points), then we have further evidence that the distribution is approximately normal\n\n\n\n\n\n8.3.4 Standardized Score\n\n\n\n\n\n\nz-score\n\n\n\nIf \\(x\\) is an observation that has a known mean and standard deviation, the standardized score of \\(x\\) is: \\[z = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}\\] A standardized score, often called a z-score, of an observation is the number of standard deviations it is from the mean.\n\n\n\n\n8.3.5 Calculating Probabilities and Percentiles\nUsing calculus, we could calculate the area (and therefore the probability) under any section of any normal distribution. But we don’t want to have to use calculus every time we want to calculate a normal probability. So, we use a table (inside the front cover of your book and also in Table A) in which many Standard Normal probabilities have been calculated for us. So all we need to do is convert our problem to Standard Normal, and we can use this table. We make this conversion using z-scores.\n\n\n\n\n\n\nImportant\n\n\n\nOnce we have a z-score, we can use the Standard Normal table to find a corresponding probability.\n\n\n\n\n8.3.6 Using your Calculator\nRemember that to access the calculator functions related to distributions on a TI-83/84 series, you go to:\n2nd -&gt; vars (distr)\nThe two functions of interest for this chapter are:\nnormalcdf(lowerbound, upperbound [, mu, sigma])\n\ninvNorm(area to the left of value [, mu, sigma])\nWhen using your calculator to calculate Normal probabilities you need to make sure you write at least the following information to ensure full credit:\n\nThe name of the distribution\nThe parameters of the distribution (for normal distributions, the mean and s.d.)\nHow to calculate the test statistic (for normal distributions, the z-score)\nThe probability you are finding\nThe answer in context of the problem\n\n\nExample\nSuppose that the time you need to spend on BART to get from Downtown Berkeley to the SF Airport is approximately normally distributed with \\(\\mu = 54\\) minutes, and \\(\\sigma = 4.6\\) minutes. In other words, BART times from Berkeley to SFO \\(\\sim N(54, 4.6)\\).\nNow find each probability (\\(X\\) represents the amount of time required for a single randomly selected BART trip from Berkeley to SFO):\n\n\\(P(X&lt;53)\\) \\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X&lt;53) &\\approx P(z &lt; -.22) \\underset{\\text{Table A}}{=} 0.4129\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X&lt;53) &\\approx P(z &lt; -0.22) \\\\\n&= \\texttt{normalcdf(-1000, -0.22)} \\approx 0.4129\n\\end{aligned}\n\\]\n\n\n\\(P(X&gt;60)\\) \\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X&gt;60) &\\approx P(z &gt; 1.30) \\\\\n&= 1 - P(z &lt; 1.30) \\underset{\\text{Table A}}{=} 1 - 0.9032  = 0.0968\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X&gt;60) &\\approx P(z &gt; 1.30) \\\\\n&= \\texttt{normalcdf(1.30, 1000)} \\approx 0.0968\n\\end{aligned}\n\\]\nThe lowest 5% of travel times are below how many minutes? \\[\n\\begin{aligned}\n0.05 & \\underset{\\text{Table A}}{\\approx} P(z&lt;-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\n\\texttt{invNorm(0.05)}& \\approx P(z&lt;-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "random-variables.html#discrete-random-variables",
    "href": "random-variables.html#discrete-random-variables",
    "title": "9  Random Variables",
    "section": "9.1 Discrete Random Variables",
    "text": "9.1 Discrete Random Variables\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA discrete random variable describes a process that only has specific, predefined numerical outcomess. For example, we can use a discrete random variable to finding the probability of Michael Jordon scoring 50 points in a single game or the probability of rolling a 3 or lower on a six-sided die.\n\n\n\n\n\n\n\n\nMean and SD of Discrete Random Variable\n\n\n\nWhen we have a discrete random variable \\(X\\) whose probability distribution is\n\\[\n\\begin{aligned}\n    \\textbf{Value:}& ~~~~ x_1 ~~~~ x_2 ~~~~ x_3 ~~~~ \\cdots \\\\\n    \\textbf{Probability:}& ~~~~ p_1 ~~~~ p_2 ~~~~ p_3 ~~~~\\cdots\n\\end{aligned}\n\\]\nwe know the following about the mean and standard deviation of \\(X\\):\n\\[\\mu_X = E(X) = \\sum x_i P(x_i)\\]\n\\[\\sigma_X = \\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\]\n\n\n\n\nKeep in mind that the standard deviation formula here (and else where) is equivalent to the formula of population standard deviation when we divide by \\(n\\) instead of multiplying by \\(P(x_i)\\).\n\nJust as a quick reminder, our formula for sample standard deviation is:\n\\[s_X = \\sqrt{\\frac{\\sum(x_i - \\bar x)^2}{n - 1}}\\]\nOur formula for population standard deviation (which we won’t use in this class is)\n\\[\\sigma_X = \\sqrt{\\frac{\\sum(x_i - \\mu_X)^2}{n}}\\]\nTaking this formula, we can notice that:\n\\[\n\\begin{aligned}\n\\sigma_X &= \\sqrt{\\frac{\\sum(x_i - \\mu_X)^2}{n}} &(1)\\\\\n&=\\sqrt{E((X - \\mu_X)^2)} &(2)\\\\\n&=\\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)} &(3)\\\\\n\\end{aligned}\n\\]\nIn other words, remember that we define standard deviation as the root mean of the squared differences from the mean.\nWe get to step (2) by recognizing that by adding up all the squared differences from the mean and dividing by \\(n\\), we are taking the mean of the squared differences. Therefore, \\(\\frac{\\sum(x_i - \\mu_X)^2}{n} = E((X - \\mu_X)^2)\\)\nAnd in the case of discrete random variables, we can find the mean of \\(X\\) as \\(\\mu_X = E(X) = \\sum x_iP(x_i)\\) which allows us to rewrite from step (2) to (3)\n\n\n9.1.1 Binomial Random Variables\nBinomial random variables have parameters \\(n\\) and \\(p\\), and can be written \\(B(n, p)\\). Remember, Normal random variables have parameters and and can be written \\(N(\\mu,\\sigma)\\).\n\n\n\n\n\n\npdf of a Binomial Random Variable\n\n\n\nThe pdf of a Binomial Random Variable (i.e. the binomial formula) is:\n\\[\n\\begin{aligned}\n    P(X=k) &= {n \\choose k} p^k (1-p)^{n-k}\\\\\n    \\text{where } k &= 0, 1, 2, 3, \\cdots, n\n\\end{aligned}\n\\]\nTo apply this formula in a graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; binompdf}\\)\nUsage: \\(\\texttt{binompdf(n, p[,x])}\\)\n\n\n\n\n\n\n\n\ncdf of a Binomial Random Variable\n\n\n\nThe cdf of a Binomial Random Variable is: \\[\n\\begin{aligned}\n    P(X\\leq k) &= \\sum_{i = 0}^n {n \\choose i} p^i (1-p)^{n-i}\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; binomcdf}\\)\nUsage: \\(\\texttt{binomcdf(n, p[,x])}\\)\n\n\n\n\n\n\n\n\nMean and SD of Binomial Random Variable\n\n\n\nThe mean and standard deviation of a binomial random variable is given by:\n\\[\n\\begin{aligned}\n    \\mu_X &= n p \\\\\n    \\sigma_X &= \\sqrt{n p q} \\\\\n    &\\text{, where } q = 1-p.\n\\end{aligned}\n\\]\n\n\n\nIdentifying Binomial Situations\n\n\n\n\n\n\nThe Binomial Setting\n\n\n\nWe can identify a binomial setting when we successfully recognize these four things about a random phenonmenon:\n\nBinary?\nThe possible outcomes of each trial can be classified as “success” or “failure”.\nIndependent?\nTrials must be independent; that is, knowing the result of one trial must not tells us anything about the result of another trial.\nNumber?\nThe number of trials n of the chance process must be fixed in advance.\nSame?\nThere is the same probability p of success on each trial.\n\n\n\n\n10% Condition\nThe second condition is often not perfectly met, as in the case of an SRS from some population. Imagine choosing 10 students from a class of 15 females and 15 males—as we choose people, the remaining population changes, which changes the probability that the next person chosen will be male or female.\nWhen we lack complete independence, we can see the consequence of this is negligible as long as our sample is small relative to the population from which we are sampling. If we were choosing our 10 people from a school of 3,300 students, the change in probability from person to person would be small enough to ignore.\n\n\n\n\n\n\n10% Condition\n\n\n\nThe general rule is that the sample needs to be less than \\(\\frac{1}{10}\\), or 10%, of the population. We refer to this as the 10% condition.\n\\[n \\leq (.10) N\\]\n\n\n\n\n\nNormal Approximation to the Binomial Distribution\nRemember that as \\(n\\) gets large, a binomial random variable \\(X\\) can take on more and more different values, and it can become tedious to continue to treat X as a discrete random variable. As \\(n\\) get larger, we can treat \\(X\\) as a continuous random variable, more specifically:\n\n\n\n\n\n\nImportant\n\n\n\nAs \\(n\\) gets larger, the binomial distribution gets closer to a normal distribution.\n\n\nHowever, before we use a normal distribution to approximate a binomial distribution, we have to check the following condition:\n\nLarge Counts condition\n\n\n\n\n\n\nLarge Counts Condition\n\n\n\nWe can use a Normal distribution to model a binomial distribution if we know that a the Large Counts Condition is fulfilled: \\[np \\geq 10 \\text{ and }n(1-p) \\geq 10\\] In other words, if the expected number of successes and failures (respectively) is greater than or equal to 10.\n\n\n\n\nDoing a Normal Approximation to a Binomial Distribution\nFirst verify all the conditions for a Binomial setting and the Large Counts Condition. Since we know that we have a binomial setting, we then know the distribution that we want to use is \\(Normal(np, \\sqrt{npq})\\) proceed with the calculations according to this distribution.\n\n\n\n\n9.1.2 Geometric Random Variables\nIf \\(X \\sim G(p)\\), \\(X\\) has a geometric distribution with a parameter \\(p\\) probability of “success”.\n\n\n\n\n\n\npdf of a Geometric Random Variable\n\n\n\nThe pdf of a geometric random variable is:\n\\[\n\\begin{aligned}\n    P(X=x) &= (1-p)^{x-1}p\\\\\n    \\text{where } x &= 1, 2, 3, \\cdots\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; geometpdf}\\)\nUsage: \\(\\texttt{geometpdf(p, x)}\\)\n\n\n\n\n\n\n\n\ncdf of a Geometric Variable\n\n\n\nThe cdf of a Geometric Variable is:\n\\[\n\\begin{aligned}\n    P(X\\leq x) &= \\sum_{i=1}^x(1-p)^{i-1}p\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; geometcdf}\\)\nUsage: \\(\\texttt{geometcdf(p, x)}\\)\n\n\n\n\n\n\n\n\nMean and SD of a Geometric Random Variable\n\n\n\nThe mean and standard deviation of a geometric random variable is given by:\n\\[\n\\begin{aligned}\n    \\mu_X &= \\frac{1}{p} \\\\\n    \\sigma_X &= \\frac{\\sqrt{q}}{p} \\\\\n    &\\text{, where } q = 1-p.\n\\end{aligned}\n\\]\n\n\n\nIdentifying Geometric Settings\nA geometric setting is very similar to a binomial setting, except that \\(n\\) the number of trials is not fixed and that we are instead finding probabilities until the first “success” occurs.\n\n\n\n\n\n\nThe Geometric Setting\n\n\n\nA geometric setting is defined as a series of observations where these 4 conditions are met:\n\nBinary?\nThe possible outcomes of each trial can be classified as “success” or “failure”\nIndependent?\nTrials must be independent, that is, knowing the result of one trial must not have any effect on the result of any other trial.\nTrials until?\nThe goal is to count the number of trials until the first success occurs.\nSuccess?\nOn each trial, the probability \\(p\\) of success must be the same."
  },
  {
    "objectID": "random-variables.html#operations-with-random-variables",
    "href": "random-variables.html#operations-with-random-variables",
    "title": "9  Random Variables",
    "section": "9.2 Operations with Random Variables",
    "text": "9.2 Operations with Random Variables\n\n9.2.1 Linear Transformations\nWhen we add a constant \\(a\\) and/or multiply by a constant \\(b\\) to a random variable \\(X\\), we perform a linear transformation of the form\n\\[\na + bX\n\\]\n\n\n\n\n\n\nMean and SD of Linearly Transformed Random Variables\n\n\n\nThe mean of \\(a + bX\\) is transformed in the same way that we change \\(X\\)\n\\[\n\\mu_{a+bX}=a+\\mu_{bX}=a+b\\mu_X\n\\]\nThe standard deviation of \\(a + bX\\) is only affected by multiplication:\n\\[\n\\sigma_{a+bX}=\\sigma_{bX}=b\\sigma_X\n\\]\n\n\n\n\n9.2.2 Sum and Difference between Random Variables\nIn addition to linear transformation, we can also, to an extent, describe the result of adding or subtracting random variables\nIf we have two random variables \\(X\\) and \\(Y\\), we can describe the mean and standard deviation of the sum or difference of these with the following.\n\n\n\n\n\n\nMean and SD of the Sum and Difference between Random Variables\n\n\n\n\\[\\mu_{X \\pm Y} = \\mu_X \\pm \\mu_Y\\]\nIf the random variables \\(X\\) and \\(Y\\) are independent, then\n\\[ \\sigma_{X \\pm Y}^2 = \\sigma_X^2 + \\sigma_Y^2 \\]\nAnd it follows that:\n\\[ \\sigma_{X \\pm Y} = \\sqrt{\\sigma_X^2 + \\sigma_Y^2} \\]\n\n\nWhat is \\(\\sigma_X^2\\)?\n\nThe variance of a random variable \\(X\\) is:\n\\[\nVar(X) = \\sigma_X^2\n\\]\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that the formula for the standard deviation here only applies if we know that \\(X\\) and \\(Y\\) are independent of each other."
  },
  {
    "objectID": "formula-sheet.html#descriptive-statistics",
    "href": "formula-sheet.html#descriptive-statistics",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.1 Descriptive Statistics",
    "text": "A.1 Descriptive Statistics\n\\[\n\\begin{aligned}\n\\bar x &= \\frac{1}{n}\\sum x_i = \\frac{\\sum x_i}{n} \\\\\\\\\ns_x &= \\sqrt{\\frac{1}{n-1}\\sum (x_i - \\bar x)^2} = \\sqrt{\\frac{\\sum(x_i -\\bar x)^2}{n - 1}}\\\\\\\\\n\\hat y &= a + bx \\\\\\\\\n\\bar y &= a + b \\bar x\\\\\\\\\nr &= \\frac{1}{n-1}\\sum \\left( \\frac{x_i - \\bar x}{s_x}\\right) \\left( \\frac{y_i -\\bar y}{s_y}\\right)\\\\\\\\\nb &= r \\frac{s_y}{s_x}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "formula-sheet.html#probability-and-distributions",
    "href": "formula-sheet.html#probability-and-distributions",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.2 Probability and Distributions",
    "text": "A.2 Probability and Distributions\n\\[\n\\begin{aligned}\nP(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\\\\\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nProbability Distribution\nMean\nStandard Deviation\n\n\n\n\nDiscrete random variable, \\(X\\)\n\\(\\mu_X = E(X) = \\sum x_i P(x_i)\\)\n\\(\\sigma_X = \\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\)\n\n\nIf \\(X\\) has a binomial distribution with parameters \\(n\\) and \\(p\\), then: \\[P(X = x) = {n \\choose x} p^x (1 - p)^{n - x}\\] where \\(x = 0, 1, 2, 3, ..., n\\)\n\\(\\mu_X = np\\)\n\\(\\sigma_X = \\sqrt{np(1-p)}\\)\n\n\nIf \\(X\\) has a geometric distribution with parameter \\(p\\), then: \\[P(X = x) = (1 - p)^{x - 1} p\\] where \\(x = 1, 2, 3, ...\\)\n\\(\\mu_X = \\frac{1}{p}\\)\n\\(\\sigma_X = \\frac{\\sqrt{1-p}}{p}\\)"
  },
  {
    "objectID": "formula-sheet.html#sampling-distributions-and-inferential-statistics",
    "href": "formula-sheet.html#sampling-distributions-and-inferential-statistics",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.3 Sampling Distributions and Inferential Statistics",
    "text": "A.3 Sampling Distributions and Inferential Statistics\n\\[\\text{Standardized test statistic:}~~~~ \\frac{\\text{statistic} - \\text{parameter}} {\\text{standard error of the statistic}}\\]\n\n\\[\\text{Confidence interval:}~~~~ \\text{statistic} \\pm (\\text{critical value}) (\\text{standard error of statistic})\\]\n\n\\[\\text{Chi-square statistic:}~~~~ \\chi^2 = \\sum \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nA.3.1 Sampling distributions for proportions:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor one population: \\(\\hat p\\)\n\\(\\mu_{\\hat p} = p\\)\n\\(\\sigma_{\\hat p} = \\sqrt{\\frac{p(1-p)}{n}}\\)\n\\(s_{\\hat p} = \\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\)\n\n\nFor two populations:\\(\\hat p_1 - \\hat p_2\\)\n\\(\\mu_{\\hat p_1 - \\hat p_2} = p_1 - p_2\\)\n\\(\\sigma_{\\hat p_1 - \\hat p_2}= \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\)\n\\(s_{\\hat p_1 - \\hat p_2}= \\sqrt{\\frac{\\hat p_1(1- \\hat p_1)}{n_1} + \\frac{\\hat p_2(1- \\hat p_2)}{n_2}}\\)When \\(p_1 = p_2\\) is assumed:\\(s_{\\hat p_1 -\\hat p_2} = \\sqrt{\\hat p_c (1 - \\hat p_c) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\)where \\(\\hat p_c = \\frac{X_1 + X_2}{n_1 + n_2}\\)\n\n\n\n\n\nA.3.2 Sampling distributions for means:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor one population: \\(\\overline{X}\\)\n\\(\\mu_{\\overline X} = \\mu\\)\n\\(\\sigma_{\\overline X} = \\frac{\\sigma}{\\sqrt n}\\)\n\\(s_{\\overline X} = \\frac{s}{\\sqrt n}\\)\n\n\nFor two populations:\\(\\overline{X_1} - \\overline{X_2}\\)\n\\(\\mu_{\\overline{X_1} - \\overline{X_2}} = \\mu_1 - \\mu_2\\)\n\\(\\sigma_{\\overline{X_1} - \\overline{X_2}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\\(s_{\\overline{X_1} - \\overline{X_2}} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\)\n\n\n\n\n\nA.3.3 Sampling distributions for simple linear regression:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor slope:\\(b\\)\n\\(\\mu_b = \\beta\\)\n\\(\\sigma_{b} = \\frac{\\sigma}{\\sigma_x \\sqrt n}\\),where \\(\\sigma_x = \\sqrt{\\frac{\\sum (x_i - \\bar x)^2}{n}}\\)\n\\(s_{b} = \\frac{s}{s_x \\sqrt{n-1}}\\) ,where \\(s = \\sqrt{\\frac{\\sum (y_i - \\hat y_i)^2)}{n - 2}}\\)and \\(s_x = \\sqrt{\\frac{\\sum (x_i - \\bar x)^2}{n - 1}}\\)\n\n\n\n*Standard deviation is a measurement of variability from the theoretical population. Standard error is the estimate of the standard deviation. If the standard deviation of the statistic is assumed to be known, then the standard deviation should be used instead of the standard error."
  },
  {
    "objectID": "formula-sheet.html#table-a",
    "href": "formula-sheet.html#table-a",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.4 Table A: Standard normal probabilities",
    "text": "A.4 Table A: Standard normal probabilities\n\n\n\n\n\n\n\nClick for table of standard normal probabilities\n\nTable entry for \\(z\\) is the probability lying below \\(z\\).\n\n\n\n\n\n\n\\(z\\)\n\n\n0.00\n\n\n0.01\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.05\n\n\n0.06\n\n\n0.07\n\n\n0.08\n\n\n0.09\n\n\n\n\n\n\n-3.4\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0002\n\n\n\n\n-3.3\n\n\n0.0005\n\n\n0.0005\n\n\n0.0005\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0003\n\n\n\n\n-3.2\n\n\n0.0007\n\n\n0.0007\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0005\n\n\n0.0005\n\n\n0.0005\n\n\n\n\n-3.1\n\n\n0.0010\n\n\n0.0009\n\n\n0.0009\n\n\n0.0009\n\n\n0.0008\n\n\n0.0008\n\n\n0.0008\n\n\n0.0008\n\n\n0.0007\n\n\n0.0007\n\n\n\n\n-3.0\n\n\n0.0013\n\n\n0.0013\n\n\n0.0013\n\n\n0.0012\n\n\n0.0012\n\n\n0.0011\n\n\n0.0011\n\n\n0.0011\n\n\n0.0010\n\n\n0.0010\n\n\n\n\n-2.9\n\n\n0.0019\n\n\n0.0018\n\n\n0.0018\n\n\n0.0017\n\n\n0.0016\n\n\n0.0016\n\n\n0.0015\n\n\n0.0015\n\n\n0.0014\n\n\n0.0014\n\n\n\n\n-2.8\n\n\n0.0026\n\n\n0.0025\n\n\n0.0024\n\n\n0.0023\n\n\n0.0023\n\n\n0.0022\n\n\n0.0021\n\n\n0.0021\n\n\n0.0020\n\n\n0.0019\n\n\n\n\n-2.7\n\n\n0.0035\n\n\n0.0034\n\n\n0.0033\n\n\n0.0032\n\n\n0.0031\n\n\n0.0030\n\n\n0.0029\n\n\n0.0028\n\n\n0.0027\n\n\n0.0026\n\n\n\n\n-2.6\n\n\n0.0047\n\n\n0.0045\n\n\n0.0044\n\n\n0.0043\n\n\n0.0041\n\n\n0.0040\n\n\n0.0039\n\n\n0.0038\n\n\n0.0037\n\n\n0.0036\n\n\n\n\n-2.5\n\n\n0.0062\n\n\n0.0060\n\n\n0.0059\n\n\n0.0057\n\n\n0.0055\n\n\n0.0054\n\n\n0.0052\n\n\n0.0051\n\n\n0.0049\n\n\n0.0048\n\n\n\n\n-2.4\n\n\n0.0082\n\n\n0.0080\n\n\n0.0078\n\n\n0.0075\n\n\n0.0073\n\n\n0.0071\n\n\n0.0069\n\n\n0.0068\n\n\n0.0066\n\n\n0.0064\n\n\n\n\n-2.3\n\n\n0.0107\n\n\n0.0104\n\n\n0.0102\n\n\n0.0099\n\n\n0.0096\n\n\n0.0094\n\n\n0.0091\n\n\n0.0089\n\n\n0.0087\n\n\n0.0084\n\n\n\n\n-2.2\n\n\n0.0139\n\n\n0.0136\n\n\n0.0132\n\n\n0.0129\n\n\n0.0125\n\n\n0.0122\n\n\n0.0119\n\n\n0.0116\n\n\n0.0113\n\n\n0.0110\n\n\n\n\n-2.1\n\n\n0.0179\n\n\n0.0174\n\n\n0.017\n\n\n0.0166\n\n\n0.0162\n\n\n0.0158\n\n\n0.0154\n\n\n0.0150\n\n\n0.0146\n\n\n0.0143\n\n\n\n\n-2.0\n\n\n0.0228\n\n\n0.0222\n\n\n0.0217\n\n\n0.0212\n\n\n0.0207\n\n\n0.0202\n\n\n0.0197\n\n\n0.0192\n\n\n0.0188\n\n\n0.0183\n\n\n\n\n-1.9\n\n\n0.0287\n\n\n0.0281\n\n\n0.0274\n\n\n0.0268\n\n\n0.0262\n\n\n0.0256\n\n\n0.0250\n\n\n0.0244\n\n\n0.0239\n\n\n0.0233\n\n\n\n\n-1.8\n\n\n0.0359\n\n\n0.0351\n\n\n0.0344\n\n\n0.0336\n\n\n0.0329\n\n\n0.0322\n\n\n0.0314\n\n\n0.0307\n\n\n0.0301\n\n\n0.0294\n\n\n\n\n-1.7\n\n\n0.0446\n\n\n0.0436\n\n\n0.0427\n\n\n0.0418\n\n\n0.0409\n\n\n0.0401\n\n\n0.0392\n\n\n0.0384\n\n\n0.0375\n\n\n0.0367\n\n\n\n\n-1.6\n\n\n0.0548\n\n\n0.0537\n\n\n0.0526\n\n\n0.0516\n\n\n0.0505\n\n\n0.0495\n\n\n0.0485\n\n\n0.0475\n\n\n0.0465\n\n\n0.0455\n\n\n\n\n-1.5\n\n\n0.0668\n\n\n0.0655\n\n\n0.0643\n\n\n0.0630\n\n\n0.0618\n\n\n0.0606\n\n\n0.0594\n\n\n0.0582\n\n\n0.0571\n\n\n0.0559\n\n\n\n\n-1.4\n\n\n0.0808\n\n\n0.0793\n\n\n0.0778\n\n\n0.0764\n\n\n0.0749\n\n\n0.0735\n\n\n0.0721\n\n\n0.0708\n\n\n0.0694\n\n\n0.0681\n\n\n\n\n-1.3\n\n\n0.0968\n\n\n0.0951\n\n\n0.0934\n\n\n0.0918\n\n\n0.0901\n\n\n0.0885\n\n\n0.0869\n\n\n0.0853\n\n\n0.0838\n\n\n0.0823\n\n\n\n\n-1.2\n\n\n0.1151\n\n\n0.1131\n\n\n0.1112\n\n\n0.1093\n\n\n0.1075\n\n\n0.1056\n\n\n0.1038\n\n\n0.1020\n\n\n0.1003\n\n\n0.0985\n\n\n\n\n-1.1\n\n\n0.1357\n\n\n0.1335\n\n\n0.1314\n\n\n0.1292\n\n\n0.1271\n\n\n0.1251\n\n\n0.1230\n\n\n0.1210\n\n\n0.1190\n\n\n0.1170\n\n\n\n\n-1.0\n\n\n0.1587\n\n\n0.1562\n\n\n0.1539\n\n\n0.1515\n\n\n0.1492\n\n\n0.1469\n\n\n0.1446\n\n\n0.1423\n\n\n0.1401\n\n\n0.1379\n\n\n\n\n-0.9\n\n\n0.1841\n\n\n0.1814\n\n\n0.1788\n\n\n0.1762\n\n\n0.1736\n\n\n0.1711\n\n\n0.1685\n\n\n0.1660\n\n\n0.1635\n\n\n0.1611\n\n\n\n\n-0.8\n\n\n0.2119\n\n\n0.2090\n\n\n0.2061\n\n\n0.2033\n\n\n0.2005\n\n\n0.1977\n\n\n0.1949\n\n\n0.1922\n\n\n0.1894\n\n\n0.1867\n\n\n\n\n-0.7\n\n\n0.2420\n\n\n0.2389\n\n\n0.2358\n\n\n0.2327\n\n\n0.2296\n\n\n0.2266\n\n\n0.2236\n\n\n0.2206\n\n\n0.2177\n\n\n0.2148\n\n\n\n\n-0.6\n\n\n0.2743\n\n\n0.2709\n\n\n0.2676\n\n\n0.2643\n\n\n0.2611\n\n\n0.2578\n\n\n0.2546\n\n\n0.2514\n\n\n0.2483\n\n\n0.2451\n\n\n\n\n-0.5\n\n\n0.3085\n\n\n0.305\n\n\n0.3015\n\n\n0.2981\n\n\n0.2946\n\n\n0.2912\n\n\n0.2877\n\n\n0.2843\n\n\n0.281\n\n\n0.2776\n\n\n\n\n-0.4\n\n\n0.3446\n\n\n0.3409\n\n\n0.3372\n\n\n0.3336\n\n\n0.3300\n\n\n0.3264\n\n\n0.3228\n\n\n0.3192\n\n\n0.3156\n\n\n0.3121\n\n\n\n\n-0.3\n\n\n0.3821\n\n\n0.3783\n\n\n0.3745\n\n\n0.3707\n\n\n0.3669\n\n\n0.3632\n\n\n0.3594\n\n\n0.3557\n\n\n0.3520\n\n\n0.3483\n\n\n\n\n-0.2\n\n\n0.4207\n\n\n0.4168\n\n\n0.4129\n\n\n0.4090\n\n\n0.4052\n\n\n0.4013\n\n\n0.3974\n\n\n0.3936\n\n\n0.3897\n\n\n0.3859\n\n\n\n\n-0.1\n\n\n0.4602\n\n\n0.4562\n\n\n0.4522\n\n\n0.4483\n\n\n0.4443\n\n\n0.4404\n\n\n0.4364\n\n\n0.4325\n\n\n0.4286\n\n\n0.4247\n\n\n\n\n0.0\n\n\n0.5000\n\n\n0.4960\n\n\n0.4920\n\n\n0.4880\n\n\n0.4840\n\n\n0.4801\n\n\n0.4761\n\n\n0.4721\n\n\n0.4681\n\n\n0.4641\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick for table of standard normal probabilities (continued)\n\nTable entry for \\(z\\) is the probability lying below \\(z\\).\n\n\n\n\n\n\n\\(z\\)\n\n\n0.00\n\n\n0.01\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.05\n\n\n0.06\n\n\n0.07\n\n\n0.08\n\n\n0.09\n\n\n\n\n\n\n0.0\n\n\n0.5000\n\n\n0.5040\n\n\n0.5080\n\n\n0.5120\n\n\n0.5160\n\n\n0.5199\n\n\n0.5239\n\n\n0.5279\n\n\n0.5319\n\n\n0.5359\n\n\n\n\n0.1\n\n\n0.5398\n\n\n0.5438\n\n\n0.5478\n\n\n0.5517\n\n\n0.5557\n\n\n0.5596\n\n\n0.5636\n\n\n0.5675\n\n\n0.5714\n\n\n0.5753\n\n\n\n\n0.2\n\n\n0.5793\n\n\n0.5832\n\n\n0.5871\n\n\n0.5910\n\n\n0.5948\n\n\n0.5987\n\n\n0.6026\n\n\n0.6064\n\n\n0.6103\n\n\n0.6141\n\n\n\n\n0.3\n\n\n0.6179\n\n\n0.6217\n\n\n0.6255\n\n\n0.6293\n\n\n0.6331\n\n\n0.6368\n\n\n0.6406\n\n\n0.6443\n\n\n0.6480\n\n\n0.6517\n\n\n\n\n0.4\n\n\n0.6554\n\n\n0.6591\n\n\n0.6628\n\n\n0.6664\n\n\n0.6700\n\n\n0.6736\n\n\n0.6772\n\n\n0.6808\n\n\n0.6844\n\n\n0.6879\n\n\n\n\n0.5\n\n\n0.6915\n\n\n0.6950\n\n\n0.6985\n\n\n0.7019\n\n\n0.7054\n\n\n0.7088\n\n\n0.7123\n\n\n0.7157\n\n\n0.7190\n\n\n0.7224\n\n\n\n\n0.6\n\n\n0.7257\n\n\n0.7291\n\n\n0.7324\n\n\n0.7357\n\n\n0.7389\n\n\n0.7422\n\n\n0.7454\n\n\n0.7486\n\n\n0.7517\n\n\n0.7549\n\n\n\n\n0.7\n\n\n0.7580\n\n\n0.7611\n\n\n0.7642\n\n\n0.7673\n\n\n0.7704\n\n\n0.7734\n\n\n0.7764\n\n\n0.7794\n\n\n0.7823\n\n\n0.7852\n\n\n\n\n0.8\n\n\n0.7881\n\n\n0.7910\n\n\n0.7939\n\n\n0.7967\n\n\n0.7995\n\n\n0.8023\n\n\n0.8051\n\n\n0.8078\n\n\n0.8106\n\n\n0.8133\n\n\n\n\n0.9\n\n\n0.8159\n\n\n0.8186\n\n\n0.8212\n\n\n0.8238\n\n\n0.8264\n\n\n0.8289\n\n\n0.8315\n\n\n0.8340\n\n\n0.8365\n\n\n0.8389\n\n\n\n\n1.0\n\n\n0.8413\n\n\n0.8438\n\n\n0.8461\n\n\n0.8485\n\n\n0.8508\n\n\n0.8531\n\n\n0.8554\n\n\n0.8577\n\n\n0.8599\n\n\n0.8621\n\n\n\n\n1.1\n\n\n0.8643\n\n\n0.8665\n\n\n0.8686\n\n\n0.8708\n\n\n0.8729\n\n\n0.8749\n\n\n0.8770\n\n\n0.8790\n\n\n0.8810\n\n\n0.8830\n\n\n\n\n1.2\n\n\n0.8849\n\n\n0.8869\n\n\n0.8888\n\n\n0.8907\n\n\n0.8925\n\n\n0.8944\n\n\n0.8962\n\n\n0.8980\n\n\n0.8997\n\n\n0.9015\n\n\n\n\n1.3\n\n\n0.9032\n\n\n0.9049\n\n\n0.9066\n\n\n0.9082\n\n\n0.9099\n\n\n0.9115\n\n\n0.9131\n\n\n0.9147\n\n\n0.9162\n\n\n0.9177\n\n\n\n\n1.4\n\n\n0.9192\n\n\n0.9207\n\n\n0.9222\n\n\n0.9236\n\n\n0.9251\n\n\n0.9265\n\n\n0.9279\n\n\n0.9292\n\n\n0.9306\n\n\n0.9319\n\n\n\n\n1.5\n\n\n0.9332\n\n\n0.9345\n\n\n0.9357\n\n\n0.9370\n\n\n0.9382\n\n\n0.9394\n\n\n0.9406\n\n\n0.9418\n\n\n0.9429\n\n\n0.9441\n\n\n\n\n1.6\n\n\n0.9452\n\n\n0.9463\n\n\n0.9474\n\n\n0.9484\n\n\n0.9495\n\n\n0.9505\n\n\n0.9515\n\n\n0.9525\n\n\n0.9535\n\n\n0.9545\n\n\n\n\n1.7\n\n\n0.9554\n\n\n0.9564\n\n\n0.9573\n\n\n0.9582\n\n\n0.9591\n\n\n0.9599\n\n\n0.9608\n\n\n0.9616\n\n\n0.9625\n\n\n0.9633\n\n\n\n\n1.8\n\n\n0.9641\n\n\n0.9649\n\n\n0.9656\n\n\n0.9664\n\n\n0.9671\n\n\n0.9678\n\n\n0.9686\n\n\n0.9693\n\n\n0.9699\n\n\n0.9706\n\n\n\n\n1.9\n\n\n0.9713\n\n\n0.9719\n\n\n0.9726\n\n\n0.9732\n\n\n0.9738\n\n\n0.9744\n\n\n0.9750\n\n\n0.9756\n\n\n0.9761\n\n\n0.9767\n\n\n\n\n2.0\n\n\n0.9772\n\n\n0.9778\n\n\n0.9783\n\n\n0.9788\n\n\n0.9793\n\n\n0.9798\n\n\n0.9803\n\n\n0.9808\n\n\n0.9812\n\n\n0.9817\n\n\n\n\n2.1\n\n\n0.9821\n\n\n0.9826\n\n\n0.9830\n\n\n0.9834\n\n\n0.9838\n\n\n0.9842\n\n\n0.9846\n\n\n0.9850\n\n\n0.9854\n\n\n0.9857\n\n\n\n\n2.2\n\n\n0.9861\n\n\n0.9864\n\n\n0.9868\n\n\n0.9871\n\n\n0.9875\n\n\n0.9878\n\n\n0.9881\n\n\n0.9884\n\n\n0.9887\n\n\n0.9890\n\n\n\n\n2.3\n\n\n0.9893\n\n\n0.9896\n\n\n0.9898\n\n\n0.9901\n\n\n0.9904\n\n\n0.9906\n\n\n0.9909\n\n\n0.9911\n\n\n0.9913\n\n\n0.9916\n\n\n\n\n2.4\n\n\n0.9918\n\n\n0.9920\n\n\n0.9922\n\n\n0.9925\n\n\n0.9927\n\n\n0.9929\n\n\n0.9931\n\n\n0.9932\n\n\n0.9934\n\n\n0.9936\n\n\n\n\n2.5\n\n\n0.9938\n\n\n0.9940\n\n\n0.9941\n\n\n0.9943\n\n\n0.9945\n\n\n0.9946\n\n\n0.9948\n\n\n0.9949\n\n\n0.9951\n\n\n0.9952\n\n\n\n\n2.6\n\n\n0.9953\n\n\n0.9955\n\n\n0.9956\n\n\n0.9957\n\n\n0.9959\n\n\n0.9960\n\n\n0.9961\n\n\n0.9962\n\n\n0.9963\n\n\n0.9964\n\n\n\n\n2.7\n\n\n0.9965\n\n\n0.9966\n\n\n0.9967\n\n\n0.9968\n\n\n0.9969\n\n\n0.9970\n\n\n0.9971\n\n\n0.9972\n\n\n0.9973\n\n\n0.9974\n\n\n\n\n2.8\n\n\n0.9974\n\n\n0.9975\n\n\n0.9976\n\n\n0.9977\n\n\n0.9977\n\n\n0.9978\n\n\n0.9979\n\n\n0.9979\n\n\n0.9980\n\n\n0.9981\n\n\n\n\n2.9\n\n\n0.9981\n\n\n0.9982\n\n\n0.9982\n\n\n0.9983\n\n\n0.9984\n\n\n0.9984\n\n\n0.9985\n\n\n0.9985\n\n\n0.9986\n\n\n0.9986\n\n\n\n\n3.0\n\n\n0.9987\n\n\n0.9987\n\n\n0.9987\n\n\n0.9988\n\n\n0.9988\n\n\n0.9989\n\n\n0.9989\n\n\n0.9989\n\n\n0.9990\n\n\n0.9990\n\n\n\n\n3.1\n\n\n0.9990\n\n\n0.9991\n\n\n0.9991\n\n\n0.9991\n\n\n0.9992\n\n\n0.9992\n\n\n0.9992\n\n\n0.9992\n\n\n0.9993\n\n\n0.9993\n\n\n\n\n3.2\n\n\n0.9993\n\n\n0.9993\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9995\n\n\n0.9995\n\n\n0.9995\n\n\n\n\n3.3\n\n\n0.9995\n\n\n0.9995\n\n\n0.9995\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9997\n\n\n\n\n3.4\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9998"
  },
  {
    "objectID": "formula-sheet.html#table-b",
    "href": "formula-sheet.html#table-b",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.5 Table B: \\(t\\) distribution critical values",
    "text": "A.5 Table B: \\(t\\) distribution critical values\n\n\n\n\n\n\n\nClick here for table of \\(t\\) distribution critical values\n\nTable entry for \\(p\\) and \\(C\\) is the point \\(t*\\) with probability \\(p\\) lying above it and probability \\(C\\) lying between \\(-t*\\) and \\(t*\\).\n\n\n\n\n\n\ndf\n\n\nTail Probability \\(p\\)\n\n\n\n\n0.25\n\n\n0.20\n\n\n0.15\n\n\n0.10\n\n\n0.05\n\n\n0.025\n\n\n0.02\n\n\n0.01\n\n\n0.005\n\n\n0.0025\n\n\n0.001\n\n\n0.0005\n\n\n\n\n\n\n1\n\n\n1.000\n\n\n1.376\n\n\n1.963\n\n\n3.078\n\n\n6.314\n\n\n12.71\n\n\n15.89\n\n\n31.82\n\n\n63.66\n\n\n127.3\n\n\n318.3\n\n\n636.6\n\n\n\n\n2\n\n\n0.816\n\n\n1.061\n\n\n1.386\n\n\n1.886\n\n\n2.920\n\n\n4.303\n\n\n4.849\n\n\n6.965\n\n\n9.925\n\n\n14.09\n\n\n22.33\n\n\n31.60\n\n\n\n\n3\n\n\n0.765\n\n\n0.978\n\n\n1.250\n\n\n1.638\n\n\n2.353\n\n\n3.182\n\n\n3.482\n\n\n4.541\n\n\n5.841\n\n\n7.453\n\n\n10.21\n\n\n12.92\n\n\n\n\n4\n\n\n0.741\n\n\n0.941\n\n\n1.190\n\n\n1.533\n\n\n2.132\n\n\n2.776\n\n\n2.999\n\n\n3.747\n\n\n4.604\n\n\n5.598\n\n\n7.173\n\n\n8.610\n\n\n\n\n5\n\n\n0.727\n\n\n0.920\n\n\n1.156\n\n\n1.476\n\n\n2.015\n\n\n2.571\n\n\n2.757\n\n\n3.365\n\n\n4.032\n\n\n4.773\n\n\n5.893\n\n\n6.869\n\n\n\n\n6\n\n\n0.718\n\n\n0.906\n\n\n1.134\n\n\n1.440\n\n\n1.943\n\n\n2.447\n\n\n2.612\n\n\n3.143\n\n\n3.707\n\n\n4.317\n\n\n5.208\n\n\n5.959\n\n\n\n\n7\n\n\n0.711\n\n\n0.896\n\n\n1.119\n\n\n1.415\n\n\n1.895\n\n\n2.365\n\n\n2.517\n\n\n2.998\n\n\n3.499\n\n\n4.029\n\n\n4.785\n\n\n5.408\n\n\n\n\n8\n\n\n0.706\n\n\n0.889\n\n\n1.108\n\n\n1.397\n\n\n1.860\n\n\n2.306\n\n\n2.449\n\n\n2.896\n\n\n3.355\n\n\n3.833\n\n\n4.501\n\n\n5.041\n\n\n\n\n9\n\n\n0.703\n\n\n0.883\n\n\n1.100\n\n\n1.383\n\n\n1.833\n\n\n2.262\n\n\n2.398\n\n\n2.821\n\n\n3.250\n\n\n3.690\n\n\n4.297\n\n\n4.781\n\n\n\n\n10\n\n\n0.700\n\n\n0.879\n\n\n1.093\n\n\n1.372\n\n\n1.812\n\n\n2.228\n\n\n2.359\n\n\n2.764\n\n\n3.169\n\n\n3.581\n\n\n4.144\n\n\n4.587\n\n\n\n\n11\n\n\n0.697\n\n\n0.876\n\n\n1.088\n\n\n1.363\n\n\n1.796\n\n\n2.201\n\n\n2.328\n\n\n2.718\n\n\n3.106\n\n\n3.497\n\n\n4.025\n\n\n4.437\n\n\n\n\n12\n\n\n0.695\n\n\n0.873\n\n\n1.083\n\n\n1.356\n\n\n1.782\n\n\n2.179\n\n\n2.303\n\n\n2.681\n\n\n3.055\n\n\n3.428\n\n\n3.930\n\n\n4.318\n\n\n\n\n13\n\n\n0.694\n\n\n0.870\n\n\n1.079\n\n\n1.350\n\n\n1.771\n\n\n2.160\n\n\n2.282\n\n\n2.650\n\n\n3.012\n\n\n3.372\n\n\n3.852\n\n\n4.221\n\n\n\n\n14\n\n\n0.692\n\n\n0.868\n\n\n1.076\n\n\n1.345\n\n\n1.761\n\n\n2.145\n\n\n2.264\n\n\n2.624\n\n\n2.977\n\n\n3.326\n\n\n3.787\n\n\n4.140\n\n\n\n\n15\n\n\n0.691\n\n\n0.866\n\n\n1.074\n\n\n1.341\n\n\n1.753\n\n\n2.131\n\n\n2.249\n\n\n2.602\n\n\n2.947\n\n\n3.286\n\n\n3.733\n\n\n4.073\n\n\n\n\n16\n\n\n0.690\n\n\n0.865\n\n\n1.071\n\n\n1.337\n\n\n1.746\n\n\n2.120\n\n\n2.235\n\n\n2.583\n\n\n2.921\n\n\n3.252\n\n\n3.686\n\n\n4.015\n\n\n\n\n17\n\n\n0.689\n\n\n0.863\n\n\n1.069\n\n\n1.333\n\n\n1.740\n\n\n2.110\n\n\n2.224\n\n\n2.567\n\n\n2.898\n\n\n3.222\n\n\n3.646\n\n\n3.965\n\n\n\n\n18\n\n\n0.688\n\n\n0.862\n\n\n1.067\n\n\n1.330\n\n\n1.734\n\n\n2.101\n\n\n2.214\n\n\n2.552\n\n\n2.878\n\n\n3.197\n\n\n3.610\n\n\n3.922\n\n\n\n\n19\n\n\n0.688\n\n\n0.861\n\n\n1.066\n\n\n1.328\n\n\n1.729\n\n\n2.093\n\n\n2.205\n\n\n2.539\n\n\n2.861\n\n\n3.174\n\n\n3.579\n\n\n3.883\n\n\n\n\n20\n\n\n0.687\n\n\n0.860\n\n\n1.064\n\n\n1.325\n\n\n1.725\n\n\n2.086\n\n\n2.197\n\n\n2.528\n\n\n2.845\n\n\n3.153\n\n\n3.552\n\n\n3.850\n\n\n\n\n21\n\n\n0.686\n\n\n0.859\n\n\n1.063\n\n\n1.323\n\n\n1.721\n\n\n2.080\n\n\n2.189\n\n\n2.518\n\n\n2.831\n\n\n3.135\n\n\n3.527\n\n\n3.819\n\n\n\n\n22\n\n\n0.686\n\n\n0.858\n\n\n1.061\n\n\n1.321\n\n\n1.717\n\n\n2.074\n\n\n2.183\n\n\n2.508\n\n\n2.819\n\n\n3.119\n\n\n3.505\n\n\n3.792\n\n\n\n\n23\n\n\n0.685\n\n\n0.858\n\n\n1.060\n\n\n1.319\n\n\n1.714\n\n\n2.069\n\n\n2.177\n\n\n2.500\n\n\n2.807\n\n\n3.104\n\n\n3.485\n\n\n3.768\n\n\n\n\n24\n\n\n0.685\n\n\n0.857\n\n\n1.059\n\n\n1.318\n\n\n1.711\n\n\n2.064\n\n\n2.172\n\n\n2.492\n\n\n2.797\n\n\n3.091\n\n\n3.467\n\n\n3.745\n\n\n\n\n25\n\n\n0.684\n\n\n0.856\n\n\n1.058\n\n\n1.316\n\n\n1.708\n\n\n2.060\n\n\n2.167\n\n\n2.485\n\n\n2.787\n\n\n3.078\n\n\n3.450\n\n\n3.725\n\n\n\n\n26\n\n\n0.684\n\n\n0.856\n\n\n1.058\n\n\n1.315\n\n\n1.706\n\n\n2.056\n\n\n2.162\n\n\n2.479\n\n\n2.779\n\n\n3.067\n\n\n3.435\n\n\n3.707\n\n\n\n\n27\n\n\n0.684\n\n\n0.855\n\n\n1.057\n\n\n1.314\n\n\n1.703\n\n\n2.052\n\n\n2.158\n\n\n2.473\n\n\n2.771\n\n\n3.057\n\n\n3.421\n\n\n3.690\n\n\n\n\n28\n\n\n0.683\n\n\n0.855\n\n\n1.056\n\n\n1.313\n\n\n1.701\n\n\n2.048\n\n\n2.154\n\n\n2.467\n\n\n2.763\n\n\n3.047\n\n\n3.408\n\n\n3.674\n\n\n\n\n29\n\n\n0.683\n\n\n0.854\n\n\n1.055\n\n\n1.311\n\n\n1.699\n\n\n2.045\n\n\n2.150\n\n\n2.462\n\n\n2.756\n\n\n3.038\n\n\n3.396\n\n\n3.659\n\n\n\n\n30\n\n\n0.683\n\n\n0.854\n\n\n1.055\n\n\n1.310\n\n\n1.697\n\n\n2.042\n\n\n2.147\n\n\n2.457\n\n\n2.750\n\n\n3.030\n\n\n3.385\n\n\n3.646\n\n\n\n\n40\n\n\n0.681\n\n\n0.851\n\n\n1.050\n\n\n1.303\n\n\n1.684\n\n\n2.021\n\n\n2.123\n\n\n2.423\n\n\n2.704\n\n\n2.971\n\n\n3.307\n\n\n3.551\n\n\n\n\n50\n\n\n0.679\n\n\n0.849\n\n\n1.047\n\n\n1.299\n\n\n1.676\n\n\n2.009\n\n\n2.109\n\n\n2.403\n\n\n2.678\n\n\n2.937\n\n\n3.261\n\n\n3.496\n\n\n\n\n60\n\n\n0.679\n\n\n0.848\n\n\n1.045\n\n\n1.296\n\n\n1.671\n\n\n2.000\n\n\n2.099\n\n\n2.390\n\n\n2.660\n\n\n2.915\n\n\n3.232\n\n\n3.460\n\n\n\n\n80\n\n\n0.678\n\n\n0.846\n\n\n1.043\n\n\n1.292\n\n\n1.664\n\n\n1.990\n\n\n2.088\n\n\n2.374\n\n\n2.639\n\n\n2.887\n\n\n3.195\n\n\n3.416\n\n\n\n\n100\n\n\n0.677\n\n\n0.845\n\n\n1.042\n\n\n1.290\n\n\n1.660\n\n\n1.984\n\n\n2.081\n\n\n2.364\n\n\n2.626\n\n\n2.871\n\n\n3.174\n\n\n3.390\n\n\n\n\n1000\n\n\n0.675\n\n\n0.842\n\n\n1.037\n\n\n1.282\n\n\n1.646\n\n\n1.962\n\n\n2.056\n\n\n2.330\n\n\n2.581\n\n\n2.813\n\n\n3.098\n\n\n3.300\n\n\n\n\n\\(\\infty\\)\n\n\n0.674\n\n\n0.842\n\n\n1.036\n\n\n1.282\n\n\n1.645\n\n\n1.960\n\n\n2.054\n\n\n2.326\n\n\n2.576\n\n\n2.807\n\n\n3.090\n\n\n3.291\n\n\n\n\n\n\n50%\n\n\n60%\n\n\n70%\n\n\n80%\n\n\n90%\n\n\n95%\n\n\n96%\n\n\n98%\n\n\n99%\n\n\n99.50%\n\n\n99.8\n\n\n99.99%\n\n\n\n\nConfidence Level \\(C\\)"
  },
  {
    "objectID": "formula-sheet.html#table-c",
    "href": "formula-sheet.html#table-c",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.6 Table C: \\(\\chi^2\\) critical values",
    "text": "A.6 Table C: \\(\\chi^2\\) critical values\n\n\n\n\n\n\n\nClick here for table of \\(\\chi^2\\) distribution critical values\n\nTable entry for \\(p\\) is the point (\\(\\chi^2\\)) with probability \\(p\\) lying above it.\n\n\n\n\n\n\ndf\n\n\nTail Probability \\(p\\)\n\n\n\n\n0.25\n\n\n0.20\n\n\n0.15\n\n\n0.10\n\n\n0.05\n\n\n0.025\n\n\n0.02\n\n\n0.01\n\n\n0.005\n\n\n0.0025\n\n\n0.001\n\n\n0.0005\n\n\n\n\n\n\n1\n\n\n1.32\n\n\n1.64\n\n\n2.07\n\n\n2.71\n\n\n3.84\n\n\n5.02\n\n\n5.41\n\n\n6.63\n\n\n7.88\n\n\n9.14\n\n\n10.83\n\n\n12.12\n\n\n\n\n2\n\n\n2.77\n\n\n3.22\n\n\n3.79\n\n\n4.61\n\n\n5.99\n\n\n7.38\n\n\n7.82\n\n\n9.21\n\n\n10.60\n\n\n11.98\n\n\n13.82\n\n\n15.20\n\n\n\n\n3\n\n\n4.11\n\n\n4.64\n\n\n5.32\n\n\n6.25\n\n\n7.81\n\n\n9.35\n\n\n9.84\n\n\n11.34\n\n\n12.84\n\n\n14.32\n\n\n16.27\n\n\n17.73\n\n\n\n\n4\n\n\n5.39\n\n\n5.99\n\n\n6.74\n\n\n7.78\n\n\n9.49\n\n\n11.14\n\n\n11.67\n\n\n13.28\n\n\n14.86\n\n\n16.42\n\n\n18.47\n\n\n20.00\n\n\n\n\n5\n\n\n6.63\n\n\n7.29\n\n\n8.12\n\n\n9.24\n\n\n11.07\n\n\n12.83\n\n\n13.39\n\n\n15.09\n\n\n16.75\n\n\n18.39\n\n\n20.52\n\n\n22.11\n\n\n\n\n6\n\n\n7.84\n\n\n8.56\n\n\n9.45\n\n\n10.64\n\n\n12.59\n\n\n14.45\n\n\n15.03\n\n\n16.81\n\n\n18.55\n\n\n20.25\n\n\n22.46\n\n\n24.10\n\n\n\n\n7\n\n\n9.04\n\n\n9.80\n\n\n10.75\n\n\n12.02\n\n\n14.07\n\n\n16.01\n\n\n16.62\n\n\n18.48\n\n\n20.28\n\n\n22.04\n\n\n24.32\n\n\n26.02\n\n\n\n\n8\n\n\n10.22\n\n\n11.03\n\n\n12.03\n\n\n13.36\n\n\n15.51\n\n\n17.53\n\n\n18.17\n\n\n20.09\n\n\n21.95\n\n\n23.77\n\n\n26.12\n\n\n27.87\n\n\n\n\n9\n\n\n11.39\n\n\n12.24\n\n\n13.29\n\n\n14.68\n\n\n16.92\n\n\n19.02\n\n\n19.68\n\n\n21.67\n\n\n23.59\n\n\n25.46\n\n\n27.88\n\n\n29.67\n\n\n\n\n10\n\n\n12.55\n\n\n13.44\n\n\n14.53\n\n\n15.99\n\n\n18.31\n\n\n20.48\n\n\n21.16\n\n\n23.21\n\n\n25.19\n\n\n27.11\n\n\n29.59\n\n\n31.42\n\n\n\n\n11\n\n\n13.70\n\n\n14.63\n\n\n15.77\n\n\n17.28\n\n\n19.68\n\n\n21.92\n\n\n22.62\n\n\n24.72\n\n\n26.76\n\n\n28.73\n\n\n31.26\n\n\n33.14\n\n\n\n\n12\n\n\n14.85\n\n\n15.81\n\n\n16.99\n\n\n18.55\n\n\n21.03\n\n\n23.34\n\n\n24.05\n\n\n26.22\n\n\n28.30\n\n\n30.32\n\n\n32.91\n\n\n34.82\n\n\n\n\n13\n\n\n15.98\n\n\n16.98\n\n\n18.20\n\n\n19.81\n\n\n22.36\n\n\n24.74\n\n\n25.47\n\n\n27.69\n\n\n29.82\n\n\n31.88\n\n\n34.53\n\n\n36.48\n\n\n\n\n14\n\n\n17.12\n\n\n18.15\n\n\n19.41\n\n\n21.06\n\n\n23.68\n\n\n26.12\n\n\n26.87\n\n\n29.14\n\n\n31.32\n\n\n33.43\n\n\n36.12\n\n\n38.11\n\n\n\n\n15\n\n\n18.25\n\n\n19.31\n\n\n20.60\n\n\n22.31\n\n\n25.00\n\n\n27.49\n\n\n28.26\n\n\n30.58\n\n\n32.80\n\n\n34.95\n\n\n37.70\n\n\n39.72\n\n\n\n\n16\n\n\n19.37\n\n\n20.47\n\n\n21.79\n\n\n23.54\n\n\n26.30\n\n\n28.85\n\n\n29.63\n\n\n32.00\n\n\n34.27\n\n\n36.46\n\n\n39.25\n\n\n41.31\n\n\n\n\n17\n\n\n20.49\n\n\n21.61\n\n\n22.98\n\n\n24.77\n\n\n27.59\n\n\n30.19\n\n\n31.00\n\n\n33.41\n\n\n35.72\n\n\n37.95\n\n\n40.79\n\n\n42.88\n\n\n\n\n18\n\n\n21.60\n\n\n22.76\n\n\n24.16\n\n\n25.99\n\n\n28.87\n\n\n31.53\n\n\n32.35\n\n\n34.81\n\n\n37.16\n\n\n39.42\n\n\n42.31\n\n\n44.43\n\n\n\n\n19\n\n\n22.72\n\n\n23.90\n\n\n25.33\n\n\n27.20\n\n\n30.14\n\n\n32.85\n\n\n33.69\n\n\n36.19\n\n\n38.58\n\n\n40.88\n\n\n43.82\n\n\n45.97\n\n\n\n\n20\n\n\n23.83\n\n\n25.04\n\n\n26.50\n\n\n28.41\n\n\n31.41\n\n\n34.17\n\n\n35.02\n\n\n37.57\n\n\n40.00\n\n\n42.34\n\n\n45.31\n\n\n47.50\n\n\n\n\n21\n\n\n24.93\n\n\n26.17\n\n\n27.66\n\n\n29.62\n\n\n32.67\n\n\n35.48\n\n\n36.34\n\n\n38.93\n\n\n41.40\n\n\n43.78\n\n\n46.80\n\n\n49.01\n\n\n\n\n22\n\n\n26.04\n\n\n27.30\n\n\n28.82\n\n\n30.81\n\n\n33.92\n\n\n36.78\n\n\n37.66\n\n\n40.29\n\n\n42.80\n\n\n45.20\n\n\n48.27\n\n\n50.51\n\n\n\n\n23\n\n\n27.14\n\n\n28.43\n\n\n29.98\n\n\n32.01\n\n\n35.17\n\n\n38.08\n\n\n38.97\n\n\n41.64\n\n\n44.18\n\n\n46.62\n\n\n49.73\n\n\n52.00\n\n\n\n\n24\n\n\n28.24\n\n\n29.55\n\n\n31.13\n\n\n33.20\n\n\n36.42\n\n\n39.36\n\n\n40.27\n\n\n42.98\n\n\n45.56\n\n\n48.03\n\n\n51.18\n\n\n53.48\n\n\n\n\n25\n\n\n29.34\n\n\n30.68\n\n\n32.28\n\n\n34.38\n\n\n37.65\n\n\n40.65\n\n\n41.57\n\n\n44.31\n\n\n46.93\n\n\n49.44\n\n\n52.62\n\n\n54.95\n\n\n\n\n26\n\n\n30.43\n\n\n31.79\n\n\n33.43\n\n\n35.56\n\n\n38.89\n\n\n41.92\n\n\n42.86\n\n\n45.64\n\n\n48.29\n\n\n50.83\n\n\n54.05\n\n\n56.41\n\n\n\n\n27\n\n\n31.53\n\n\n32.91\n\n\n34.57\n\n\n36.74\n\n\n40.11\n\n\n43.19\n\n\n44.14\n\n\n46.96\n\n\n49.64\n\n\n52.22\n\n\n55.48\n\n\n57.86\n\n\n\n\n28\n\n\n32.62\n\n\n34.03\n\n\n35.71\n\n\n37.92\n\n\n41.34\n\n\n44.46\n\n\n45.42\n\n\n48.28\n\n\n50.99\n\n\n53.59\n\n\n56.89\n\n\n59.30\n\n\n\n\n29\n\n\n33.71\n\n\n35.14\n\n\n36.85\n\n\n39.09\n\n\n42.56\n\n\n45.72\n\n\n46.69\n\n\n49.59\n\n\n52.34\n\n\n54.97\n\n\n58.30\n\n\n60.73\n\n\n\n\n30\n\n\n34.80\n\n\n36.25\n\n\n37.99\n\n\n40.26\n\n\n43.77\n\n\n46.98\n\n\n47.96\n\n\n50.89\n\n\n53.67\n\n\n56.33\n\n\n59.70\n\n\n62.16\n\n\n\n\n40\n\n\n45.62\n\n\n47.27\n\n\n49.24\n\n\n51.81\n\n\n55.76\n\n\n59.34\n\n\n60.44\n\n\n63.69\n\n\n66.77\n\n\n69.70\n\n\n73.40\n\n\n76.09\n\n\n\n\n50\n\n\n56.33\n\n\n58.16\n\n\n60.35\n\n\n63.17\n\n\n67.50\n\n\n71.42\n\n\n72.61\n\n\n76.15\n\n\n79.49\n\n\n82.66\n\n\n86.66\n\n\n89.56\n\n\n\n\n60\n\n\n66.98\n\n\n68.97\n\n\n71.34\n\n\n74.40\n\n\n79.08\n\n\n83.30\n\n\n84.58\n\n\n88.38\n\n\n91.95\n\n\n95.34\n\n\n99.61\n\n\n102.69\n\n\n\n\n80\n\n\n88.13\n\n\n90.41\n\n\n93.11\n\n\n96.58\n\n\n101.9\n\n\n106.6\n\n\n108.1\n\n\n112.3\n\n\n116.3\n\n\n120.1\n\n\n124.8\n\n\n128.3\n\n\n\n\n100\n\n\n109.1\n\n\n111.7\n\n\n114.7\n\n\n118.5\n\n\n124.3\n\n\n129.6\n\n\n131.1\n\n\n135.8\n\n\n140.2\n\n\n144.3\n\n\n149.4\n\n\n153.2"
  }
]