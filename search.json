[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AP Statistics Notes",
    "section": "",
    "text": "1 Introduction\nThese are my notes based off what we learned in class. The numbers next to the sections are not related to the textbook / the chapter numbers. Please take a look at what we are learning to know which page to look at.\nThere are various functions on this website that make your notes accessible in the case that you need more support in reading the material. Take a look at the tool bar on the top and try right clicking on math equations to see options to enlarge math equations. Ask Mr. Chang if you want to explore more.\nThese notes are incomplete, but I’ll be trying to update them completely before the AP test. Give me any feedback that you might want to see on these notes.\nLook at the rest of these notes through the navigation bar on the left hand side (might have to click on the hamburger icon), or go through these sequentially by clicking the arrows on the left and right hand side."
  },
  {
    "objectID": "notation.html#lists",
    "href": "notation.html#lists",
    "title": "2  Basic Notation",
    "section": "2.1 Lists",
    "text": "2.1 Lists\nMost of the time, you will see \\(x\\) denote a list of values (i.e. a variable in a data table).\nFor example, if \\(x\\) was the list of numerical values \\(a\\) to \\(g\\), we can write it as:\n\\(x = [a, b, c, d, e, f, g]\\)\nThen, \\(x_i\\) means the \\(i^{th}\\) value in the list of \\(x\\), so\n\\(x_1 = a\\) and \\(x_2 = b\\) and \\(x_7 = g\\)."
  },
  {
    "objectID": "notation.html#n",
    "href": "notation.html#n",
    "title": "2  Basic Notation",
    "section": "2.2 \\(n\\)",
    "text": "2.2 \\(n\\)\nIn regards to a data table or list of values, \\(n\\) stands for the number of rows or data points that are in the data table or list (we will learn this as the sample size later on)\nSo, for list \\(x\\), \\(n = 7\\).\nAdding on, \\(x_n\\) would mean the last value in the list \\(x\\) (since there are only \\(n\\) values in \\(x\\))"
  },
  {
    "objectID": "notation.html#summation-sigma",
    "href": "notation.html#summation-sigma",
    "title": "2  Basic Notation",
    "section": "2.3 Summation (\\(\\Sigma\\))",
    "text": "2.3 Summation (\\(\\Sigma\\))\nYou will also see the greek letter \\(\\Sigma\\) in formulas. Usage of this sign means that we are using summation notation.\nIf we want the sum of all numbers from 1 to 7, we would write it as,\n\\[\\sum_{i=1}^7 i\\]\nWe interpret this as,\n\nStart from \\(i = 1\\), evaluate the expression, which is \\(i\\).\nKeep our evaluated expression to the side and get ready to add the other values to it, so\n\n\\[1 + \\cdots\\]\n\nNow go the next numbers until we get to \\(7\\) (the number on the top of the \\(\\Sigma\\)) So moving onto \\(i = 2\\), we end up with \\[1 + 2 + \\cdots\\] And with \\(i = 3\\), we end up with \\[1 + 2 + 3 + \\cdots\\] \nWhen we get to the end of it (when we reach \\(i = 7\\)), we have the expanded form of the summation. \\[1 + 2 + 3+ 4 + 5 + 6 + 7\\]"
  },
  {
    "objectID": "notation.html#other-notation",
    "href": "notation.html#other-notation",
    "title": "2  Basic Notation",
    "section": "2.4 Other notation",
    "text": "2.4 Other notation\n\n\\(\\bar x\\): The “line” on top of \\(x\\) means the mean of \\(x\\). If we had \\(\\bar a\\), I would be asking for the mean of \\(a\\).\n\nPronounced “x bar”\n\n\\(\\hat p\\): The “hat” on top of \\(p\\) means the estimate of \\(p\\). If we had \\(\\hat x\\), I would be asking for the estimate of \\(x\\).\n\nPronounced “p hat”"
  },
  {
    "objectID": "notation.html#other-commonly-used-symbols",
    "href": "notation.html#other-commonly-used-symbols",
    "title": "2  Basic Notation",
    "section": "2.5 Other commonly used symbols",
    "text": "2.5 Other commonly used symbols\n\n\\(N\\): population size\n\\(p\\): proportion, probability, or p-value\n\\(\\mu\\): population mean (true mean)\n\\(\\bar x\\): sample mean\n\\(\\sigma\\): population standard deviation (true standard deviation)\n\\(s_x\\): sample standard deviation (of x), so \\(s_y\\) is the sample standard deviation of \\(y\\)\n\\(SE_{\\bar x}\\): Standard error of the sample mean (of x), the estimate of the standard deviation of the sample mean."
  },
  {
    "objectID": "basic-statistics.html#sample-mean",
    "href": "basic-statistics.html#sample-mean",
    "title": "3  Basic Statistics",
    "section": "3.1 Sample Mean",
    "text": "3.1 Sample Mean\n\\[\\bar x = \\frac{1}{n}\\sum x_i = \\frac{\\sum x_i}{n}\\]"
  },
  {
    "objectID": "basic-statistics.html#sample-standard-deviation",
    "href": "basic-statistics.html#sample-standard-deviation",
    "title": "3  Basic Statistics",
    "section": "3.2 Sample Standard Deviation",
    "text": "3.2 Sample Standard Deviation\n\\[s_x = \\sqrt{\\frac{1}{n-1}\\sum (x_i - \\bar x)^2} = \\sqrt{\\frac{\\sum(x_i -\\bar x)^2}{n - 1}}\\]\n\n\n\n\n\n\nInterpretation of Standard Deviation\n\n\n\nThe [variable in the context of the problem] typically varies by [value of SD] from the mean of [value of the mean]"
  },
  {
    "objectID": "categorical-displays.html#bar-plots",
    "href": "categorical-displays.html#bar-plots",
    "title": "4  Categorical Data Visualizations",
    "section": "4.1 Bar plots",
    "text": "4.1 Bar plots\n\n\n\n\n\n\nBar Plots\n\n\n\nRepresent the number or proportion of each unique value. These numbers or proportions are represented with rectangular bars with heights proportional to the values that they represent.\nYou can plot these vertically or horizontally (i.e. categories on the x-axis or categories on the y-axis)\n\n\nFollowing data from this table:\n\n\n\n\n  \n\n\n\n\nCount up the number of values per category (make a frequency table). Note: This table is missing the total\n\n\n\n\n\n  \n\n\n\n\nPlot the frequencies with them as the height of the bars\n\n\n\n\n\n\nIf needed (if you need proportions for the y-axis instead, calculate the relative frequency table for the frequency table first). Note: again, this one is missing the total"
  },
  {
    "objectID": "categorical-displays.html#stacked-bar-plots-and-side-by-side-bar-graphs",
    "href": "categorical-displays.html#stacked-bar-plots-and-side-by-side-bar-graphs",
    "title": "4  Categorical Data Visualizations",
    "section": "4.2 Stacked Bar Plots and Side-by-Side Bar Graphs",
    "text": "4.2 Stacked Bar Plots and Side-by-Side Bar Graphs\n\n\n\n\n\n\nMultivariate Bar Plots\n\n\n\nStacked bar plots show two categorical variables, one on the x-axis/y-axis, and the other as the legend (colours). We will call the variable on the x-axis as the “groups” and the variable on the legend as the “categories.”\nSide-by-Side bar graphs are similar in concept.\n\n\nWhen constructing these bar plots, we first want to determine which variable goes where (your choice or given choice to you). Then you calculate relative frequencies per group\nFor example, here I have a two-way table detailing the hair and eye colour of some statistics students\n\n\n\n\n  \n\n\n\nSo if I want eye colour to be my groups, I would calculate the relative frequencies by column (use the total of the column and divide the whole column by it), so each group/column will add up to 1.\n\n\n\n\n  \n\n\n\nThese numbers will be my bar heights. So for the bar(s) representing brown eyes:\n\nblack hair will be .3091\nbrown hair will be .5409\nred hair will be 0.1182\nblond hair will be 0.0318\n\n\n\n\n\n\nHere’s the corresponding side-by-side bar plot. Note that the heights of the bars are the same as the segmented bar graph.\n\n\n\n\n\nOn the other hand, if I want my eye colour to be my groups, I would calculate the relative frequencies by row (use the total of the row and divide the whole row by it), so each group/row will add up to 1.\n\n\n\n\n  \n\n\n\nThese numbers will be my bar heights. So for the bar(s) representing black hair:\n\nbrown eyes will be 0.6296296\nblue eyes will be 0.1851852\nhazel eyes will be 0.1388889\ngreen eyes will be 0.0462963"
  },
  {
    "objectID": "categorical-displays.html#mosaic-plots",
    "href": "categorical-displays.html#mosaic-plots",
    "title": "4  Categorical Data Visualizations",
    "section": "4.3 Mosaic Plots",
    "text": "4.3 Mosaic Plots\n\n\n\n\n\n\nMosaic Plots\n\n\n\nMosaic plots are the almost the same as stacked bar plots. The only difference is that the widths of the bars change according to the proportion of points in each group. In a mosaic plot, the x-axis will also measure the proportion of observations/data points within the groupings (i.e. the x-axis reflects the marginal distribution of the variable on the x-axis).\n\n\nFollowing the same steps as the side-by-side and stacked bar charts to find the heights, we now add an additional step before plotting.\n\n\n\n\n\n\nTip\n\n\n\nFind the widths of the bars by finding the marginal distribution of the variable on the x-axis (the groups)\n\n\n\nFor each group, find the probability of having that trait. So for our previous example, we had this table:\n\n\n\n\n\n  \n\n\n\nUsing our eye colours as the groups (vertical bars), we will find:\n\n\\(P(Brown) \\approx .3716\\)\n\\(P(Blue) \\approx .3632\\)\n\\(P(Hazel) \\approx .1571\\)\n\\(P(Green) \\approx .1081\\)\n\nWhen we plot our mosaic plot, we do the same thing, except now, we have our bars differ in widths according to the numbers that we just calculated."
  },
  {
    "objectID": "quantitative-displays.html#dot-plots",
    "href": "quantitative-displays.html#dot-plots",
    "title": "5  Quantative Data Visualizations",
    "section": "5.1 Dot Plots",
    "text": "5.1 Dot Plots\n\n\n\n\n\n\nDot Plots\n\n\n\nDots Plots represent one quantitative variable by marking a dot for each value observed. They are mainly for discrete quantitative variables only and they are mainly useful in situations when you have a small range of number so that you can actually see how the data distribution varies across values.\n\n\nDot plots are simple, you draw a number line and then plot points above the number for each of the number that you see in the data.\nTake this data for example:\n\n\n\n\n  \n\n\n\nNow count up each value to figure out how many dots you need at each value on the number line then plot your graph"
  },
  {
    "objectID": "quantitative-displays.html#stemplots",
    "href": "quantitative-displays.html#stemplots",
    "title": "5  Quantative Data Visualizations",
    "section": "5.2 Stemplots",
    "text": "5.2 Stemplots\n\n\n\n\n\n\nStemplots\n\n\n\nIn a stem plot, you need to determine a common “stem” of all the numbers that you’re plotting. So if you have integer numbers from 10 to 200, your stems will be everything from the tens and so on, so you’ll have stems from 1-20. Once you take the stems, you just write the “leaves” next to the stem that they belong.\nThese are also known as stem-and-leaf plots.\n\n\nUsing this data as an example,\n\n\n\n\n  \n\n\n\nA stem plot looks like this:\n\n\n1 | 2: represents 12\n leaf unit: 1\n            n: 20\n    0 | 1\n    1 | 8\n    2 | \n    3 | 36689\n    4 | 4\n    5 | 2339\n    6 | 0228\n    7 | \n    8 | 19\n    9 | 35\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that you also have to add a key to show what a stem + leaf means. The stem and leaves give no information on the decimals in the data, so as you see above, you need to give an example like (as shown in the example stemplot):\nKey: 1\\|2 = 12\n\n\nHere’s another example (sorted for convenience)\n\n\n\n\n  \n\n\n\n1 | 2: represents 1.2\n leaf unit: 0.1\n            n: 50\n   3 | 3\n   3 | 579999\n   4 | 00334\n   4 | 556677788899\n   5 | 00112233444\n   5 | 667888999\n   6 | 234\n   6 | 5\n   7 | 12"
  },
  {
    "objectID": "quantitative-displays.html#boxplots",
    "href": "quantitative-displays.html#boxplots",
    "title": "5  Quantative Data Visualizations",
    "section": "5.3 Boxplots",
    "text": "5.3 Boxplots\n\n\n\n\n\n\nBoxplots\n\n\n\nBoxplots are primarily made of the five number summary of the data. The five number summary is made up of the:\n\nMinimum (min)\nFirst Quartile (\\(Q_1\\))\nMedian\nThird Quartile (\\(Q_3\\))\nMaximum (max)\n\nAlso known as a box-and-whisker plot.\n\n\nTo make a simple boxplot, you use the first quartile, median, and third quartile to make the “box” and then use the minimum and maximum to make the “whiskers.”\nFor this simple list of numbers:\n\n\n\n\n  \n\n\n\nOur five number summary is:\n\n\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n      1       2       4       6       7 \n\n\nAs detailed above, our box plot then looks like:\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe last detail is that we can calculate outliers using the 1.5 IQR rule and show them on the boxplot. For either direction (left or right), if we see outliers in that direction, we only extend the whisker to the smallest and/or largest point that is not an outlier. Then we plot any outliers as individual points.\n\n\nLook at this example data:\n\n\n\n\n  \n\n\n\nFive number summary:\n\n\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n    -12      -5      -3       0      12 \n\n\nOur numbers calculated by the 1.5 IQR rule are:\n\n\n[1] -12.5   7.5\n\n\nSo our 12 is an outlier. which means we draw our right whisker to 6 and plot the 12 individually on the number line. Like so:"
  },
  {
    "objectID": "quantitative-displays.html#histograms",
    "href": "quantitative-displays.html#histograms",
    "title": "5  Quantative Data Visualizations",
    "section": "5.4 Histograms",
    "text": "5.4 Histograms\n\n\n\n\n\n\nHistograms\n\n\n\nA histogram is similar to a bar plot, except that histograms are made for quantitative data and bars are continuous in the sense that there is no gap between bars. To make a histogram, select an appropriate equal intervals that make it so that you don’t have too many bars and that you don’t have too few bars.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYour goal with histograms, as with many other visualizations, is to be able to see the shape and characteristics of the distribution in question. If you have too many bars or too few bars, you won’t be able to see much important information (especially think of situations when you have many data points with very precise decimal measurements).\n\n\n\nDecide on your intervals (e.g. by 5’s, by 10’s, by 100’s)\nWithin your intervals, count up the number of observations that belong in that “bin”. When you do so, count up observations so that you count the left end inclusive and the right end inclusive. So if you did intervals of 5, you would do something like counting up points \\(0 \\leq x &lt; 5\\), \\(5 \\leq x &lt; 10\\), and so on.\nPlot your bars.\n\nExample:\nConsider this example data set:\nOur data has this set of summary statistics:\n\n\n       x        \n Min.   :1.522  \n 1st Qu.:1.912  \n Median :2.022  \n Mean   :2.110  \n 3rd Qu.:2.224  \n Max.   :2.704  \n\n\nWith this knowledge, let’s make our 7 “bins”, so let’s do these by every 0.2, starting at 1.5 to 2.9. This will be something that you build by intuition.\nNow, count up our values:\n\n\n[1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n        1         4         6         5         1         2         1 \n\n\nNow, we just put it together. For each bin, we have a bar and the bars’ heights correspond to the number of individuals in each bin.\n\n\n\n\n\nAgain, just like bar graphs, we can instead do the relative frequencies (this is what you’ll see most of the time!!!)\n\n\n[1.5,1.7) [1.7,1.9) [1.9,2.1) [2.1,2.3) [2.3,2.5) [2.5,2.7) [2.7,2.9] \n     0.05      0.20      0.30      0.25      0.05      0.10      0.05 \n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you have a histogram like this, keep in mind that the bars always add up to 1 (or 100%)."
  },
  {
    "objectID": "sampling.html#bias",
    "href": "sampling.html#bias",
    "title": "6  Sampling",
    "section": "6.1 Bias",
    "text": "6.1 Bias\nWhen we collect data, there is the possibility of the data becoming systematically pushed towards a specific outcome. For example, if we want to learn about the GPA average in the school and take a sample of students only from a class, it’s quite possible that the sample is not representative of the school. We will probably result in a GPA average that is higher or lower than the actual GPA average in the school.\nThere are several ways that this can happen. Based off a scenario, you should be able to identify the bias that is occurring. The main ways that we learn when this occurs is when:\n\n\n\n\n\n\nResponse Bias\n\n\n\nResponse bias is created when something causes people’s responses to be systematically pushed in one direction.\n\n\n\n\n\n\n\n\nSelf-reported response bias\n\n\n\nWhen individuals inaccurately report their own traits, typically to avoid embarrassment or other situation where they feel more inclined to not tell the truth.\n\n\n\n\n\n\n\n\nQuestion wording bias\n\n\n\nWhen survey questions are confusing or leading to favor or disfavor certain answers from a respondent.\n\n\n\n\n\n\n\n\nNon-response Bias\n\n\n\nNonresponse occurs when an individual chosen for the sample can’t be contacted or refuses to participate.\n\n\n\n\n\n\n\n\nUndercoverage\n\n\n\nUndercoverage occurs when some members of the population cannot be chosen in the sample or specific individuals have a reduced chance of being chosen to be in the sample.\n\n\n\n\n\n\n\n\nVoluntary Response Sample\n\n\n\nVoluntary response samples consists of people who choose themselves by responding to general invitation, causing the chosen sample to not be representative of the population.\n\n\n\n\n\n\n\n\nConvenience Sample\n\n\n\nChoosing individuals from a population who are easy to reach results in a convenience sample."
  },
  {
    "objectID": "sampling.html#random-sampling-methods",
    "href": "sampling.html#random-sampling-methods",
    "title": "6  Sampling",
    "section": "6.2 Random Sampling Methods",
    "text": "6.2 Random Sampling Methods\nIt’s important to avoid bias in our data so that we have conclusive results. A big idea in avoiding bias is to have a proper sampling method. A good sampling method always keeps the population in mind. Your sample should be representative of the population, which means that each individual that is part of your population should have an equal chance of being selected for your sample.\n\n\n\n\n\n\nSimple Random Sample (SRS)\n\n\n\nIn a SRS, each individual (the words) and each subgroup of individuals has the same chance of being chosen from the population of all words in the song.\n\n\n\n\n\n\n\n\nStratified Random Sample\n\n\n\nIn a stratified random sample, we first determine strata within our population. You can think of strata as subgroups of people that we divide based on the type/status of the things that we are sampling. The goal of the random sample is then to take an SRS from each strata (normally, you want the same amount of people from each stratum).\n\n\n\n\n\n\n\n\nRandom Cluster Sample\n\n\n\nIn a random cluster sample, we first determine clusters within our population. You can think of clusters as subgroups of people that we divide based on the location of the things that we are sampling (there should be no way to distinguish between the clusters besides where they are). The goal of the random sample is then to take select random clusters and then sample all people in the chosen clusters.\n\n\n\n\n\n\n\n\nSystematic Random Sample\n\n\n\nIn a systematic random sample, we randomly choose an interval (n) and/or a starting point at which to select individuals and then we select every nth individual.\n\n\n\n6.2.1 Box analogy\nAlthough not as descriptive as the above definitions, here’s how you can think about each sampling method:\nImagine that you have a box of balls, all of different weights and different colors (red, white, blue). Balls of similar colors tend to be more similar in weight compared to balls of different colors. We want to study the weights of the balls by randomly selecting some balls.\n\nWe can take a SRS from this box of balls by randomly picking out some balls. Use these randomly picked out balls as your sample.\nWe can take a Stratified Random Sample by first separating the balls into 3 separate boxes. One box with just the red balls, another with just the white balls, and a last one with just the blue balls. Lastly, take an SRS from each of the boxes by randomly taking out some balls from each of the boxes. Use these randomly picked out balls as your sample.\nWe can take a Cluster Sample by separating the balls into several different boxes. We determine out boxes by just taking maybe picking out 10 balls from the box (just take balls out top to bottom) and placing 10 balls in each box. Or we could dump out the balls and just section off the balls into different boxes. Take an SRS of the boxes by randomly selecting some boxes. Take out all of the balls from the boxes that you selected to be part of your sample.\nWe can take a Systematic Random Sample by taking out all the balls in the box and lining them up. Randomly pick one of the balls. From that ball onwards, pick every 5th ball to be part of your sample.\n\n\n\n6.2.2 Benefits of Methods\nAn SRS is an overall good method, giving us a random selection of multiple trees, but we can’t always trust it because it’s just left up to chance. There’s a chance that we select only individuals of a certain type, which is bad since we only have one chance to sample in real life.\nTo guarantee a good spread of people from all over, we would normally prefer a Systematic Random Sample, because by counting people off, you’ll be almost ensuring that you get a random spread of people from all of the population.\nIf we know that there are specific types of people within our population, then a better idea is to take a Stratified Random Sample. By first splitting people off in strata (again, this people types of people), we then are more capable of getting a good spread of people from each type of person.\nLastly, if all we want to do is save as much money and resources as possible, then we do a Random Cluster Sample, which does not take as much time and effort compared to the rest of the sampling methods."
  },
  {
    "objectID": "sampling.html#describing-sampling",
    "href": "sampling.html#describing-sampling",
    "title": "6  Sampling",
    "section": "6.3 Describing Sampling",
    "text": "6.3 Describing Sampling\n\n\n\n\n\n\nTip\n\n\n\nHere’s a general framework for how describing SRS’s should look like:\n\nAssign each individual in the population a number \\(1\\) to \\(N\\) (the population size).\nUse a random number generator to obtain \\(n\\) (sample size) unique numbers.\nSample the individuals whose numbers were generated.\n\n\n\nHere’s some examples of how this can work.\n\nImagine we have a forest of 1000 trees and I want to find out how old they are on average (and we don’t have the resources to visit each tree)\n\nTo take an SRS of 50 trees, we can\n\n\nNumber each tree with a number 1 to 1000.\nGenerate 50 unique random integers from 1 to 1000.\nSelect the trees corresponding to the numbers and record their age.\n\n\nTo take a random cluster sample of 50 trees, we can\n\n\nUse a map to split up the trees by plots of land (assume that each plot has 5 trees.\nNumber each plot of land with a number 1 to 200.\nGenerate 10 unique random integers from 1 to 200.\nSelect the plots corresponding to the numbers and record all the ages from the trees in the plots."
  },
  {
    "objectID": "experimental-design.html#experiment-principles",
    "href": "experimental-design.html#experiment-principles",
    "title": "7  Experimental Design",
    "section": "7.1 Experiment Principles",
    "text": "7.1 Experiment Principles\nIn general, the quality of experiments (their internal validity) can be judged based on the degree to which they have four things: comparison, randomization, control, and replication. Stronger internal validity gives us a better cause-effect link in our experiment. Whenever you are describing or evaluating the design of an experiment, you need to be sure to discuss all four of these!\n\n\n\n\n\n\nPrinciples of Experimental Design\n\n\n\nThese four determine how much internal validity we have in our experiment.\n\nComparison\n\nUse a design that compares two or more treatments.\n\nRandomization\n\nUse chance to assign experimental units to treatments. Doing so helps create roughly equivalent groups of experimental units by balancing the effects of other variables among treatment groups.\n\nControl\n\nKeep other variables that might affect the response the same for all groups.\n\nReplication\n\nUse enough experimental units in each group so that difference in the effects of the treatments can be distinguished from chance differences between the groups.\n\n\n\n\nThe logic of a randomized comparative experiment depends on our ability to treat all the subjects the same in every way except for the actual treatments being compared. Good experiments, therefore, require careful attention to details to ensure that all subjects really are treated identically.\n\n7.1.1 Placebos\n\n\n\n\n\n\nPlacebo Effect\n\n\n\nThe response to a dummy treatment is called the placebo effect. Subjects are given a placebo treatment to control for the placebo effect.\n\nFor example, If I tell someone that I am giving them an energy drink (when it in fact has doesn’t actually provide “energy”), and they feel like they have energy after, they have fallen for the placebo effect.\n\n\n\nIt’s well known that someone’s mental state can easily affect their physical state, so it’s important to control for the placebo effect. Typically, this applies to medicine settings, where you might give a pill with the actual medicine and a placebo (a pill with everything but the actual medicine) and conduct it in a blind.\n\n\n\n\n\n\nBlinding\n\n\n\nConducting an experiment in a blind means that you give treatments to patients without allowing them to know which treatment they are taking.\nHowever, Whenever possible, experiments with human subjects take it a bit further and conduct their experiment in a double-blind, where neither the subjects nor those who interact with them and measure the response variable know which treatment a subject received."
  },
  {
    "objectID": "experimental-design.html#experiment-designs",
    "href": "experimental-design.html#experiment-designs",
    "title": "7  Experimental Design",
    "section": "7.2 Experiment Designs",
    "text": "7.2 Experiment Designs\n\n7.2.1 Completely Randomized Design\nIn a completely randomized design, the experimental units are assigned to the treatments completely by chance. This is similar to (but NOT the same as) a simple random sample (SRS), because in both cases we ignore other variables. Here’s the difference: In an SRS, we’re picking some people (our sample) to study, and ignoring the rest. In a completely randomized experiment, however, we already have our sample (the people in our experiment), and we’re randomly deciding how we’re going to study each person (or, which treatment they’re going to get). So in complete randomization, the randomization is in the assignment, not in the selection, of people in our study.\n\n\n7.2.2 Randomized Block Design\nIn a randomized block design, the experimental units are first assigned to blocks according to the different types/status of the experimental units in the experiment. This is similar to stratified random sampling, however, we are not taking a sample. Any reference to stratified random sampling is wrong when describing an experiment design.\nAfter each experimental unit is assigned to their block, the experiment is carried out in each block, where a completely randomized design is carried out within the block.\nAfterwards, you compare and analyze results from each block and finally combine all results and analyze the differences between blocks.\n\n\n7.2.3 Matched Pairs Design\nA matched pairs design is a special case of a randomized block design that uses blocks of size 2. In this kind of design, you have to have “matched pairs.” In other words, you need to have two extremely similar individuals that make up each block. In some cases, you have a single person for each block and that person recieves both treatments in randomized order (because who is more similar to a person than themselves?)."
  },
  {
    "objectID": "experimental-design.html#inference",
    "href": "experimental-design.html#inference",
    "title": "7  Experimental Design",
    "section": "7.3 Inference",
    "text": "7.3 Inference\nThe main purpose of experiments is to be able to infer something about what we did. Does A actually affect B? Is it true for anyone else other than the people we experimented on?\n\n\n\n\n\n\nStatistical Significance\n\n\n\nAn observed effect so large that it would rarely occur by chance is said to be statistically significant. If we test something according to a single assumption that we make and find out that the data that we collect doesn’t really match up with that claim (if the chance of seeing data like the one we obtained is too low), then we’d say that it is statistically significant evidence.\n\n\n\n7.3.1 Scope of Inference\n\n\n\n\n\n\nScope of Inference\n\n\n\nThe scope of inference refers to the type of inferences (conclusions) that can be drawn from a study. The types of inferences we can make (inferences about the population and inferences about cause-and-effect) are determined by two factors in the design of the study.\n\n\n\n\n\n\n\n\n\n\nWere individuals randomly assigned to groups?\n\n\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nWere individuals randomly selected from a population?\n\n\nYes\n\n\nCan make inferences about the populationCan make inferences about cause and effect (Rare in the real world)\n\n\nCan make inferences about the populationCannot make inferences about cause and effect (Some observational studies)\n\n\n\n\nNo\n\n\nCannot make inferences about the populationCan make Inferences about cause and effect:   (Most experiments)\n\n\nCannot make inferences about the populationCannot make Inferences about cause and effect (Some observational studies)"
  },
  {
    "objectID": "probability.html#probability-rules",
    "href": "probability.html#probability-rules",
    "title": "8  Probability",
    "section": "8.1 Probability Rules",
    "text": "8.1 Probability Rules\nThe sample space \\(\\Omega\\) of a chance process is the set of all possible outcomes.\nA probability model is a description of some chance process that consists of two parts: a sample space \\(\\Omega\\) and a probability for each outcome.\nAn event is any collection of outcomes from some chance process. That is, an event is a subset of the sample space. Events are usually designated by capital letters, like \\(A\\), \\(B\\), \\(C\\), and so on.\n\n\n\n\n\n\nProbability Model\n\n\n\n\nFor any event A, \\(0 \\leq P(A) \\leq 1\\)\nIf \\(\\Omega\\) is the sample space in the probability model, \\(P(\\Omega)=1\\)\nIn the case of equally likely outcomes, \\(P(A)=\\frac{\\text{number of outcomes corresponding to event A}}{\\text{total number of outcomes in sample space}}\\)\n\n\n\nWe also have the following rules to calculate other things:\n\\(P(A|B)\\) : the probability of event \\(A\\) occurring, given that event \\(B\\) has already occurred. (conditional probability)\n\n\n\n\n\n\nIndependent Events\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if the occurrence of one has no effect on the likelihood of the other occurring (like successive coin flips).\nFor independent events, \\[P(A)=P(A|B) \\text{ and } P(B)=P(B|A)\\]\n\n\n\n\n\n\n\n\nMutually Exclusive (or Disjoint)\n\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive if it’s impossible for both to occur. In one flip of a coin, heads and tails are mutually exclusive events.\nFor mutually exclusive events,\n\\[P(A\\cap B)=0\\]\n\n\n\n8.1.1 Complement Rule\nThe complement to event \\(A\\) is the event that \\(A\\) doesn’t happen. This event is sometimes written \\(AC\\). In one flip of a coin, heads and tails are complementary events (assuming the coin won’t land on its side).\n\n\n\n\n\n\nComplementary Rule\n\n\n\nFor complementary events,\n\\[P(A)+P(A^c)=1 \\text{ or } P(A^c)=1-P(A)\\]\n\n\n\n\n8.1.2 Addition Rule\n\n\n\n\n\n\nAddition Rule\n\n\n\nThe general addition rule for any events \\(A\\) and \\(B\\) is:\n\\[P(A \\cup B)=P(A) + P(B)-P(A \\cap B)\\]\nFor mutually exclusive events, we know that \\(P(A\\cap B)=0\\), so we can also have the following simplified rule:\n\\[P(A \\cup B)=P(A) + P(B)\\]\n\n\n\n\n8.1.3 Multiplication Rule\n\n\n\n\n\n\nMultiplication Rule\n\n\n\nThe general multiplication rule for any events A and B is:\n\\[P(A\\cap B)=P(A)\\cdot P(B|A)\\]\nFor independent events \\(A\\) and \\(B\\), we know that \\(P(B|A) = P(B)\\), so we can also have the simplified rule:\n\\[P(A\\cap B)=P(A)\\cdot P(B)\\]"
  },
  {
    "objectID": "density-curves.html#percentiles",
    "href": "density-curves.html#percentiles",
    "title": "9  Density Curves",
    "section": "9.1 Percentiles",
    "text": "9.1 Percentiles\n\n\n\n\n\n\nPercentile\n\n\n\nThe \\(p^{th}\\) percentile is represented by a given value. \\(p\\)% of values are less than or equal to the given value."
  },
  {
    "objectID": "density-curves.html#cumulative-relative-frequency-graphs",
    "href": "density-curves.html#cumulative-relative-frequency-graphs",
    "title": "9  Density Curves",
    "section": "9.2 Cumulative Relative Frequency Graphs",
    "text": "9.2 Cumulative Relative Frequency Graphs\n\n\n\n\n\n\nCumulative Relative Frequency Graphs\n\n\n\nCumulative Relative Frequency graphs represent the cumulative relative frequency as the values increase (what percentage of values are less than or equal to each x-value). These always range from 0 to 1 on the y-axis (or 0% to 100%)."
  },
  {
    "objectID": "density-curves.html#normal-distributions",
    "href": "density-curves.html#normal-distributions",
    "title": "9  Density Curves",
    "section": "9.3 Normal Distributions",
    "text": "9.3 Normal Distributions\nIn the early 19th Century, the mathematicians Gauss and Adrian both noticed that many, many natural processes (like dimensions and weights of plants and animals) and many human processes (manufacturing, scores on tests) follow (approximately, not exactly) a very nice bell-shaped curve that Abraham de Moivre, an 18th century statistician and consultant to gamblers, had developed. This bell-shaped curve is now called the normal curve, and has the equation\n\\[P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\]\nWhere \\(P(x)\\) represents the height of the curve, \\(\\mu\\) is the mean (of all the \\(x\\)’s), and \\(\\sigma\\) is the standard deviation. If the mean is 0 and the standard deviation is 1, the graph looks like this:\n\n\n\n\n\n\n\n\n\n\n\nInflection Points\n\n\n\nNotice that the inflection points (where the curve changes its concavity, like in a cubic function) are at exactly one standard deviation from the mean, and by the time the graph gets to 3 standard deviationss in each direction, the height of the curve is basically zero.\n\n\n\n\n\n\n\n\nCheck Your Conditions\n\n\n\nThis is called the Standard Normal Distribution, or \\(N(0,1)\\). Remember that we use this to approximate real life, and we always need to check that this approximation is appropriate in a given situation.\n\n\n\n9.3.1 Characteristics of the Normal Distribution\n\n\n\n\n\n\nNormal Distribution Characteristics\n\n\n\n\nThey are always unimodal, bell-shaped, and symmetric about the mean\nThe x-axis represents different values of the variable, and the probability of any single value of x is zero (not true in real-life situations, but remember that this is an approximation).\nThe probability of a range of x-values is shown on the graph by the area under the curve for that range. The shaded area in the Standard Normal graph above represents the probability that \\(x\\) will be at least 1.5 standard deviations below the mean (about 10%).\nThe total area under the curve is 1, or 100%\nThe graph can be narrow or broad, depending on the standard deviation, but never skewed.\n\n\n\n\n\n9.3.2 Empirical Rule\n\n\n\n\n\n\nEmpirical Rule (68-95-99.7 Rule)\n\n\n\n\nabout 68% of the distribution is within 1 standard deviation of the mean\nabout 95% of the distribution is within 2 standard deviation of the mean\nabout 99.7% of the distribution is within 3 standard deviation of the mean\n\n\n\n\n\n\n\n\n\n\n9.3.3 Assessing for Normality\nWhen assessing normality, you have to think about what makes a normal distribution…. a normal distribution. Refer to the characteristics of a normal distribution to remind yourself what you need to look for.\n\n\n\n\n\n\nChecking Normality…\n\n\n\n\nGraph your data using a dot plot, histogram, boxplot, or stem plot. Whichever one is most convenient and would show the distribution of data the best.\nCompare the mean and median of the distribution. Normal curves are symmetric, so our data should also be symmetric. Are the mean and median almost the same?\nCheck to see if the data follows the empirical rule. Is there 68% of the data between -1 and 1 standard deviations? What about 95% and 99.7%?\nLook at the normal probability plot if given. (Know how to interpret and read, not make). If the plot is fairly linear (does a straight line fit through the points), then we have further evidence that the distribution is approximately normal\n\n\n\n\n\n9.3.4 Standardized Score\n\n\n\n\n\n\nz-score\n\n\n\nIf \\(x\\) is an observation that has a known mean and standard deviation, the standardized score of \\(x\\) is: \\[z = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}\\] A standardized score, often called a z-score, of an observation is the number of standard deviations it is from the mean.\n\n\n\n\n9.3.5 Calculating Probabilities and Percentiles\nUsing calculus, we could calculate the area (and therefore the probability) under any section of any normal distribution. But we don’t want to have to use calculus every time we want to calculate a normal probability. So, we use a table (inside the front cover of your book and also in Table A) in which many Standard Normal probabilities have been calculated for us. So all we need to do is convert our problem to Standard Normal, and we can use this table. We make this conversion using z-scores.\n\n\n\n\n\n\nImportant\n\n\n\nOnce we have a z-score, we can use the Standard Normal table to find a corresponding probability.\n\n\n\n\n9.3.6 Using your Calculator\nRemember that to access the calculator functions related to distributions on a TI-83/84 series, you go to:\n2nd -&gt; vars (distr)\nThe two functions of interest for this chapter are:\nnormalcdf(lowerbound, upperbound [, mu, sigma])\n\ninvNorm(area to the left of value [, mu, sigma])\nWhen using your calculator to calculate Normal probabilities you need to make sure you write at least the following information to ensure full credit:\n\nThe name of the distribution\nThe parameters of the distribution (for normal distributions, the mean and s.d.)\nHow to calculate the test statistic (for normal distributions, the z-score)\nThe probability you are finding\nThe answer in context of the problem\n\n\nExample\nSuppose that the time you need to spend on BART to get from Downtown Berkeley to the SF Airport is approximately normally distributed with \\(\\mu = 54\\) minutes, and \\(\\sigma = 4.6\\) minutes. In other words, BART times from Berkeley to SFO \\(\\sim N(54, 4.6)\\).\nNow find each probability (\\(X\\) represents the amount of time required for a single randomly selected BART trip from Berkeley to SFO):\n\n\\(P(X&lt;53)\\) \\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X&lt;53) &\\approx P(z &lt; -.22) \\underset{\\text{Table A}}{=} 0.4129\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\nz &= \\frac{53 - 54}{4.6} \\approx -0.22 \\\\\nP(X&lt;53) &\\approx P(z &lt; -0.22) \\\\\n&= \\texttt{normalcdf(-1000, -0.22)} \\approx 0.4129\n\\end{aligned}\n\\]\n\n\n\\(P(X&gt;60)\\) \\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X&gt;60) &\\approx P(z &gt; 1.30) \\\\\n&= 1 - P(z &lt; 1.30) \\underset{\\text{Table A}}{=} 1 - 0.9032  = 0.0968\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\nz &= \\frac{60 - 54}{4.6} \\approx 1.30 \\\\\nP(X&gt;60) &\\approx P(z &gt; 1.30) \\\\\n&= \\texttt{normalcdf(1.30, 1000)} \\approx 0.0968\n\\end{aligned}\n\\]\nThe lowest 5% of travel times are below how many minutes? \\[\n\\begin{aligned}\n0.05 & \\underset{\\text{Table A}}{\\approx} P(z&lt;-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\] Using calculator: \\[\n\\begin{aligned}\n\\texttt{invNorm(0.05)}& \\approx P(z&lt;-1.64) \\\\\n\\rightarrow z = \\frac{x - 54}{4.6} &= -1.64 \\\\\nx &= 46.45 \\text{ minutes}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "random-variables.html#discrete-random-variables",
    "href": "random-variables.html#discrete-random-variables",
    "title": "10  Random Variables",
    "section": "10.1 Discrete Random Variables",
    "text": "10.1 Discrete Random Variables\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA discrete random variable describes a process that only has specific, predefined numerical outcomess. For example, we can use a discrete random variable to finding the probability of Michael Jordon scoring 50 points in a single game or the probability of rolling a 3 or lower on a six-sided die.\n\n\n\n\n\n\n\n\nMean and SD of Discrete Random Variable\n\n\n\nWhen we have a discrete random variable \\(X\\) whose probability distribution is\n\\[\n\\begin{aligned}\n    \\textbf{Value:}& ~~~~ x_1 ~~~~ x_2 ~~~~ x_3 ~~~~ \\cdots \\\\\n    \\textbf{Probability:}& ~~~~ p_1 ~~~~ p_2 ~~~~ p_3 ~~~~\\cdots\n\\end{aligned}\n\\]\nwe know the following about the mean and standard deviation of \\(X\\):\n\\[\\mu_X = E(X) = \\sum x_i P(x_i)\\]\n\\[\\sigma_X = \\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\]\n\n\n\n\nKeep in mind that the standard deviation formula here (and else where) is equivalent to the formula of population standard deviation when we divide by \\(n\\) instead of multiplying by \\(P(x_i)\\).\n\nJust as a quick reminder, our formula for sample standard deviation is:\n\\[s_X = \\sqrt{\\frac{\\sum(x_i - \\bar x)^2}{n - 1}}\\]\nOur formula for population standard deviation (which we won’t use in this class is)\n\\[\\sigma_X = \\sqrt{\\frac{\\sum(x_i - \\mu_X)^2}{n}}\\]\nTaking this formula, we can notice that:\n\\[\n\\begin{aligned}\n\\sigma_X &= \\sqrt{\\frac{\\sum(x_i - \\mu_X)^2}{n}} &(1)\\\\\n&=\\sqrt{E((X - \\mu_X)^2)} &(2)\\\\\n&=\\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)} &(3)\\\\\n\\end{aligned}\n\\]\nIn other words, remember that we define standard deviation as the root mean of the squared differences from the mean.\nWe get to step (2) by recognizing that by adding up all the squared differences from the mean and dividing by \\(n\\), we are taking the mean of the squared differences. Therefore, \\(\\frac{\\sum(x_i - \\mu_X)^2}{n} = E((X - \\mu_X)^2)\\)\nAnd in the case of discrete random variables, we can find the mean of \\(X\\) as \\(\\mu_X = E(X) = \\sum x_iP(x_i)\\) which allows us to rewrite from step (2) to (3)\n\n\n10.1.1 Binomial Random Variables\nBinomial random variables have parameters \\(n\\) and \\(p\\), and can be written \\(B(n, p)\\). Remember, Normal random variables have parameters and and can be written \\(N(\\mu,\\sigma)\\).\n\n\n\n\n\n\npdf of a Binomial Random Variable\n\n\n\nThe pdf of a Binomial Random Variable (i.e. the binomial formula) is:\n\\[\n\\begin{aligned}\n    P(X=k) &= {n \\choose k} p^k (1-p)^{n-k}\\\\\n    \\text{where } k &= 0, 1, 2, 3, \\cdots, n\n\\end{aligned}\n\\]\nTo apply this formula in a graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; binompdf}\\)\nUsage: \\(\\texttt{binompdf(n, p[,x])}\\)\n\n\n\n\n\n\n\n\ncdf of a Binomial Random Variable\n\n\n\nThe cdf of a Binomial Random Variable is:\n\\[\n\\begin{aligned}\n    P(X\\leq k) &= \\sum_{i = 0}^n {n \\choose i} p^i (1-p)^{n-i}\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; binomcdf}\\)\nUsage: \\(\\texttt{binomcdf(n, p[,x])}\\)\n\n\n\n\n\n\n\n\nMean and SD of Binomial Random Variable\n\n\n\nThe mean and standard deviation of a binomial random variable is given by:\n\\[\n\\begin{aligned}\n    \\mu_X &= n p \\\\\n    \\sigma_X &= \\sqrt{n p q} \\\\\n    &\\text{, where } q = 1-p.\n\\end{aligned}\n\\]\n\n\n\nIdentifying Binomial Situations\n\n\n\n\n\n\nThe Binomial Setting\n\n\n\nWe can identify a binomial setting when we successfully recognize these four things about a random phenonmenon:\n\nBinary?\nThe possible outcomes of each trial can be classified as “success” or “failure”.\nIndependent?\nTrials must be independent; that is, knowing the result of one trial must not tells us anything about the result of another trial.\nNumber?\nThe number of trials n of the chance process must be fixed in advance.\nSame?\nThere is the same probability p of success on each trial.\n\n\n\n\n10% Condition\nThe second condition is often not perfectly met, as in the case of an SRS from some population. Imagine choosing 10 students from a class of 15 females and 15 males—as we choose people, the remaining population changes, which changes the probability that the next person chosen will be male or female.\nWhen we lack complete independence, we can see the consequence of this is negligible as long as our sample is small relative to the population from which we are sampling. If we were choosing our 10 people from a school of 3,300 students, the change in probability from person to person would be small enough to ignore.\n\n\n\n\n\n\n10% Condition\n\n\n\nThe general rule is that the sample needs to be less than \\(\\frac{1}{10}\\), or 10%, of the population. We refer to this as the 10% condition. \\[\nn \\leq (.10) N\n\\]\n\n\n\n\n\nNormal Approximation to the Binomial Distribution\nRemember that as \\(n\\) gets large, a binomial random variable \\(X\\) can take on more and more different values, and it can become tedious to continue to treat X as a discrete random variable. As \\(n\\) get larger, we can treat \\(X\\) as a continuous random variable, more specifically:\n\n\n\n\n\n\nImportant\n\n\n\nAs \\(n\\) gets larger, the binomial distribution gets closer to a normal distribution.\n\n\nHowever, before we use a normal distribution to approximate a binomial distribution, we have to check the following condition:\n\nLarge Counts condition\n\n\n\n\n\n\nLarge Counts Condition\n\n\n\nWe can use a Normal distribution to model a binomial distribution if we know that a the Large Counts Condition is fulfilled: \\[np \\geq 10 \\text{ and }n(1-p) \\geq 10\\] In other words, if the expected number of successes and failures (respectively) is greater than or equal to 10.\n\n\n\n\nDoing a Normal Approximation to a Binomial Distribution\nFirst verify all the conditions for a Binomial setting and the Large Counts Condition. Since we know that we have a binomial setting, we then know the distribution that we want to use is \\(Normal(np, \\sqrt{npq})\\) proceed with the calculations according to this distribution.\n\n\n\n\n10.1.2 Geometric Random Variables\nIf \\(X \\sim G(p)\\), \\(X\\) has a geometric distribution with a parameter \\(p\\) probability of “success”.\n\n\n\n\n\n\npdf of a Geometric Random Variable\n\n\n\nThe pdf of a geometric random variable is:\n\\[\n\\begin{aligned}\n    P(X=x) &= (1-p)^{x-1}p\\\\\n    \\text{where } x &= 1, 2, 3, \\cdots\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; geometpdf}\\)\nUsage: \\(\\texttt{geometpdf(p, x)}\\)\n\n\n\n\n\n\n\n\ncdf of a Geometric Variable\n\n\n\nThe cdf of a Geometric Variable is:\n\\[\n\\begin{aligned}\n    P(X\\leq x) &= \\sum_{i=1}^x(1-p)^{i-1}p\n\\end{aligned}\n\\]\nIn graphing calculator: \\(\\texttt{2nd -&gt; vars (distr) -&gt; geometcdf}\\)\nUsage: \\(\\texttt{geometcdf(p, x)}\\)\n\n\n\n\n\n\n\n\nMean and SD of a Geometric Random Variable\n\n\n\nThe mean and standard deviation of a geometric random variable is given by:\n\\[\n\\begin{aligned}\n    \\mu_X &= \\frac{1}{p} \\\\\n    \\sigma_X &= \\frac{\\sqrt{q}}{p} \\\\\n    &\\text{, where } q = 1-p.\n\\end{aligned}\n\\]\n\n\n\nIdentifying Geometric Settings\nA geometric setting is very similar to a binomial setting, except that \\(n\\) the number of trials is not fixed and that we are instead finding probabilities until the first “success” occurs.\n\n\n\n\n\n\nThe Geometric Setting\n\n\n\nA geometric setting is defined as a series of observations where these 4 conditions are met:\n\nBinary?\nThe possible outcomes of each trial can be classified as “success” or “failure”\nIndependent?\nTrials must be independent, that is, knowing the result of one trial must not have any effect on the result of any other trial.\nTrials until?\nThe goal is to count the number of trials until the first success occurs.\nSuccess?\nOn each trial, the probability \\(p\\) of success must be the same."
  },
  {
    "objectID": "random-variables.html#operations-with-random-variables",
    "href": "random-variables.html#operations-with-random-variables",
    "title": "10  Random Variables",
    "section": "10.2 Operations with Random Variables",
    "text": "10.2 Operations with Random Variables\n\n10.2.1 Linear Transformations\nWhen we add a constant \\(a\\) and/or multiply by a constant \\(b\\) to a random variable \\(X\\), we perform a linear transformation of the form\n\\[\na + bX\n\\]\n\n\n\n\n\n\nMean and SD of Linearly Transformed Random Variables\n\n\n\nThe mean of \\(a + bX\\) is transformed in the same way that we change \\(X\\)\n\\[\n\\mu_{a+bX}=a+\\mu_{bX}=a+b\\mu_X\n\\]\nThe standard deviation of \\(a + bX\\) is only affected by multiplication:\n\\[\n\\sigma_{a+bX}=\\sigma_{bX}=b\\sigma_X\n\\]\n\n\n\n\n10.2.2 Sum and Difference between Random Variables\nIn addition to linear transformation, we can also, to an extent, describe the result of adding or subtracting random variables\nIf we have two random variables \\(X\\) and \\(Y\\), we can describe the mean and standard deviation of the sum or difference of these with the following.\n\n\n\n\n\n\nMean and SD of the Sum and Difference between Random Variables\n\n\n\n\\[\\mu_{X \\pm Y} = \\mu_X \\pm \\mu_Y\\]\nIf the random variables \\(X\\) and \\(Y\\) are independent, then\n\\[ \\sigma_{X \\pm Y}^2 = \\sigma_X^2 + \\sigma_Y^2 \\]\nAnd it follows that:\n\\[ \\sigma_{X \\pm Y} = \\sqrt{\\sigma_X^2 + \\sigma_Y^2} \\]\n\n\nWhat is \\(\\sigma_X^2\\)?\n\nThe variance of a random variable \\(X\\) is:\n\\[\nVar(X) = \\sigma_X^2\n\\]\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that the formula for the standard deviation here only applies if we know that \\(X\\) and \\(Y\\) are independent of each other."
  },
  {
    "objectID": "random-variables.html#interpretations",
    "href": "random-variables.html#interpretations",
    "title": "10  Random Variables",
    "section": "10.3 Interpretations",
    "text": "10.3 Interpretations\n\n10.3.1 Probabilities and Means\nBe aware that you are still expected to know how to interpret probabilities. Now that we are working with means / expected values, we are going to extend our long run probability interpretation to means too.\nRemember, if we are given a probability \\(p\\), we interpret it in the form of:\n\nIf [do the situation] many many times, [this thing] will happen about [\\(p\\)] of the time.\n\nThis extends to any mean \\(\\mu\\):\n\nIf [do the situation] many many times, we will have an average about [\\(\\mu\\)].\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that you should be describing the situation properly.\nIf we are looking at the average amount of heads in 50 coin flips, we’d say:\n\nIf we flip a coin 50 times many many times, we will see an average of about 25 heads.\n\nOr if we are looking at the probability that we get 25 heads in 50 coin flips (which is \\(\\approx\\) 0.1123)\n\nIf we flip a coin 50 times many many times, we will observe getting 25 heads about 0.1123 of the time.\n\n\n\n\n\n10.3.2 Standard Deviation\nWe previously interpreted standard deviation in Chapter 1, and now we can tone down the interpretation, we’d just say something along the lines of\n\n[The variable] typically varies by [standard deviation] from the [mean]."
  },
  {
    "objectID": "sampling-distributions.html#bias-and-variability",
    "href": "sampling-distributions.html#bias-and-variability",
    "title": "11  Sampling Distributions",
    "section": "11.1 Bias and Variability",
    "text": "11.1 Bias and Variability\n\n\n\n\n\n\nUnbiased Estimator\n\n\n\nA statistic used to estimate a parameter is an unbiased estimator if the mean of its sampling distribution is equal to the true value of the parameter being estimated.\nNot all statistics are unbiased estimators and the data that we collect always has to follow a random process that is representative of the population.\n\n\nFor this course, typical statistics that we’ll use in problems include \\(\\hat p\\), \\(\\bar x\\), and \\(s_x\\). These statistics are only unbiased if we have a random sample or random assignment.\n\\[\\mu_{\\bar x} = \\mu_X\\] \\[\\mu_{\\hat p} = p\\] \\[\\mu_{s_x} = \\sigma_X\\]\n\n\n\n\n\n\nVariability of a Statistic\n\n\n\nThe variability of a statistic is described by the spread of its sampling distribution.\nThis spread is determined mainly by the size of the random sample. Larger samples give smaller spreads. The spread of the sampling distribution does not depend much on the size of the population, as long as the population is at least 10 times larger than the sample (10% Condition).\n\n\nIntuitively, if we have more data, we will have more precise results. Precision means that, on average, we will be able to get answers that are more closely grouped together. This means that our statistics, between many different possible samples, will vary less from each other.\n\n\n\n\n\n\nImportant\n\n\n\nIn the most ideal conditions, we want our sampling distribution to be accurate and precise, so that we know that we are getting the best possible answer.\nTo be accurate, we want to expect our statistic to be the parameter (e.g. if we were to take the sample mean, we’d want to see that \\(\\mu_{\\bar x} = \\mu_X\\)) and for that to be true, our statistic needs to be an unbiased estimator.\nAnd to ensure that we will get something close to desired parameter, we want precision. This means a larger sample size so that our sampling distribution has low spread, reducing the variability of our statistic.\nIn ideal circumstances, we aim for low bias, low variability. This is achieved by mainly ensuring that we have a random process for obtaining our data and collecting a large amount of data."
  },
  {
    "objectID": "sampling-distributions.html#sampling-distribution-of-hat-p",
    "href": "sampling-distributions.html#sampling-distribution-of-hat-p",
    "title": "11  Sampling Distributions",
    "section": "11.2 Sampling Distribution of \\(\\hat p\\)",
    "text": "11.2 Sampling Distribution of \\(\\hat p\\)\n\n\n\n\n\n\nSampling Distribution of \\(\\hat p\\)\n\n\n\nChoose an SRS of size \\(n\\) from a population of size \\(N\\) with proportion \\(p\\) of successes. Let \\(\\hat p\\) be the sample proportion of success. Then:\n\nThe mean of the sampling distribution of \\(\\hat p\\) is \\[\\mu_{\\hat p} = p\\]\nThe standard deviation of the sampling distribution of \\(\\hat p\\) is \\[\\sigma_{\\hat p} = \\frac{\\sigma_p}{\\sqrt{n}} = \\sqrt{\\frac{p(1-p)}{n}}\\] as long as the 10% condition is satisfied.\nAs \\(n\\) increases, the sampling distribution of \\(\\hat p\\) becomes approximately Normal. Before you perform calculations, check that the Large Counts condition is satisfied:\n\n\\[\\begin{equation}\nnp \\geq 10 \\text{ and }n(1-p) \\geq 10\n\\end{equation}\\]"
  },
  {
    "objectID": "sampling-distributions.html#sampling-distribution-of-bar-x",
    "href": "sampling-distributions.html#sampling-distribution-of-bar-x",
    "title": "11  Sampling Distributions",
    "section": "11.3 Sampling Distribution of \\(\\bar x\\)",
    "text": "11.3 Sampling Distribution of \\(\\bar x\\)\n\n\n\n\n\n\nSampling Distribution of \\(\\bar x\\)\n\n\n\nSuppose that \\(\\bar x\\) is the mean of an SRS of size \\(n\\) drawn from a large population with mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\). Then:\n\nThe mean of the sampling distribution of \\(\\bar x\\) is \\[\\mu_{\\bar x} = \\mu_X\\]\nThe standard deviation of the sampling distribution of \\(\\bar x\\) is \\[\\sigma_{\\bar x} = \\frac{\\sigma_X}{\\sqrt{n}}\\] as long as the 10% condition is satisfied.\n\n* These facts about the mean and standard deviation of \\(\\bar x\\) are true no matter what shape the population distribution has.\n\n\n\n11.3.1 Normal/Large Condition for \\(\\bar x\\)\n\n\n\n\n\n\nNormal/Large Condition for \\(\\bar x\\)\n\n\n\nIf the population distribution is Normal, then so is the sampling distribution of \\(\\bar x\\). This is true no matter what the sample size \\(n\\) is.\nIf the population distribution is not Normal, the sampling distribution of \\(\\bar x\\) will be approximately Normal in most cases if \\(n \\geq 30\\). This fact is justified by the Central Limit Theorem.\n\n\n\n\n11.3.2 Central Limit Theorem (CLT)\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nIf you have a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) and take sufficiently large random samples from the population with replacement, then the distribution of the sample means will be approximately normal distributed.\n* This allows us to use Normal probability calculations to answer questions about sample means/proportions from many observations even when the population distribution is not Normal\n\n\nThe CLT justifies why we have the check the Large Condition above and the section below. Only statistics where our sample size is sufficient will have approximately normally distributed sampling distributions.\n\n\n11.3.3 When do we have normality?\n\n\n\n\n\n\n\nShape of Population Distribution\nMinimum Sample Size (\\(n\\)) needed to achieve an approximately normal \\(\\bar x\\) sampling distribution\n\n\n\n\nApproximately Normal\n1\n\n\nSlightly non-normal\nSomewhere between 10-30, depending on how “slightly” non-normal\n\n\nVery non-normal, skewed\nAbout 30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.highlight}\nChoose an SRS of size \\(n_1\\) from Population 1 with mean \\(\\mu_1\\) and standard deviation \\(\\sigma_1\\) and an independent SRS of size \\(n_2\\) from Population 2 with mean \\(\\mu_2\\) and standard deviation \\(\\sigma_2\\).\nShape:\nWhen the population distributions are Normal, the sampling distribution of \\(\\bar x_1- \\bar x_2\\) is approximately Normal. In other cases, the sampling distribution will be approximately Normal if the sample sizes are large enough (\\(n_1 \\geq 30\\) and \\(n_2 \\geq 30\\))\nCenter:\nThe mean of the sampling distribution is\n\\[\\mu_{\\bar x_1- \\bar x_2} = \\mu_1- \\mu_2\\].\nSpread:\nAs long as each sample is no more than 10% of its population (10% condition), the standard deviation of the sampling distribution of \\(\\bar x_1- \\bar x_2\\) is\n\\[\\sigma_{\\bar x_1- \\bar x_2} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\]"
  },
  {
    "objectID": "confidence-intervals.html#interpreting-confidence-intervals",
    "href": "confidence-intervals.html#interpreting-confidence-intervals",
    "title": "12  Confidence Intervals",
    "section": "12.1 Interpreting Confidence Intervals",
    "text": "12.1 Interpreting Confidence Intervals\nYou can say something of the form:\n\n\n\n\n\n\nConfidence Interval Interpretation\n\n\n\nI am [\\(C\\)]% confident the interval from [lower bound] to [upper bound] captures the [true parameter in the context of the problem].\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou CANNOT say that “There is a 95% chance that this interval contains the population (proportion or mean)” because your single interval either did or did not capture the real value, and either has a probability of 0 or 1"
  },
  {
    "objectID": "confidence-intervals.html#interpreting-confidence-level",
    "href": "confidence-intervals.html#interpreting-confidence-level",
    "title": "12  Confidence Intervals",
    "section": "12.2 Interpreting Confidence Level",
    "text": "12.2 Interpreting Confidence Level\n\n\n\n\n\n\nConfidence Level Interpretation\n\n\n\nIf we take many, many samples and create many, many intervals using the same method, about [\\(C\\)]% of them will capture the [true parameter in the context of the problem].\n\n\nIn other words, you can say that: “95% of intervals created in this manner will capture the true population (proportion or mean)”"
  },
  {
    "objectID": "confidence-intervals.html#changing-confidence-levels",
    "href": "confidence-intervals.html#changing-confidence-levels",
    "title": "12  Confidence Intervals",
    "section": "12.3 Changing Confidence Levels",
    "text": "12.3 Changing Confidence Levels\nWhen we increase confidence level, we are saying that more intervals in this manner should capture the true parameter. This means that to increase confidence level means that the confidence intervals need to increase in width.\nWhen we increase the size of a sample, the precision of our statistic increases because of the reduction in sampling variability (standard deviation of the statistic decreases as n increases). This means that we can now guess/estimate a smaller range compared to if we had a smaller sample size. So, when we increase the size of our samples, our confidence intervals will have smaller widths compared to those with a smaller sample size."
  },
  {
    "objectID": "confidence-intervals.html#conditions-for-confidence-intervals",
    "href": "confidence-intervals.html#conditions-for-confidence-intervals",
    "title": "12  Confidence Intervals",
    "section": "12.4 Conditions for Confidence Intervals",
    "text": "12.4 Conditions for Confidence Intervals\n\n1. Random\nThe data must come from a random sample, or random assignment in an experiment.\nIn order to infer about a larger population, we must have a random, unbiased sample from that population. If the sample is not random, we need to think about whether it’s at least unbiased and/or representative. If so, we may be able to treat it as a random sample, but we must state that we’re doing this. If you’re not told that a sample is random, you can write something like “I’m going to have to assume I can treat this as a random sample”.\nIn the case of experiments, we can perform statistical tests on the data from our experimental groups, even if we don’t have a random sample from a larger population. In this case, we must make sure we have internal validity in our experiment (a well-conducted experiment with control, replication, and random assignment of treatments). If we do, our results are valid for our subjects, but we may or may not be able to generalize to a larger population. This depends on whether our subjects are a random sample from, or at least representative of, some larger population.\n\n\n2. Large\nYou must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.\n\nProportions\nFor proportions, remember, the population is never anywhere near normal (it’s always two bars, yes and no). In this case we check the Large Counts Condition.\nThis ensures that our sample proportion can take on enough different values (and make enough bars in a histogram) to create an approximately normal sampling distribution.\n\n\nSample means\nIn the case of sample means, the sampling distribution is approximately normal if at least one of the following conditions are met.\n\nNormal/Large Condition\nWhen \\(n&lt;30\\), check if the graph of the sample data does not show any strong skew or outliers. Otherwise,\n\n\n\n\n3. Sampling Independence\nObservations in our sample must be independent of each other.\nIn random samples from a population, observations are never independent because the population changes with every person we sample and remove from it. However, this effect is small enough to ignore as long as the population from which we’re sampling is at least 10 times as large as our sample. This needs to be stated!\nIn experiments, or other situations where we’re not sampling from a population (rolls of dice for example), we just need to think about whether the outcome of one observation could be having any effect on the outcome of another observation. Often we don’t know this, so we need to state that we’re assuming it; “I’m going to have to assume I can treat these observations as independent.”"
  },
  {
    "objectID": "confidence-intervals.html#the-four-step-process",
    "href": "confidence-intervals.html#the-four-step-process",
    "title": "12  Confidence Intervals",
    "section": "12.5 The Four-Step Process",
    "text": "12.5 The Four-Step Process\n\nSTATE exactly what you’re doing.\n\n\n\n\n\n\n\nState\n\n\n\nBe sure to be specific here, especially with regard to defining the actual numbers you will be analyzing. You need to demonstrate your understanding of the population about which you’re inferring, the procedure you’re using, and the parameter of interest.\n\n\n\nI will estimate \\(p =\\) the proportion of all non-solar Berkeley homeowners who would install solar panels if they knew that they could recover the cost in 10 years with 92% confidence.\n\n\nPLAN which method you will use and the conditions required to use the method.\n\n\n\n\n\n\n\nPlan\n\n\n\nState the name of the procedure that you’ll attempt.\nCheck the conditions, and if the conditions are not met, or if you can’t be sure they’re met, you need to explain what needs to be true in order for the conditions to be met.\n\n\n\nIf the conditions are met, I will use a one-sample z-interval for the population parameter \\(p\\).\nRandom Sample: Yes, it’s stated that the sample is random.\nLarge Sample: \\(n\\hat{p}=75\\left(\\frac{43}{75}\\right)=43\\) and \\(n\\left(1-\\hat{p}\\right)=75\\left(1-\\frac{43}{75}\\right)=32\\)\nBoth amounts are over 10, so the sample is large enough for the sampling distribution of to be approximately normal.\nIndependence: We have independence as long as the population of non-solar Berkeley homeowners is at least 750. This is definitely true.\n\n\nDO the calculations\n\n\n\n\n\n\n\nDo\n\n\n\nPerform the calculations necessary for the method you chose to use. Show all work!\n\n\n\n\\[\n\\begin{aligned}\nz^\\ast &= \\mathtt{invNorm((1 - 0.92) / 2 + 0.92)} \\approx 1.75 \\\\ \\\\\nCI_{92}&=\\hat{p}\\pm z^\\ast\\cdot SE_{\\hat{p}} \\\\\n&=\\hat{p}\\pm1.75\\cdot\\sqrt{\\frac{\\hat{p}\\left(1-\\hat{p}\\right)}{n}} \\text{, and we know that } n=75 \\text{ and } \\hat{p}=\\frac{43}{75}=0.5733 \\\\\n&=0.5733\\pm1.75\\sqrt{\\frac{0.5733\\left(1-0.5733\\right)}{75}} \\\\\n&=\\left[0.4734,\\ 0.6732\\right]\n\\end{aligned}\n\\]\n\n\nCONCLUDE in the context of the situation.\n\n\n\n\n\n\n\nConclude\n\n\n\nInterpret the confidence interval in the context of the problem.\n\n\n\nI am 92% confident that \\(p\\) (as defined in State) is somewhere between 47.34% and 67.32%."
  },
  {
    "objectID": "hypothesis-testing.html#significance-tests",
    "href": "hypothesis-testing.html#significance-tests",
    "title": "13  Hypothesis Testing",
    "section": "13.1 Significance Tests",
    "text": "13.1 Significance Tests\nConfidence intervals are one of the two most common types of statistical inference. Use a confidence interval when your goal is to estimate a population parameter. The second common type of inference, called significance tests, has a different goal: to assess the evidence provided by data about some claim concerning a population.\nA significance test is a formal procedure for comparing observed data with a claim (also called a hypothesis) whose truth we want to assess. The claim is a statement about a parameter, like the population proportion \\(p\\) or the population mean \\(\\mu\\). We express the results of a significance test in terms of a probability that measures how well the data and the claim agree.\n\n13.1.1 Hypotheses\nA significance test starts with a careful statement of the claims we want to compare.\n\n\n\\(H_0\\): The claim we weigh evidence against in a statistical test is called the null hypothesis. Often the null hypothesis is a statement of “no difference.”\n\nIn any significance test, the null hypothesis has the form:\n\n\n\\[H_0 : \\text{parameter} = \\text{null value} \\]\n\n\\(H_a\\): The claim about the population that we are trying to find evidence for is the alternative hypothesis.\n\nThe alternative hypothesis has one of the forms:\n\n\n\\[\n\\begin{aligned}\nH_0 &: \\text{parameter} &gt; \\text{null value}\\\\\nH_0 &: \\text{parameter} &lt; \\text{null value}\\\\\nH_0 &: \\text{parameter} \\not = \\text{null value}\n\\end{aligned}\n\\]\n\nTo determine the correct form of \\(H_a\\), read the problem carefully.\n\nThe alternative hypothesis is one-sided if it states that a parameter is larger than the null hypothesis value or if it states that the parameter is smaller than the null value.\nIt is two-sided if it states that the parameter is different from the null hypothesis value (it could be either larger or smaller).\n\n\nThe hypotheses should express the hopes or suspicions we have before we see the data. It is cheating to look at the data first and then frame hypotheses to fit what the data show (i.e. “p-hacking”).\nHypotheses always refer to a population, not to a sample. Be sure to state \\(H_0\\) and \\(H_a\\) in terms of population parameters.\nIt is never correct to write a hypothesis about a sample statistic, such as \\(\\hat p=0.64\\) or \\(\\bar x=85\\).\n\n\n\n13.1.2 Interpreting P-values\nThe null hypothesis \\(H_0\\) states the claim that we are seeking evidence against. The probability that measures the strength of the evidence against a null hypothesis is called a P-value. The probability, computed assuming \\(H_0\\) is true, that the statistic would take a value as extreme or more extreme than the one actually observed is called the P-value of the test. Small P-values are evidence against \\(H_0\\) because they say that the observed result is unlikely to occur when \\(H_0\\) is true. Large P-values fail to give convincing evidence against \\(H_0\\) because they say that the observed result is likely to occur by chance when \\(H_0\\) is true.\n\n\n13.1.3 Drawing a Conclusion\nThe final step in performing a significance test is to draw a conclusion about the competing claims you were testing. We make one of two decisions based on the strength of the evidence against the null hypothesis (and in favor of the alternative hypothesis):\n\\[\\text{reject } H_0 \\text{ or fail to reject } H_0\\]\nNote: A fail-to-reject \\(H_0\\) decision in a significance test doesn’t mean that \\(H_0\\) is true. For that reason, you should never “accept \\(H_0\\)” or use language implying that you believe \\(H_0\\) is true.\nThere is no rule for how small a P-value we should require in order to reject \\(H_0\\). But we can compare the P-value with a fixed value that we regard as decisive, called the significance level. We write it as \\(\\alpha\\), the Greek letter alpha.\n\nIf the P-value is smaller than alpha, we say that the data are [statistically significant][Statistical Significance] at the level . In that case, we reject the null hypothesis \\(H_0\\) and conclude that there is convincing evidence in favor of the alternative hypothesis \\(H_a\\).\nIn a nutshell, our conclusion in a significance test comes down to\n\\[\n\\begin{aligned}\n\\text{P-value} &&lt;  \\alpha \\rightarrow \\text{reject } H_0 \\rightarrow \\text{convincing evidence for } H_a \\\\\n\\text{P-value} &\\geq \\alpha \\rightarrow\\text{fail to reject } H_0  \\rightarrow \\text{not convincing evidence for } H_a\n\\end{aligned}\n\\]\n\n\n\n13.1.4 Example\n(From “Is this gender discrimination” CYU)\\ Factinate.com claims that 84% of teenagers think highly of their mother. To investigate this claim, a school psychologist selects a random sample of 150 teenagers and finds that 135 think highly of their mother. Do these data provide convincing evidence that the true proportion of teens who think highly of their mother is greater than 0.84?\n\nState appropriate hypotheses for performing a significance test. Be sure to define the parameter of interest.\n\n\nLet \\(p =\\) the proportion of teenagers who think highly of their mother.  \n\\(H_0: p = 0.85\\) (This is because we are assuming there is “no difference” from Factinate.com’s claim)  \n\\(H_0: p &gt; 0.85\\) (The evidence that we got is \\(\\frac{135}{150}\\), which is greater than 0.85; also the problem asks, “Do these data provide convincing evidence that the true proportion of teens who think highly of their mother is greater than 0.84?”)\n\n\nThe school psychologist performed the significance test and obtained a P-value of 0.0225. Interpret the P-value.\n\n\nAssuming that that \\(H_0\\) is true (i.e. \\(p = 0.84\\)), there is a 0.0225 probability of getting a sample proportion of \\(\\frac{135}{150}\\) or greater purely by chance.  \n* You can also include the context as we have before, but the context in this answer already infers knowledge that we are interpreting in the context of long-term frequency. This is really all you need for this interpretation.\n\n\nWhat conclusion would you make at the \\(\\alpha = 0.05\\) level?\n\n\nSince \\(\\text{p-value } = 0.0225&lt;0.05=\\alpha\\), we reject the null hypothesis that the proportion of teenagers who think highly of their mother is 0.85. There is convincing evidence that the true proportion of teens who think highly of their mother is greater than 0.84."
  },
  {
    "objectID": "hypothesis-testing.html#type-i-and-ii-error",
    "href": "hypothesis-testing.html#type-i-and-ii-error",
    "title": "13  Hypothesis Testing",
    "section": "13.2 Type I and II Error",
    "text": "13.2 Type I and II Error\nWhen we draw a conclusion from a significance test, we hope our conclusion will be correct. But sometimes it will be wrong. There are two types of mistakes we can make.\n\n\nIf we reject \\(H_0\\) when \\(H_a\\) is true, we have committed a Type I error.\nIf we fail to reject \\(H_0\\) when \\(H_a\\) is true, we have committed a Type II error.\n\n\nWe can sum this up with this up with this table:\n\n\n\n\n\n\n\n\nTruth about the population\n\n\n\n\n\\(H_0\\) is true\n\n\n\\(H_0\\) is false\n\n\n\n\n\n\nConclusion basedon the sample\n\n\nReject \\(H_0\\)\n\n\nType I Error\n\n\n\n\n\n\nFail to reject \\(H_0\\)\n\n\n\n\nType II Error\n\n\n\n\n\nWhen considering the consequences of the results of an inference task, we want to consider Type I and II errors. In some cases, we’d prefer to have Type I errors over Type II. In other instances, it’s the opposite.\n\n13.2.1 Type I Error\n\nThe probability of a Type I Error is \\(\\alpha\\) \\[P(\\text{Type I Error})=\\alpha\\]\n\n\n\n13.2.2 Type II Errors and Power\nA significance test makes a Type II error when it fails to reject a null hypothesis \\(H_0\\) that really is false. There are many values of the parameter that make the alternative hypothesis \\(H_a\\) true, so we concentrate on one value. The probability of making a Type II error depends on several factors, including the actual value of the parameter. A high probability of Type II error for a specific alternative parameter value means that the test is not sensitive enough to usually detect that alternative.\nThe power of a test against a specific alternative is the probability that the test will reject \\(H_0\\) at a chosen significance level when the specified alternative value of the parameter is true.\nThe significance level of a test is the probability of reaching the wrong conclusion when the null hypothesis is true.\nThe power of a test to detect a specific alternative is the probability of reaching the right conclusion when that alternative is true.\n\nThe power of a test against any alternative is 1 minus the probability of a Type II error for that alternative; that is, \\(power = 1-\\beta\\), where is the probability of making a Type II error.\n\\[\n\\begin{aligned}\nP(\\text{Type II Error}) &= \\beta \\\\\nP(\\text{Type II Error}) &= 1 - \\text{power} \\\\\n\\text{power} &= 1 - \\beta\n\\end{aligned}\n\\]\n\n* Keep in mind that you are not expected to know how to calculate power or \\(\\beta\\) without one of the other. i.e. you will always be given the power or \\(\\beta\\) in a problem.\n\nHow power looks like\n\n\n\n\n\nThis diagram illustrates how we can visualize power in a one-sided test checking to see if a parameter is larger than the null parameter. The null parameter here is 0 and the alternative parameter is 3. Each of their respective distributions are centered at 0 and 3. To think about power, first we look at the \\(\\alpha\\) value, which gives us the rejection region (shaded in black) in which any statistic that we get there will reject the null hypothesis.\nPower is the probability that we reject the null hypothesis given that the alternative parameter is actually true. So, power is represented in the yellow area, which starts at the same x (our “cut-off” value for rejection based off alpha) that the rejection region starts.\n\\(\\beta\\) is the probability of Type II error, where we do not reject the null hypothesis when it is actually wrong. So it is represented in the green area.\nSee an interactive version of this here."
  },
  {
    "objectID": "inference-for-proportions.html#conditions-for-inference",
    "href": "inference-for-proportions.html#conditions-for-inference",
    "title": "14  Inference for Proportions",
    "section": "14.1 Conditions for Inference",
    "text": "14.1 Conditions for Inference\n\n1. Random\nThe data must come from a random sample, or random assignment in an experiment.\n\n\n2. Large\nYou must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.\nFor proportions, remember, the population is never anywhere near normal (it’s always two bars, yes and no). In this case we check the Large Counts Condition.\nThis ensures that our sample proportion can take on enough different values (and make enough bars in a histogram) to create an approximately normal sampling distribution.\n\n\n3. Sampling Independence\nObservations in our sample must be independent of each other.\nIn random samples from a population, observations are never independent because the population changes with every person we sample and remove from it. However, this effect is small enough to ignore as long as the population from which we’re sampling is at least 10 times as large as our sample. This needs to be stated!"
  },
  {
    "objectID": "inference-for-proportions.html#one-sample-z-procedures",
    "href": "inference-for-proportions.html#one-sample-z-procedures",
    "title": "14  Inference for Proportions",
    "section": "14.2 One-Sample \\(z\\)-procedures",
    "text": "14.2 One-Sample \\(z\\)-procedures\n\n14.2.1 One-Sample \\(z\\)-interval\n\n\n\n\n\n\nOne-Sample \\(z\\)-interval\n\n\n\nA \\(C\\)% confidence interval for the unknown population proportion \\(p\\) when all conditions are met is calculated with the following:\n\\[\\hat p \\pm z^\\ast \\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\]\nwhere \\(z^\\ast\\) is the critical value for the standard Normal curve with \\(C\\)% of its area between \\(-z^\\ast\\) and \\(z^\\ast\\).\nSee Appendix C for examples on how to calculate \\(z^\\ast\\) for a given confidence level \\(C\\)%\n\n\nWhy?\n\nAll confidence intervals are calculated by: \\(\\text{point estimate } \\pm \\text{ margin of error}\\)\nThe point estimate for the true population proportion is \\(\\hat p\\)\nThe margin of error is always calculated as \\((\\text{critical value})(\\text{standard error})\\)\nWe use \\(z^\\ast\\) critical values for \\(z\\)-intervals (which is why they are called \\(z\\)-intervals).\nThe standard error is calculated as \\(\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\)\n\n\n\n\n\n14.2.2 One-Sample \\(z\\)-test\n\n\n\n\n\n\nOne-Sample \\(z\\)-test\n\n\n\nGiven the null hypothesis \\(H_0: p=p_0\\), and an observed sample proportion \\(\\hat p\\) from a sample of size \\(n\\) when all conditions are met. The null sampling distribution of \\(\\hat p\\), calculated assuming \\(H_0\\) is true, has the following:\n\\[\\mu_{\\hat p} = p_0\\] \\[\\sigma_{\\hat p} = \\frac{\\sigma_{p_0}}{\\sqrt n} = \\sqrt{\\frac{p_0(1- p_0)}{n}}\\]\nThe z-statistic is the standardized score of \\(\\hat p\\) under the mean and standard deviation of the null distribution:\n\\[z=\\frac{\\hat p - p_0}{\\sqrt{\\frac{p_0(1- p_0)}{n}}}\\]\nThe p-value, calculated using the z-statistic, is based off the direction of the alternative hypothesis, \\(H_a\\). \\(Z\\) represents the standard normal distribution.\n\\[\n\\text{p-value}= \\begin{cases}\nP(Z&lt;z) & \\text{ if } H_a:p&lt;p_0 \\\\\nP(Z&gt;z) & \\text{ if } H_a:p&gt;p_0 \\\\\n2\\cdot P(Z&lt;-|z|) & \\text{ if } H_a:p\\not =p_0\n\\end{cases}\n\\]"
  },
  {
    "objectID": "inference-for-proportions.html#two-sample-z-procedures",
    "href": "inference-for-proportions.html#two-sample-z-procedures",
    "title": "14  Inference for Proportions",
    "section": "14.3 Two-Sample \\(z\\)-procedures",
    "text": "14.3 Two-Sample \\(z\\)-procedures\nTwo-Sample z-procedures are justified by knowledge about the sampling distribution of \\(\\hat p_1 - \\hat p_2\\).\nThe conditions for two-sample z-procedures are the same for the two that we learn:\n\n\n\n\n\n\nConditions for Two-Sample \\(z\\)-procedures\n\n\n\n\nRandom: The data come from two independent random samples or from two groups in a randomized experiment.\nLarge Counts: The counts of “successes” and “failures” in each sample or group \\(n_1\\hat p_1\\), \\(n_1(1-\\hat p_1)\\), \\(n_2\\hat p_2\\), and \\(n_2(1-\\hat p_2)\\) are all at least 10.\n10%: When sampling without replacement, check that \\(n_1\\leq(.10)N_1\\) and \\(n2\\leq(.10)N_2\\).\n\n\n\n\n14.3.1 Two-Sample \\(z\\)-interval\n\n\n\n\n\n\nTwo-Sample \\(z\\)-interval\n\n\n\nWhen the conditions are met, an approximate \\(C\\)% confidence interval for \\(p_1-p_2\\) is:\n\\[(\\hat p_1-\\hat p_2)\\pm z^\\ast\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1}+\\frac{\\hat p_2(1-\\hat p_2)}{n_2}}\\]\nWhere \\(z^\\ast\\) is the critical value for the standard Normal curve with \\(C\\)% of its area between \\(-z^\\ast\\) and \\(z^\\ast\\)\n\n\n\n\n14.3.2 Two-Sample \\(z\\)-test (#two-sample-z-test)\n\n\n\n\n\n\nTwo-Sample \\(z\\)-test\n\n\n\nThe null hypothesis has the general form \\[ H_0:p_1-p_2=p_0 \\] where \\(p_0\\) is our hypothesized difference. We’ll restrict ourselves to situations in which \\(p_0 =0\\). Then the null hypothesis says that there is no difference between the two parameters: \\[H_0:p_1-p_2=0~~\\text{or}~~H_0:p_1=p_2\\] The alternative hypothesis says what kind of difference we expect: \\[H_a:p_1-p_2&gt;0~~\\text{or}~~Ha:p_1-p_2&lt;0~~\\text{or}~~Ha:p_1-p_2\\not =0  \\]\n\n\n\n\n\n\n\n\nCalculations for Two-Sample \\(z\\)-test\n\n\n\nSuppose the conditions are met. To test the hypotheses \\(H_0: p_1 - p_2 = 0\\), first find the pooled proportion \\(\\hat p_c\\) of successes in both samples combined.\n\\[\\hat p_c = \\frac{X_1 + X_2}{n_1 + n_2}\\]\nThen compute the z statistic: \\[ z = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\frac{\\hat p_c(1-\\hat p_c)}{n_1} +\\frac{\\hat p_c(1-\\hat p_c)}{n_2}}}\\]"
  },
  {
    "objectID": "inference-for-means.html#conditions-for-inference",
    "href": "inference-for-means.html#conditions-for-inference",
    "title": "15  Inference for Means",
    "section": "15.1 Conditions for Inference",
    "text": "15.1 Conditions for Inference\n\n1. Random\nThe data must come from a random sample, or random assignment in an experiment.\n\n\n2. Large\nYou must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.\nIn the case of sample means, the sampling distribution is approximately normal if at least one of the following conditions are met.\n\nNormal/Large Condition\nWhen \\(n&lt;30\\), check if the graph of the sample data does not show any strong skew or outliers. Otherwise,\n\n\n\n3. Sampling Independence\nObservations in our sample must be independent of each other.\nIn random samples from a population, observations are never independent because the population changes with every person we sample and remove from it. However, this effect is small enough to ignore as long as the population from which we’re sampling is at least 10 times as large as our sample. This needs to be stated!"
  },
  {
    "objectID": "inference-for-means.html#one-sample-t-procedures",
    "href": "inference-for-means.html#one-sample-t-procedures",
    "title": "15  Inference for Means",
    "section": "15.2 One-Sample \\(t\\)-procedures",
    "text": "15.2 One-Sample \\(t\\)-procedures\n\n15.2.1 One-Sample \\(t\\)-interval\n\n\n\n\n\n\nOne-Sample \\(t\\)-interval\n\n\n\nA \\(C\\)% confidence interval for the unknown population mean \\(\\mu\\) when all conditions are met is calculated with the following:\n\\[\\bar x \\pm t^\\ast \\frac{s_x}{\\sqrt{n}}\\]\nwhere \\(t^\\ast\\) is the critical value for the Student’s \\(t\\) distribution corresponding with degrees of freedom \\(df = n - 1\\) with \\(C\\)% of its area between \\(-t^\\ast\\) and \\(t^\\ast\\).\nSee Appendix C for examples on how to calculate \\(t^\\ast\\) for a given confidence level \\(C\\)% and degrees of freedom \\(df\\)\n\n\nWhy?\n\nAll confidence intervals are calculated by: \\(\\text{point estimate } \\pm \\text{ margin of error}\\)\nThe point estimate for the true population mean is \\(\\bar x\\)\nThe margin of error is always calculated as \\((\\text{critical value})(\\text{standard error})\\)\nWe use \\(t^\\ast\\) critical values for \\(t\\)-intervals (which is why they are called \\(t\\)-intervals).\nThe standard error is calculated as \\(\\frac{s_x}{\\sqrt{n}}\\)\n\n\n\n\n\n15.2.2 One-Sample \\(t\\)-test\n\n\n\n\n\n\nOne-Sample \\(t\\)-test\n\n\n\nGiven the null hypothesis \\(H_0: \\mu=\\mu_0\\), and an observed sample mean \\(\\bar x\\) and sample standard deviation \\(s_x\\) from a sample of size \\(n\\) when all conditions are met. The null sampling distribution of \\(\\bar x\\), calculated assuming \\(H_0\\) is true, has the following:\n\\[\\mu_{\\bar x} = \\mu_0\\] \\[\\sigma_{\\bar x} = \\frac{\\sigma_x}{\\sqrt n} \\approx \\frac{s_x}{\\sqrt n}\\]\nWhere we use \\(s_x\\) in place of \\(\\sigma_x\\) because the population standard deviation is usually unknown and we can use the sample standard deviation because it is an unbiased estimator of the population standard deviation.\nThe t-statistic is the standardized score of \\(\\bar x\\) under the mean and standard deviation of the null distribution:\n\\[t=\\frac{\\bar x - \\mu_0}{\\frac{s_x}{\\sqrt n}}\\]\nThe p-value, calculated using the t-statistic, is based off the direction of the alternative hypothesis, \\(H_a\\). \\(t_{n-1}\\) represents the \\(t\\) distribution with \\(df = n-1\\) degrees of freedom.\n\\[\n\\text{p-value}= \\begin{cases}\nP(t_{n-1}&lt;t) & \\text{ if } H_a:\\mu&lt;\\mu_0 \\\\\nP(t_{n-1}&gt;t) & \\text{ if } H_a:\\mu&gt;\\mu_0 \\\\\n2\\cdot P(t_{n-1}&lt;-|t|) & \\text{ if } H_a: \\mu\\not =\\mu_0\n\\end{cases}\n\\]\n\n\n\nPaired \\(t\\)-test\nA paired \\(t\\)-test is used when the data being compared are related or matched in some way, whereas a 2-sample \\(t\\)-test is used when the data being compared are independent. Here are some scenarios where a paired \\(t\\)-test would be more appropriate than a 2-sample \\(t\\)-test:\nMatched pair design:\n\nBefore-and-after studies: When the same group of individuals is measured twice, such as before and after receiving a treatment or intervention, a paired \\(t\\)-test is used to determine whether there is a significant difference in the measurements.\nMatched case-control studies: When cases and controls are matched based on certain criteria (such as age, sex, or race), a paired \\(t\\)-test can be used to compare the measurements of the two groups.\n\nRepeated measures: When a group of individuals is measured at multiple time points, a paired \\(t\\)-test can be used to compare the measurements within each individual.\nIn these scenarios, a paired \\(t\\)-test can be more powerful than a 2-sample \\(t\\)-test because it takes into account the relationship or correlation between the data being compared. By contrast, a 2-sample \\(t\\)-test assumes that the data being compared are independent, which may not be the case in these situations.\n\n\n\n\n\n\nPaired \\(t\\)-test\n\n\n\nGiven the null hypothesis \\(H_0: \\mu_d=\\mu_0\\) (where \\(\\mu_d\\) represents the true mean difference) and an observed sample mean \\(\\bar x\\) (of the differences) and sample standard deviation \\(s_x\\) (of the differences) from a sample of size \\(n\\) when all conditions are met. The null sampling distribution of \\(\\bar x\\), calculated assuming \\(H_0\\) is true, has the same null distribution and \\(t\\)-statistic as a one-sample t-test.\n\n\n\nConditions\n\n\n\n\n\n\nConditions for a paired \\(t\\)-procedure\n\n\n\n1. Random\nThe data must come from a random sample, or random assignment in an experiment. The latter of which will likely apply to a paired test.\n2. Large\nYou must check the following to know if the sample is large enough for us to know if we have an approximately normal distribution.\nIn the case of paired data, you’ll be only checking the sampling distribution of the differences. The sampling distribution is approximately normal if at least one of the following conditions are met.\n\nNormal/Large Condition\nWhen \\(n&lt;30\\), check if the graph of the sample data does not show any strong skew or outliers. Otherwise,\n\n3. Sampling Independence\nObservations in our sample must be independent of each other.\nIn random samples from a population, observations are never independent because the population changes with every person we sample and remove from it. However, this effect is small enough to ignore as long as the population from which we’re sampling is at least 10 times as large as our sample. This needs to be stated!\nHowever, since we’ll be likely working with experiments, independence will not apply, so you will not have to state it. You can say something like “since we have an experiment, we do not have to check independence”."
  },
  {
    "objectID": "inference-for-means.html#two-sample-t-procedures",
    "href": "inference-for-means.html#two-sample-t-procedures",
    "title": "15  Inference for Means",
    "section": "15.3 Two-Sample \\(t\\)-procedures",
    "text": "15.3 Two-Sample \\(t\\)-procedures\nTwo-Sample \\(t\\)-procedures are justified by knowledge about the sampling distribution of \\(\\bar x_1 - \\bar x_2\\).\nThe conditions for two-sample \\(t\\)-procedures are the same for the two that we learn:\n\n\n\n\n\n\nConditions for Two-Sample \\(t\\)-procedures\n\n\n\n\nRandom: The data come from two independent random samples or from two groups in a randomized experiment.\nNormal/Large Sample: Both population distributions (or the true distributions of responses to the two treatments) are Normal or both sample sizes are large (\\(n_1 \\geq 30\\) and \\(n_2 \\geq 30\\)). If either population (treatment) distribution has unknown shape and the corresponding sample size is less than 30, use a graph of the sample data to assess the Normality of the population (treatment) distribution. Do not use two-sample t procedures if the graph shows strong skewness or outliers.\n10%: When sampling without replacement, check that \\(n_1 \\leq (.10)N_1\\) and \\(n_2 \\leq (.10)N_2\\).\n\n\n\n\n15.3.1 Calculating degrees of freedom\nThere are two options for calculating degrees of freedom:\n\n\n\n\n\n\n\\(df\\) for Two-Sample \\(t\\)-procedures\n\n\n\nOption 1 (Technology): Use the \\(t\\) distribution with degrees of freedom calculated from the data by the formula below. Note that the \\(df\\) given by this formula is not usually a whole number. \\[df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{1}{n_1-1}\\left( \\frac{s_1^2}{n_1}\\right)^2 + \\frac{1}{n_2-1}\\left( \\frac{s_2^2}{n_2}\\right)^2}\\]\nOption 2 (Conservative): Use the \\(t\\) distribution with degrees of freedom equal to the smaller of \\(n_1 – 1\\) and \\(n_2 – 1\\). With this option, the resulting confidence interval has a margin of error as large or larger than is needed for the desired confidence level. The significance test using this option gives a P-value equal to or greater than the true P-value. As the sample sizes increase, the confidence levels and P-values from Option 2 become more accurate.\n\n\n\n\n15.3.2 Two-Sample t-interval\n\n\n\n\n\n\nTwo-Sample \\(t\\)-interval\n\n\n\nWhen the conditions are met, an approximate \\(C\\)% confidence interval for \\(\\mu_1 - \\mu_2\\) is:\n\\[(\\bar x_1 - \\bar x_2)\\pm t^\\ast\\sqrt{\\frac{s_{x_1}}{n_1}+\\frac{s_{x_2}}{n_2}}\\]\nWhere \\(t^\\ast\\) is the critical value for the \\(t\\) distribution with degrees of freedom calculated with Option 1 or 2 with \\(C\\)% of its area between \\(-t^\\ast\\) and \\(t^\\ast\\)\n\n\n\n\n15.3.3 Two-Sample t-test\n\n\n\n\n\n\nTwo-Sample \\(t\\)-test\n\n\n\nThe null hypothesis has the general form \\[ H_0:\\mu_1-\\mu_2=\\mu_0 \\] where \\(\\mu_0\\) is our hypothesized difference. We’ll restrict ourselves to situations in which \\(\\mu = 0\\). Then the null hypothesis says that there is no difference between the two parameters: \\[H_0:\\mu_1-\\mu_2=0~~\\text{or}~~H_0:\\mu_1=\\mu_2\\] The alternative hypothesis says what kind of difference we expect: \\[H_a:\\mu_1-\\mu_2&gt;0~~\\text{or}~~Ha:\\mu_1-\\mu_2&lt;0~~\\text{or}~~Ha:\\mu_1-\\mu_2\\not =0  \\]\n\n\n\n\n\n\n\n\n\\(t\\)-statistic for Two-Sample \\(t\\)-tests\n\n\n\nCompute the \\(t\\) statistic as:\n\\[\nt = \\frac{(\\bar x_1 - \\bar x_2) - (\\mu_1-\\mu_2)_0}{\\sqrt{\\frac{s_{x_1}}{n_1} + \\frac{s_{x_2}}{n_2}}}\n\\]\n\n\n\n\nThe standard error of the \\(t\\) statistic, unlike the a two-sample z-test, is not pooled.\n\n\nThe reason for this is that the null hypothesis for a two sample z-test is a statement of no difference between the proportions, so we’re saying that the populations are practically the same. The null hypothesis for a two sample t-test is a statement of no difference between the means. Since there’s no assumption of the populations themselves, we will not pool the standard deviations for two-sample t-tests."
  },
  {
    "objectID": "inference-for-means.html#the-students-t-distribution",
    "href": "inference-for-means.html#the-students-t-distribution",
    "title": "15  Inference for Means",
    "section": "15.4 The Student’s \\(t\\) distribution",
    "text": "15.4 The Student’s \\(t\\) distribution\n\n\n\n\n\n\nThe Student’s \\(t\\) distribution\n\n\n\nDraw an SRS of size \\(n\\) from a large population that has a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The statistic \\[t=\\frac{\\bar x - \\mu}{s_x / \\sqrt{n}}\\] has a \\(t\\)-distribution with degrees of freedom \\(df = n – 1\\), denoted as \\(t_{n-1}\\). When the population distribution isn’t Normal, this statistic will be approximately \\(t_{n-1}\\) if the sample size is large enough.\nAs the \\(df\\) increases, the \\(t\\)-distribution approaches \\(N(0,1)\\) (standard normal) (See Figure @ref(fig:t-dist-to-norm)). This happens because \\(s_x\\) estimates \\(\\sigma\\) more accurately as \\(n\\) increases. So using \\(s_x\\) in place of \\(\\sigma\\) causes little extra variation when the sample is large enough.\n\n\n\n15.4.1 Why a \\(t\\)-distribution?\nWhen we’re conducting inference for a population proportion, there’s only one parameter (\\(p\\)) that we don’t know. The sampling distributions in these cases follow a Normal curve very well (as long as conditions are met), allowing us to use \\(z\\)-procedures. However, when we’re conducting inference for a population mean, there is additional uncertainty created by the fact that there are two parameters we don’t know: the population mean \\(\\mu\\) and the population standard deviation \\(\\sigma\\). Since we now have, we get a different sampling distribution that is not quite normal, especially at small \\(n\\).\nAs a result, we use what’s called the Student’s \\(t\\) distribution (a Guinness Beer brewer developed this), which is a slightly more conservative version of a normal distribution. The \\(t\\) distribution is still symmetric with a single peak at 0, but with much more area in the tails. The statistic \\(t\\) has the same interpretation as any standardized \\(z\\) statistic: it says many standard deviations \\(\\sigma\\) that \\(x\\) is from the distribution’s mean \\(\\mu\\).\n\nfunction tDemo(df) {\n    var x = d3.range(-4, 4.005, 0.01);\n    var result = [];\n    x.forEach((val, i) =&gt; {\n        result.push({\"x\": val, \"y\": jStat.normal.pdf(val, 0, 1), \"lab\": \"Standard Normal\"});\n        result.push({\"x\": val, \"y\": jStat.studentt.pdf(val, df), \"lab\": \"t distribution\"});\n    })\n    return result;\n}\n\nPlot.plot({\n  marks: [\n    Plot.line(tDemo(df2),\n      {\n        x: \"x\", \n        y: \"y\",\n        stroke: \"lab\",\n        strokeWidth: 3\n      }\n    ),\n    Plot.ruleY([0]),\n    Plot.ruleX([-4])\n  ],\n    x: { label: \"t statistic\" },\n    y: {domain: [0, .41],\n    label: \"Density\"},\n    color: {\n        legend: true\n    },\n    caption: \"Student's t Distribution\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof df2 = Inputs.range(\n[1, 100],\n{value: 3, step: 1, label: \"Degrees of Freedom (df):\"}\n)"
  },
  {
    "objectID": "formula-sheet.html#descriptive-statistics",
    "href": "formula-sheet.html#descriptive-statistics",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.1 Descriptive Statistics",
    "text": "A.1 Descriptive Statistics\n\\[\n\\begin{aligned}\n\\bar x &= \\frac{1}{n}\\sum x_i = \\frac{\\sum x_i}{n} \\\\\\\\\ns_x &= \\sqrt{\\frac{1}{n-1}\\sum (x_i - \\bar x)^2} = \\sqrt{\\frac{\\sum(x_i -\\bar x)^2}{n - 1}}\\\\\\\\\n\\hat y &= a + bx \\\\\\\\\n\\bar y &= a + b \\bar x\\\\\\\\\nr &= \\frac{1}{n-1}\\sum \\left( \\frac{x_i - \\bar x}{s_x}\\right) \\left( \\frac{y_i -\\bar y}{s_y}\\right)\\\\\\\\\nb &= r \\frac{s_y}{s_x}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "formula-sheet.html#probability-and-distributions",
    "href": "formula-sheet.html#probability-and-distributions",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.2 Probability and Distributions",
    "text": "A.2 Probability and Distributions\n\\[\n\\begin{aligned}\nP(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\\\\\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nProbability Distribution\nMean\nStandard Deviation\n\n\n\n\nDiscrete random variable, \\(X\\)\n\\(\\mu_X = E(X) = \\sum x_i P(x_i)\\)\n\\(\\sigma_X = \\sqrt{\\sum \\left( x_i - \\mu_X \\right) ^2 P \\left( x_i \\right)}\\)\n\n\nIf \\(X\\) has a binomial distribution with parameters \\(n\\) and \\(p\\), then: \\[P(X = x) = {n \\choose x} p^x (1 - p)^{n - x}\\] where \\(x = 0, 1, 2, 3, ..., n\\)\n\\(\\mu_X = np\\)\n\\(\\sigma_X = \\sqrt{np(1-p)}\\)\n\n\nIf \\(X\\) has a geometric distribution with parameter \\(p\\), then: \\[P(X = x) = (1 - p)^{x - 1} p\\] where \\(x = 1, 2, 3, ...\\)\n\\(\\mu_X = \\frac{1}{p}\\)\n\\(\\sigma_X = \\frac{\\sqrt{1-p}}{p}\\)"
  },
  {
    "objectID": "formula-sheet.html#sampling-distributions-and-inferential-statistics",
    "href": "formula-sheet.html#sampling-distributions-and-inferential-statistics",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.3 Sampling Distributions and Inferential Statistics",
    "text": "A.3 Sampling Distributions and Inferential Statistics\n\\[\\text{Standardized test statistic:}~~~~ \\frac{\\text{statistic} - \\text{parameter}} {\\text{standard error of the statistic}}\\]\n\n\\[\\text{Confidence interval:}~~~~ \\text{statistic} \\pm (\\text{critical value}) (\\text{standard error of statistic})\\]\n\n\\[\\text{Chi-square statistic:}~~~~ \\chi^2 = \\sum \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nA.3.1 Sampling distributions for proportions:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor one population: \\(\\hat p\\)\n\\(\\mu_{\\hat p} = p\\)\n\\(\\sigma_{\\hat p} = \\sqrt{\\frac{p(1-p)}{n}}\\)\n\\(s_{\\hat p} = \\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}\\)\n\n\nFor two populations:\\(\\hat p_1 - \\hat p_2\\)\n\\(\\mu_{\\hat p_1 - \\hat p_2} = p_1 - p_2\\)\n\\(\\sigma_{\\hat p_1 - \\hat p_2}= \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\)\n\\(s_{\\hat p_1 - \\hat p_2}= \\sqrt{\\frac{\\hat p_1(1- \\hat p_1)}{n_1} + \\frac{\\hat p_2(1- \\hat p_2)}{n_2}}\\)When \\(p_1 = p_2\\) is assumed:\\(s_{\\hat p_1 -\\hat p_2} = \\sqrt{\\hat p_c (1 - \\hat p_c) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\)where \\(\\hat p_c = \\frac{X_1 + X_2}{n_1 + n_2}\\)\n\n\n\n\n\nA.3.2 Sampling distributions for means:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor one population: \\(\\overline{X}\\)\n\\(\\mu_{\\overline X} = \\mu\\)\n\\(\\sigma_{\\overline X} = \\frac{\\sigma}{\\sqrt n}\\)\n\\(s_{\\overline X} = \\frac{s}{\\sqrt n}\\)\n\n\nFor two populations:\\(\\overline{X_1} - \\overline{X_2}\\)\n\\(\\mu_{\\overline{X_1} - \\overline{X_2}} = \\mu_1 - \\mu_2\\)\n\\(\\sigma_{\\overline{X_1} - \\overline{X_2}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\\(s_{\\overline{X_1} - \\overline{X_2}} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\)\n\n\n\n\n\nA.3.3 Sampling distributions for simple linear regression:\n\n\n\n\n\n\n\n\n\nRandom Variable\nParameters of Sampling Distribution (mean)\nParameters of Sampling Distribution (standard deviation)\nStandard Error* of Sample Statistic\n\n\n\n\nFor slope:\\(b\\)\n\\(\\mu_b = \\beta\\)\n\\(\\sigma_{b} = \\frac{\\sigma}{\\sigma_x \\sqrt n}\\),where \\(\\sigma_x = \\sqrt{\\frac{\\sum (x_i - \\bar x)^2}{n}}\\)\n\\(s_{b} = \\frac{s}{s_x \\sqrt{n-1}}\\) ,where \\(s = \\sqrt{\\frac{\\sum (y_i - \\hat y_i)^2)}{n - 2}}\\)and \\(s_x = \\sqrt{\\frac{\\sum (x_i - \\bar x)^2}{n - 1}}\\)\n\n\n\n*Standard deviation is a measurement of variability from the theoretical population. Standard error is the estimate of the standard deviation. If the standard deviation of the statistic is assumed to be known, then the standard deviation should be used instead of the standard error."
  },
  {
    "objectID": "formula-sheet.html#table-a",
    "href": "formula-sheet.html#table-a",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.4 Table A: Standard normal probabilities",
    "text": "A.4 Table A: Standard normal probabilities\n\n\n\n\n\n\n\nClick for table of standard normal probabilities\n\nTable entry for \\(z\\) is the probability lying below \\(z\\).\n\n\n\n\n\n\n\\(z\\)\n\n\n0.00\n\n\n0.01\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.05\n\n\n0.06\n\n\n0.07\n\n\n0.08\n\n\n0.09\n\n\n\n\n\n\n-3.4\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0003\n\n\n0.0002\n\n\n\n\n-3.3\n\n\n0.0005\n\n\n0.0005\n\n\n0.0005\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0004\n\n\n0.0003\n\n\n\n\n-3.2\n\n\n0.0007\n\n\n0.0007\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0006\n\n\n0.0005\n\n\n0.0005\n\n\n0.0005\n\n\n\n\n-3.1\n\n\n0.0010\n\n\n0.0009\n\n\n0.0009\n\n\n0.0009\n\n\n0.0008\n\n\n0.0008\n\n\n0.0008\n\n\n0.0008\n\n\n0.0007\n\n\n0.0007\n\n\n\n\n-3.0\n\n\n0.0013\n\n\n0.0013\n\n\n0.0013\n\n\n0.0012\n\n\n0.0012\n\n\n0.0011\n\n\n0.0011\n\n\n0.0011\n\n\n0.0010\n\n\n0.0010\n\n\n\n\n-2.9\n\n\n0.0019\n\n\n0.0018\n\n\n0.0018\n\n\n0.0017\n\n\n0.0016\n\n\n0.0016\n\n\n0.0015\n\n\n0.0015\n\n\n0.0014\n\n\n0.0014\n\n\n\n\n-2.8\n\n\n0.0026\n\n\n0.0025\n\n\n0.0024\n\n\n0.0023\n\n\n0.0023\n\n\n0.0022\n\n\n0.0021\n\n\n0.0021\n\n\n0.0020\n\n\n0.0019\n\n\n\n\n-2.7\n\n\n0.0035\n\n\n0.0034\n\n\n0.0033\n\n\n0.0032\n\n\n0.0031\n\n\n0.0030\n\n\n0.0029\n\n\n0.0028\n\n\n0.0027\n\n\n0.0026\n\n\n\n\n-2.6\n\n\n0.0047\n\n\n0.0045\n\n\n0.0044\n\n\n0.0043\n\n\n0.0041\n\n\n0.0040\n\n\n0.0039\n\n\n0.0038\n\n\n0.0037\n\n\n0.0036\n\n\n\n\n-2.5\n\n\n0.0062\n\n\n0.0060\n\n\n0.0059\n\n\n0.0057\n\n\n0.0055\n\n\n0.0054\n\n\n0.0052\n\n\n0.0051\n\n\n0.0049\n\n\n0.0048\n\n\n\n\n-2.4\n\n\n0.0082\n\n\n0.0080\n\n\n0.0078\n\n\n0.0075\n\n\n0.0073\n\n\n0.0071\n\n\n0.0069\n\n\n0.0068\n\n\n0.0066\n\n\n0.0064\n\n\n\n\n-2.3\n\n\n0.0107\n\n\n0.0104\n\n\n0.0102\n\n\n0.0099\n\n\n0.0096\n\n\n0.0094\n\n\n0.0091\n\n\n0.0089\n\n\n0.0087\n\n\n0.0084\n\n\n\n\n-2.2\n\n\n0.0139\n\n\n0.0136\n\n\n0.0132\n\n\n0.0129\n\n\n0.0125\n\n\n0.0122\n\n\n0.0119\n\n\n0.0116\n\n\n0.0113\n\n\n0.0110\n\n\n\n\n-2.1\n\n\n0.0179\n\n\n0.0174\n\n\n0.017\n\n\n0.0166\n\n\n0.0162\n\n\n0.0158\n\n\n0.0154\n\n\n0.0150\n\n\n0.0146\n\n\n0.0143\n\n\n\n\n-2.0\n\n\n0.0228\n\n\n0.0222\n\n\n0.0217\n\n\n0.0212\n\n\n0.0207\n\n\n0.0202\n\n\n0.0197\n\n\n0.0192\n\n\n0.0188\n\n\n0.0183\n\n\n\n\n-1.9\n\n\n0.0287\n\n\n0.0281\n\n\n0.0274\n\n\n0.0268\n\n\n0.0262\n\n\n0.0256\n\n\n0.0250\n\n\n0.0244\n\n\n0.0239\n\n\n0.0233\n\n\n\n\n-1.8\n\n\n0.0359\n\n\n0.0351\n\n\n0.0344\n\n\n0.0336\n\n\n0.0329\n\n\n0.0322\n\n\n0.0314\n\n\n0.0307\n\n\n0.0301\n\n\n0.0294\n\n\n\n\n-1.7\n\n\n0.0446\n\n\n0.0436\n\n\n0.0427\n\n\n0.0418\n\n\n0.0409\n\n\n0.0401\n\n\n0.0392\n\n\n0.0384\n\n\n0.0375\n\n\n0.0367\n\n\n\n\n-1.6\n\n\n0.0548\n\n\n0.0537\n\n\n0.0526\n\n\n0.0516\n\n\n0.0505\n\n\n0.0495\n\n\n0.0485\n\n\n0.0475\n\n\n0.0465\n\n\n0.0455\n\n\n\n\n-1.5\n\n\n0.0668\n\n\n0.0655\n\n\n0.0643\n\n\n0.0630\n\n\n0.0618\n\n\n0.0606\n\n\n0.0594\n\n\n0.0582\n\n\n0.0571\n\n\n0.0559\n\n\n\n\n-1.4\n\n\n0.0808\n\n\n0.0793\n\n\n0.0778\n\n\n0.0764\n\n\n0.0749\n\n\n0.0735\n\n\n0.0721\n\n\n0.0708\n\n\n0.0694\n\n\n0.0681\n\n\n\n\n-1.3\n\n\n0.0968\n\n\n0.0951\n\n\n0.0934\n\n\n0.0918\n\n\n0.0901\n\n\n0.0885\n\n\n0.0869\n\n\n0.0853\n\n\n0.0838\n\n\n0.0823\n\n\n\n\n-1.2\n\n\n0.1151\n\n\n0.1131\n\n\n0.1112\n\n\n0.1093\n\n\n0.1075\n\n\n0.1056\n\n\n0.1038\n\n\n0.1020\n\n\n0.1003\n\n\n0.0985\n\n\n\n\n-1.1\n\n\n0.1357\n\n\n0.1335\n\n\n0.1314\n\n\n0.1292\n\n\n0.1271\n\n\n0.1251\n\n\n0.1230\n\n\n0.1210\n\n\n0.1190\n\n\n0.1170\n\n\n\n\n-1.0\n\n\n0.1587\n\n\n0.1562\n\n\n0.1539\n\n\n0.1515\n\n\n0.1492\n\n\n0.1469\n\n\n0.1446\n\n\n0.1423\n\n\n0.1401\n\n\n0.1379\n\n\n\n\n-0.9\n\n\n0.1841\n\n\n0.1814\n\n\n0.1788\n\n\n0.1762\n\n\n0.1736\n\n\n0.1711\n\n\n0.1685\n\n\n0.1660\n\n\n0.1635\n\n\n0.1611\n\n\n\n\n-0.8\n\n\n0.2119\n\n\n0.2090\n\n\n0.2061\n\n\n0.2033\n\n\n0.2005\n\n\n0.1977\n\n\n0.1949\n\n\n0.1922\n\n\n0.1894\n\n\n0.1867\n\n\n\n\n-0.7\n\n\n0.2420\n\n\n0.2389\n\n\n0.2358\n\n\n0.2327\n\n\n0.2296\n\n\n0.2266\n\n\n0.2236\n\n\n0.2206\n\n\n0.2177\n\n\n0.2148\n\n\n\n\n-0.6\n\n\n0.2743\n\n\n0.2709\n\n\n0.2676\n\n\n0.2643\n\n\n0.2611\n\n\n0.2578\n\n\n0.2546\n\n\n0.2514\n\n\n0.2483\n\n\n0.2451\n\n\n\n\n-0.5\n\n\n0.3085\n\n\n0.305\n\n\n0.3015\n\n\n0.2981\n\n\n0.2946\n\n\n0.2912\n\n\n0.2877\n\n\n0.2843\n\n\n0.281\n\n\n0.2776\n\n\n\n\n-0.4\n\n\n0.3446\n\n\n0.3409\n\n\n0.3372\n\n\n0.3336\n\n\n0.3300\n\n\n0.3264\n\n\n0.3228\n\n\n0.3192\n\n\n0.3156\n\n\n0.3121\n\n\n\n\n-0.3\n\n\n0.3821\n\n\n0.3783\n\n\n0.3745\n\n\n0.3707\n\n\n0.3669\n\n\n0.3632\n\n\n0.3594\n\n\n0.3557\n\n\n0.3520\n\n\n0.3483\n\n\n\n\n-0.2\n\n\n0.4207\n\n\n0.4168\n\n\n0.4129\n\n\n0.4090\n\n\n0.4052\n\n\n0.4013\n\n\n0.3974\n\n\n0.3936\n\n\n0.3897\n\n\n0.3859\n\n\n\n\n-0.1\n\n\n0.4602\n\n\n0.4562\n\n\n0.4522\n\n\n0.4483\n\n\n0.4443\n\n\n0.4404\n\n\n0.4364\n\n\n0.4325\n\n\n0.4286\n\n\n0.4247\n\n\n\n\n0.0\n\n\n0.5000\n\n\n0.4960\n\n\n0.4920\n\n\n0.4880\n\n\n0.4840\n\n\n0.4801\n\n\n0.4761\n\n\n0.4721\n\n\n0.4681\n\n\n0.4641\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick for table of standard normal probabilities (continued)\n\nTable entry for \\(z\\) is the probability lying below \\(z\\).\n\n\n\n\n\n\n\\(z\\)\n\n\n0.00\n\n\n0.01\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.05\n\n\n0.06\n\n\n0.07\n\n\n0.08\n\n\n0.09\n\n\n\n\n\n\n0.0\n\n\n0.5000\n\n\n0.5040\n\n\n0.5080\n\n\n0.5120\n\n\n0.5160\n\n\n0.5199\n\n\n0.5239\n\n\n0.5279\n\n\n0.5319\n\n\n0.5359\n\n\n\n\n0.1\n\n\n0.5398\n\n\n0.5438\n\n\n0.5478\n\n\n0.5517\n\n\n0.5557\n\n\n0.5596\n\n\n0.5636\n\n\n0.5675\n\n\n0.5714\n\n\n0.5753\n\n\n\n\n0.2\n\n\n0.5793\n\n\n0.5832\n\n\n0.5871\n\n\n0.5910\n\n\n0.5948\n\n\n0.5987\n\n\n0.6026\n\n\n0.6064\n\n\n0.6103\n\n\n0.6141\n\n\n\n\n0.3\n\n\n0.6179\n\n\n0.6217\n\n\n0.6255\n\n\n0.6293\n\n\n0.6331\n\n\n0.6368\n\n\n0.6406\n\n\n0.6443\n\n\n0.6480\n\n\n0.6517\n\n\n\n\n0.4\n\n\n0.6554\n\n\n0.6591\n\n\n0.6628\n\n\n0.6664\n\n\n0.6700\n\n\n0.6736\n\n\n0.6772\n\n\n0.6808\n\n\n0.6844\n\n\n0.6879\n\n\n\n\n0.5\n\n\n0.6915\n\n\n0.6950\n\n\n0.6985\n\n\n0.7019\n\n\n0.7054\n\n\n0.7088\n\n\n0.7123\n\n\n0.7157\n\n\n0.7190\n\n\n0.7224\n\n\n\n\n0.6\n\n\n0.7257\n\n\n0.7291\n\n\n0.7324\n\n\n0.7357\n\n\n0.7389\n\n\n0.7422\n\n\n0.7454\n\n\n0.7486\n\n\n0.7517\n\n\n0.7549\n\n\n\n\n0.7\n\n\n0.7580\n\n\n0.7611\n\n\n0.7642\n\n\n0.7673\n\n\n0.7704\n\n\n0.7734\n\n\n0.7764\n\n\n0.7794\n\n\n0.7823\n\n\n0.7852\n\n\n\n\n0.8\n\n\n0.7881\n\n\n0.7910\n\n\n0.7939\n\n\n0.7967\n\n\n0.7995\n\n\n0.8023\n\n\n0.8051\n\n\n0.8078\n\n\n0.8106\n\n\n0.8133\n\n\n\n\n0.9\n\n\n0.8159\n\n\n0.8186\n\n\n0.8212\n\n\n0.8238\n\n\n0.8264\n\n\n0.8289\n\n\n0.8315\n\n\n0.8340\n\n\n0.8365\n\n\n0.8389\n\n\n\n\n1.0\n\n\n0.8413\n\n\n0.8438\n\n\n0.8461\n\n\n0.8485\n\n\n0.8508\n\n\n0.8531\n\n\n0.8554\n\n\n0.8577\n\n\n0.8599\n\n\n0.8621\n\n\n\n\n1.1\n\n\n0.8643\n\n\n0.8665\n\n\n0.8686\n\n\n0.8708\n\n\n0.8729\n\n\n0.8749\n\n\n0.8770\n\n\n0.8790\n\n\n0.8810\n\n\n0.8830\n\n\n\n\n1.2\n\n\n0.8849\n\n\n0.8869\n\n\n0.8888\n\n\n0.8907\n\n\n0.8925\n\n\n0.8944\n\n\n0.8962\n\n\n0.8980\n\n\n0.8997\n\n\n0.9015\n\n\n\n\n1.3\n\n\n0.9032\n\n\n0.9049\n\n\n0.9066\n\n\n0.9082\n\n\n0.9099\n\n\n0.9115\n\n\n0.9131\n\n\n0.9147\n\n\n0.9162\n\n\n0.9177\n\n\n\n\n1.4\n\n\n0.9192\n\n\n0.9207\n\n\n0.9222\n\n\n0.9236\n\n\n0.9251\n\n\n0.9265\n\n\n0.9279\n\n\n0.9292\n\n\n0.9306\n\n\n0.9319\n\n\n\n\n1.5\n\n\n0.9332\n\n\n0.9345\n\n\n0.9357\n\n\n0.9370\n\n\n0.9382\n\n\n0.9394\n\n\n0.9406\n\n\n0.9418\n\n\n0.9429\n\n\n0.9441\n\n\n\n\n1.6\n\n\n0.9452\n\n\n0.9463\n\n\n0.9474\n\n\n0.9484\n\n\n0.9495\n\n\n0.9505\n\n\n0.9515\n\n\n0.9525\n\n\n0.9535\n\n\n0.9545\n\n\n\n\n1.7\n\n\n0.9554\n\n\n0.9564\n\n\n0.9573\n\n\n0.9582\n\n\n0.9591\n\n\n0.9599\n\n\n0.9608\n\n\n0.9616\n\n\n0.9625\n\n\n0.9633\n\n\n\n\n1.8\n\n\n0.9641\n\n\n0.9649\n\n\n0.9656\n\n\n0.9664\n\n\n0.9671\n\n\n0.9678\n\n\n0.9686\n\n\n0.9693\n\n\n0.9699\n\n\n0.9706\n\n\n\n\n1.9\n\n\n0.9713\n\n\n0.9719\n\n\n0.9726\n\n\n0.9732\n\n\n0.9738\n\n\n0.9744\n\n\n0.9750\n\n\n0.9756\n\n\n0.9761\n\n\n0.9767\n\n\n\n\n2.0\n\n\n0.9772\n\n\n0.9778\n\n\n0.9783\n\n\n0.9788\n\n\n0.9793\n\n\n0.9798\n\n\n0.9803\n\n\n0.9808\n\n\n0.9812\n\n\n0.9817\n\n\n\n\n2.1\n\n\n0.9821\n\n\n0.9826\n\n\n0.9830\n\n\n0.9834\n\n\n0.9838\n\n\n0.9842\n\n\n0.9846\n\n\n0.9850\n\n\n0.9854\n\n\n0.9857\n\n\n\n\n2.2\n\n\n0.9861\n\n\n0.9864\n\n\n0.9868\n\n\n0.9871\n\n\n0.9875\n\n\n0.9878\n\n\n0.9881\n\n\n0.9884\n\n\n0.9887\n\n\n0.9890\n\n\n\n\n2.3\n\n\n0.9893\n\n\n0.9896\n\n\n0.9898\n\n\n0.9901\n\n\n0.9904\n\n\n0.9906\n\n\n0.9909\n\n\n0.9911\n\n\n0.9913\n\n\n0.9916\n\n\n\n\n2.4\n\n\n0.9918\n\n\n0.9920\n\n\n0.9922\n\n\n0.9925\n\n\n0.9927\n\n\n0.9929\n\n\n0.9931\n\n\n0.9932\n\n\n0.9934\n\n\n0.9936\n\n\n\n\n2.5\n\n\n0.9938\n\n\n0.9940\n\n\n0.9941\n\n\n0.9943\n\n\n0.9945\n\n\n0.9946\n\n\n0.9948\n\n\n0.9949\n\n\n0.9951\n\n\n0.9952\n\n\n\n\n2.6\n\n\n0.9953\n\n\n0.9955\n\n\n0.9956\n\n\n0.9957\n\n\n0.9959\n\n\n0.9960\n\n\n0.9961\n\n\n0.9962\n\n\n0.9963\n\n\n0.9964\n\n\n\n\n2.7\n\n\n0.9965\n\n\n0.9966\n\n\n0.9967\n\n\n0.9968\n\n\n0.9969\n\n\n0.9970\n\n\n0.9971\n\n\n0.9972\n\n\n0.9973\n\n\n0.9974\n\n\n\n\n2.8\n\n\n0.9974\n\n\n0.9975\n\n\n0.9976\n\n\n0.9977\n\n\n0.9977\n\n\n0.9978\n\n\n0.9979\n\n\n0.9979\n\n\n0.9980\n\n\n0.9981\n\n\n\n\n2.9\n\n\n0.9981\n\n\n0.9982\n\n\n0.9982\n\n\n0.9983\n\n\n0.9984\n\n\n0.9984\n\n\n0.9985\n\n\n0.9985\n\n\n0.9986\n\n\n0.9986\n\n\n\n\n3.0\n\n\n0.9987\n\n\n0.9987\n\n\n0.9987\n\n\n0.9988\n\n\n0.9988\n\n\n0.9989\n\n\n0.9989\n\n\n0.9989\n\n\n0.9990\n\n\n0.9990\n\n\n\n\n3.1\n\n\n0.9990\n\n\n0.9991\n\n\n0.9991\n\n\n0.9991\n\n\n0.9992\n\n\n0.9992\n\n\n0.9992\n\n\n0.9992\n\n\n0.9993\n\n\n0.9993\n\n\n\n\n3.2\n\n\n0.9993\n\n\n0.9993\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9994\n\n\n0.9995\n\n\n0.9995\n\n\n0.9995\n\n\n\n\n3.3\n\n\n0.9995\n\n\n0.9995\n\n\n0.9995\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9996\n\n\n0.9997\n\n\n\n\n3.4\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9997\n\n\n0.9998"
  },
  {
    "objectID": "formula-sheet.html#table-b",
    "href": "formula-sheet.html#table-b",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.5 Table B: \\(t\\) distribution critical values",
    "text": "A.5 Table B: \\(t\\) distribution critical values\n\n\n\n\n\n\n\nClick here for table of \\(t\\) distribution critical values\n\nTable entry for \\(p\\) and \\(C\\) is the point \\(t*\\) with probability \\(p\\) lying above it and probability \\(C\\) lying between \\(-t*\\) and \\(t*\\).\n\n\n\n\n\n\ndf\n\n\nTail Probability \\(p\\)\n\n\n\n\n0.25\n\n\n0.20\n\n\n0.15\n\n\n0.10\n\n\n0.05\n\n\n0.025\n\n\n0.02\n\n\n0.01\n\n\n0.005\n\n\n0.0025\n\n\n0.001\n\n\n0.0005\n\n\n\n\n\n\n1\n\n\n1.000\n\n\n1.376\n\n\n1.963\n\n\n3.078\n\n\n6.314\n\n\n12.71\n\n\n15.89\n\n\n31.82\n\n\n63.66\n\n\n127.3\n\n\n318.3\n\n\n636.6\n\n\n\n\n2\n\n\n0.816\n\n\n1.061\n\n\n1.386\n\n\n1.886\n\n\n2.920\n\n\n4.303\n\n\n4.849\n\n\n6.965\n\n\n9.925\n\n\n14.09\n\n\n22.33\n\n\n31.60\n\n\n\n\n3\n\n\n0.765\n\n\n0.978\n\n\n1.250\n\n\n1.638\n\n\n2.353\n\n\n3.182\n\n\n3.482\n\n\n4.541\n\n\n5.841\n\n\n7.453\n\n\n10.21\n\n\n12.92\n\n\n\n\n4\n\n\n0.741\n\n\n0.941\n\n\n1.190\n\n\n1.533\n\n\n2.132\n\n\n2.776\n\n\n2.999\n\n\n3.747\n\n\n4.604\n\n\n5.598\n\n\n7.173\n\n\n8.610\n\n\n\n\n5\n\n\n0.727\n\n\n0.920\n\n\n1.156\n\n\n1.476\n\n\n2.015\n\n\n2.571\n\n\n2.757\n\n\n3.365\n\n\n4.032\n\n\n4.773\n\n\n5.893\n\n\n6.869\n\n\n\n\n6\n\n\n0.718\n\n\n0.906\n\n\n1.134\n\n\n1.440\n\n\n1.943\n\n\n2.447\n\n\n2.612\n\n\n3.143\n\n\n3.707\n\n\n4.317\n\n\n5.208\n\n\n5.959\n\n\n\n\n7\n\n\n0.711\n\n\n0.896\n\n\n1.119\n\n\n1.415\n\n\n1.895\n\n\n2.365\n\n\n2.517\n\n\n2.998\n\n\n3.499\n\n\n4.029\n\n\n4.785\n\n\n5.408\n\n\n\n\n8\n\n\n0.706\n\n\n0.889\n\n\n1.108\n\n\n1.397\n\n\n1.860\n\n\n2.306\n\n\n2.449\n\n\n2.896\n\n\n3.355\n\n\n3.833\n\n\n4.501\n\n\n5.041\n\n\n\n\n9\n\n\n0.703\n\n\n0.883\n\n\n1.100\n\n\n1.383\n\n\n1.833\n\n\n2.262\n\n\n2.398\n\n\n2.821\n\n\n3.250\n\n\n3.690\n\n\n4.297\n\n\n4.781\n\n\n\n\n10\n\n\n0.700\n\n\n0.879\n\n\n1.093\n\n\n1.372\n\n\n1.812\n\n\n2.228\n\n\n2.359\n\n\n2.764\n\n\n3.169\n\n\n3.581\n\n\n4.144\n\n\n4.587\n\n\n\n\n11\n\n\n0.697\n\n\n0.876\n\n\n1.088\n\n\n1.363\n\n\n1.796\n\n\n2.201\n\n\n2.328\n\n\n2.718\n\n\n3.106\n\n\n3.497\n\n\n4.025\n\n\n4.437\n\n\n\n\n12\n\n\n0.695\n\n\n0.873\n\n\n1.083\n\n\n1.356\n\n\n1.782\n\n\n2.179\n\n\n2.303\n\n\n2.681\n\n\n3.055\n\n\n3.428\n\n\n3.930\n\n\n4.318\n\n\n\n\n13\n\n\n0.694\n\n\n0.870\n\n\n1.079\n\n\n1.350\n\n\n1.771\n\n\n2.160\n\n\n2.282\n\n\n2.650\n\n\n3.012\n\n\n3.372\n\n\n3.852\n\n\n4.221\n\n\n\n\n14\n\n\n0.692\n\n\n0.868\n\n\n1.076\n\n\n1.345\n\n\n1.761\n\n\n2.145\n\n\n2.264\n\n\n2.624\n\n\n2.977\n\n\n3.326\n\n\n3.787\n\n\n4.140\n\n\n\n\n15\n\n\n0.691\n\n\n0.866\n\n\n1.074\n\n\n1.341\n\n\n1.753\n\n\n2.131\n\n\n2.249\n\n\n2.602\n\n\n2.947\n\n\n3.286\n\n\n3.733\n\n\n4.073\n\n\n\n\n16\n\n\n0.690\n\n\n0.865\n\n\n1.071\n\n\n1.337\n\n\n1.746\n\n\n2.120\n\n\n2.235\n\n\n2.583\n\n\n2.921\n\n\n3.252\n\n\n3.686\n\n\n4.015\n\n\n\n\n17\n\n\n0.689\n\n\n0.863\n\n\n1.069\n\n\n1.333\n\n\n1.740\n\n\n2.110\n\n\n2.224\n\n\n2.567\n\n\n2.898\n\n\n3.222\n\n\n3.646\n\n\n3.965\n\n\n\n\n18\n\n\n0.688\n\n\n0.862\n\n\n1.067\n\n\n1.330\n\n\n1.734\n\n\n2.101\n\n\n2.214\n\n\n2.552\n\n\n2.878\n\n\n3.197\n\n\n3.610\n\n\n3.922\n\n\n\n\n19\n\n\n0.688\n\n\n0.861\n\n\n1.066\n\n\n1.328\n\n\n1.729\n\n\n2.093\n\n\n2.205\n\n\n2.539\n\n\n2.861\n\n\n3.174\n\n\n3.579\n\n\n3.883\n\n\n\n\n20\n\n\n0.687\n\n\n0.860\n\n\n1.064\n\n\n1.325\n\n\n1.725\n\n\n2.086\n\n\n2.197\n\n\n2.528\n\n\n2.845\n\n\n3.153\n\n\n3.552\n\n\n3.850\n\n\n\n\n21\n\n\n0.686\n\n\n0.859\n\n\n1.063\n\n\n1.323\n\n\n1.721\n\n\n2.080\n\n\n2.189\n\n\n2.518\n\n\n2.831\n\n\n3.135\n\n\n3.527\n\n\n3.819\n\n\n\n\n22\n\n\n0.686\n\n\n0.858\n\n\n1.061\n\n\n1.321\n\n\n1.717\n\n\n2.074\n\n\n2.183\n\n\n2.508\n\n\n2.819\n\n\n3.119\n\n\n3.505\n\n\n3.792\n\n\n\n\n23\n\n\n0.685\n\n\n0.858\n\n\n1.060\n\n\n1.319\n\n\n1.714\n\n\n2.069\n\n\n2.177\n\n\n2.500\n\n\n2.807\n\n\n3.104\n\n\n3.485\n\n\n3.768\n\n\n\n\n24\n\n\n0.685\n\n\n0.857\n\n\n1.059\n\n\n1.318\n\n\n1.711\n\n\n2.064\n\n\n2.172\n\n\n2.492\n\n\n2.797\n\n\n3.091\n\n\n3.467\n\n\n3.745\n\n\n\n\n25\n\n\n0.684\n\n\n0.856\n\n\n1.058\n\n\n1.316\n\n\n1.708\n\n\n2.060\n\n\n2.167\n\n\n2.485\n\n\n2.787\n\n\n3.078\n\n\n3.450\n\n\n3.725\n\n\n\n\n26\n\n\n0.684\n\n\n0.856\n\n\n1.058\n\n\n1.315\n\n\n1.706\n\n\n2.056\n\n\n2.162\n\n\n2.479\n\n\n2.779\n\n\n3.067\n\n\n3.435\n\n\n3.707\n\n\n\n\n27\n\n\n0.684\n\n\n0.855\n\n\n1.057\n\n\n1.314\n\n\n1.703\n\n\n2.052\n\n\n2.158\n\n\n2.473\n\n\n2.771\n\n\n3.057\n\n\n3.421\n\n\n3.690\n\n\n\n\n28\n\n\n0.683\n\n\n0.855\n\n\n1.056\n\n\n1.313\n\n\n1.701\n\n\n2.048\n\n\n2.154\n\n\n2.467\n\n\n2.763\n\n\n3.047\n\n\n3.408\n\n\n3.674\n\n\n\n\n29\n\n\n0.683\n\n\n0.854\n\n\n1.055\n\n\n1.311\n\n\n1.699\n\n\n2.045\n\n\n2.150\n\n\n2.462\n\n\n2.756\n\n\n3.038\n\n\n3.396\n\n\n3.659\n\n\n\n\n30\n\n\n0.683\n\n\n0.854\n\n\n1.055\n\n\n1.310\n\n\n1.697\n\n\n2.042\n\n\n2.147\n\n\n2.457\n\n\n2.750\n\n\n3.030\n\n\n3.385\n\n\n3.646\n\n\n\n\n40\n\n\n0.681\n\n\n0.851\n\n\n1.050\n\n\n1.303\n\n\n1.684\n\n\n2.021\n\n\n2.123\n\n\n2.423\n\n\n2.704\n\n\n2.971\n\n\n3.307\n\n\n3.551\n\n\n\n\n50\n\n\n0.679\n\n\n0.849\n\n\n1.047\n\n\n1.299\n\n\n1.676\n\n\n2.009\n\n\n2.109\n\n\n2.403\n\n\n2.678\n\n\n2.937\n\n\n3.261\n\n\n3.496\n\n\n\n\n60\n\n\n0.679\n\n\n0.848\n\n\n1.045\n\n\n1.296\n\n\n1.671\n\n\n2.000\n\n\n2.099\n\n\n2.390\n\n\n2.660\n\n\n2.915\n\n\n3.232\n\n\n3.460\n\n\n\n\n80\n\n\n0.678\n\n\n0.846\n\n\n1.043\n\n\n1.292\n\n\n1.664\n\n\n1.990\n\n\n2.088\n\n\n2.374\n\n\n2.639\n\n\n2.887\n\n\n3.195\n\n\n3.416\n\n\n\n\n100\n\n\n0.677\n\n\n0.845\n\n\n1.042\n\n\n1.290\n\n\n1.660\n\n\n1.984\n\n\n2.081\n\n\n2.364\n\n\n2.626\n\n\n2.871\n\n\n3.174\n\n\n3.390\n\n\n\n\n1000\n\n\n0.675\n\n\n0.842\n\n\n1.037\n\n\n1.282\n\n\n1.646\n\n\n1.962\n\n\n2.056\n\n\n2.330\n\n\n2.581\n\n\n2.813\n\n\n3.098\n\n\n3.300\n\n\n\n\n\\(\\infty\\)\n\n\n0.674\n\n\n0.842\n\n\n1.036\n\n\n1.282\n\n\n1.645\n\n\n1.960\n\n\n2.054\n\n\n2.326\n\n\n2.576\n\n\n2.807\n\n\n3.090\n\n\n3.291\n\n\n\n\n\n\n50%\n\n\n60%\n\n\n70%\n\n\n80%\n\n\n90%\n\n\n95%\n\n\n96%\n\n\n98%\n\n\n99%\n\n\n99.50%\n\n\n99.8\n\n\n99.99%\n\n\n\n\nConfidence Level \\(C\\)"
  },
  {
    "objectID": "formula-sheet.html#table-c",
    "href": "formula-sheet.html#table-c",
    "title": "Appendix A — AP Exam Formula Sheet",
    "section": "A.6 Table C: \\(\\chi^2\\) critical values",
    "text": "A.6 Table C: \\(\\chi^2\\) critical values\n\n\n\n\n\n\n\nClick here for table of \\(\\chi^2\\) distribution critical values\n\nTable entry for \\(p\\) is the point (\\(\\chi^2\\)) with probability \\(p\\) lying above it.\n\n\n\n\n\n\ndf\n\n\nTail Probability \\(p\\)\n\n\n\n\n0.25\n\n\n0.20\n\n\n0.15\n\n\n0.10\n\n\n0.05\n\n\n0.025\n\n\n0.02\n\n\n0.01\n\n\n0.005\n\n\n0.0025\n\n\n0.001\n\n\n0.0005\n\n\n\n\n\n\n1\n\n\n1.32\n\n\n1.64\n\n\n2.07\n\n\n2.71\n\n\n3.84\n\n\n5.02\n\n\n5.41\n\n\n6.63\n\n\n7.88\n\n\n9.14\n\n\n10.83\n\n\n12.12\n\n\n\n\n2\n\n\n2.77\n\n\n3.22\n\n\n3.79\n\n\n4.61\n\n\n5.99\n\n\n7.38\n\n\n7.82\n\n\n9.21\n\n\n10.60\n\n\n11.98\n\n\n13.82\n\n\n15.20\n\n\n\n\n3\n\n\n4.11\n\n\n4.64\n\n\n5.32\n\n\n6.25\n\n\n7.81\n\n\n9.35\n\n\n9.84\n\n\n11.34\n\n\n12.84\n\n\n14.32\n\n\n16.27\n\n\n17.73\n\n\n\n\n4\n\n\n5.39\n\n\n5.99\n\n\n6.74\n\n\n7.78\n\n\n9.49\n\n\n11.14\n\n\n11.67\n\n\n13.28\n\n\n14.86\n\n\n16.42\n\n\n18.47\n\n\n20.00\n\n\n\n\n5\n\n\n6.63\n\n\n7.29\n\n\n8.12\n\n\n9.24\n\n\n11.07\n\n\n12.83\n\n\n13.39\n\n\n15.09\n\n\n16.75\n\n\n18.39\n\n\n20.52\n\n\n22.11\n\n\n\n\n6\n\n\n7.84\n\n\n8.56\n\n\n9.45\n\n\n10.64\n\n\n12.59\n\n\n14.45\n\n\n15.03\n\n\n16.81\n\n\n18.55\n\n\n20.25\n\n\n22.46\n\n\n24.10\n\n\n\n\n7\n\n\n9.04\n\n\n9.80\n\n\n10.75\n\n\n12.02\n\n\n14.07\n\n\n16.01\n\n\n16.62\n\n\n18.48\n\n\n20.28\n\n\n22.04\n\n\n24.32\n\n\n26.02\n\n\n\n\n8\n\n\n10.22\n\n\n11.03\n\n\n12.03\n\n\n13.36\n\n\n15.51\n\n\n17.53\n\n\n18.17\n\n\n20.09\n\n\n21.95\n\n\n23.77\n\n\n26.12\n\n\n27.87\n\n\n\n\n9\n\n\n11.39\n\n\n12.24\n\n\n13.29\n\n\n14.68\n\n\n16.92\n\n\n19.02\n\n\n19.68\n\n\n21.67\n\n\n23.59\n\n\n25.46\n\n\n27.88\n\n\n29.67\n\n\n\n\n10\n\n\n12.55\n\n\n13.44\n\n\n14.53\n\n\n15.99\n\n\n18.31\n\n\n20.48\n\n\n21.16\n\n\n23.21\n\n\n25.19\n\n\n27.11\n\n\n29.59\n\n\n31.42\n\n\n\n\n11\n\n\n13.70\n\n\n14.63\n\n\n15.77\n\n\n17.28\n\n\n19.68\n\n\n21.92\n\n\n22.62\n\n\n24.72\n\n\n26.76\n\n\n28.73\n\n\n31.26\n\n\n33.14\n\n\n\n\n12\n\n\n14.85\n\n\n15.81\n\n\n16.99\n\n\n18.55\n\n\n21.03\n\n\n23.34\n\n\n24.05\n\n\n26.22\n\n\n28.30\n\n\n30.32\n\n\n32.91\n\n\n34.82\n\n\n\n\n13\n\n\n15.98\n\n\n16.98\n\n\n18.20\n\n\n19.81\n\n\n22.36\n\n\n24.74\n\n\n25.47\n\n\n27.69\n\n\n29.82\n\n\n31.88\n\n\n34.53\n\n\n36.48\n\n\n\n\n14\n\n\n17.12\n\n\n18.15\n\n\n19.41\n\n\n21.06\n\n\n23.68\n\n\n26.12\n\n\n26.87\n\n\n29.14\n\n\n31.32\n\n\n33.43\n\n\n36.12\n\n\n38.11\n\n\n\n\n15\n\n\n18.25\n\n\n19.31\n\n\n20.60\n\n\n22.31\n\n\n25.00\n\n\n27.49\n\n\n28.26\n\n\n30.58\n\n\n32.80\n\n\n34.95\n\n\n37.70\n\n\n39.72\n\n\n\n\n16\n\n\n19.37\n\n\n20.47\n\n\n21.79\n\n\n23.54\n\n\n26.30\n\n\n28.85\n\n\n29.63\n\n\n32.00\n\n\n34.27\n\n\n36.46\n\n\n39.25\n\n\n41.31\n\n\n\n\n17\n\n\n20.49\n\n\n21.61\n\n\n22.98\n\n\n24.77\n\n\n27.59\n\n\n30.19\n\n\n31.00\n\n\n33.41\n\n\n35.72\n\n\n37.95\n\n\n40.79\n\n\n42.88\n\n\n\n\n18\n\n\n21.60\n\n\n22.76\n\n\n24.16\n\n\n25.99\n\n\n28.87\n\n\n31.53\n\n\n32.35\n\n\n34.81\n\n\n37.16\n\n\n39.42\n\n\n42.31\n\n\n44.43\n\n\n\n\n19\n\n\n22.72\n\n\n23.90\n\n\n25.33\n\n\n27.20\n\n\n30.14\n\n\n32.85\n\n\n33.69\n\n\n36.19\n\n\n38.58\n\n\n40.88\n\n\n43.82\n\n\n45.97\n\n\n\n\n20\n\n\n23.83\n\n\n25.04\n\n\n26.50\n\n\n28.41\n\n\n31.41\n\n\n34.17\n\n\n35.02\n\n\n37.57\n\n\n40.00\n\n\n42.34\n\n\n45.31\n\n\n47.50\n\n\n\n\n21\n\n\n24.93\n\n\n26.17\n\n\n27.66\n\n\n29.62\n\n\n32.67\n\n\n35.48\n\n\n36.34\n\n\n38.93\n\n\n41.40\n\n\n43.78\n\n\n46.80\n\n\n49.01\n\n\n\n\n22\n\n\n26.04\n\n\n27.30\n\n\n28.82\n\n\n30.81\n\n\n33.92\n\n\n36.78\n\n\n37.66\n\n\n40.29\n\n\n42.80\n\n\n45.20\n\n\n48.27\n\n\n50.51\n\n\n\n\n23\n\n\n27.14\n\n\n28.43\n\n\n29.98\n\n\n32.01\n\n\n35.17\n\n\n38.08\n\n\n38.97\n\n\n41.64\n\n\n44.18\n\n\n46.62\n\n\n49.73\n\n\n52.00\n\n\n\n\n24\n\n\n28.24\n\n\n29.55\n\n\n31.13\n\n\n33.20\n\n\n36.42\n\n\n39.36\n\n\n40.27\n\n\n42.98\n\n\n45.56\n\n\n48.03\n\n\n51.18\n\n\n53.48\n\n\n\n\n25\n\n\n29.34\n\n\n30.68\n\n\n32.28\n\n\n34.38\n\n\n37.65\n\n\n40.65\n\n\n41.57\n\n\n44.31\n\n\n46.93\n\n\n49.44\n\n\n52.62\n\n\n54.95\n\n\n\n\n26\n\n\n30.43\n\n\n31.79\n\n\n33.43\n\n\n35.56\n\n\n38.89\n\n\n41.92\n\n\n42.86\n\n\n45.64\n\n\n48.29\n\n\n50.83\n\n\n54.05\n\n\n56.41\n\n\n\n\n27\n\n\n31.53\n\n\n32.91\n\n\n34.57\n\n\n36.74\n\n\n40.11\n\n\n43.19\n\n\n44.14\n\n\n46.96\n\n\n49.64\n\n\n52.22\n\n\n55.48\n\n\n57.86\n\n\n\n\n28\n\n\n32.62\n\n\n34.03\n\n\n35.71\n\n\n37.92\n\n\n41.34\n\n\n44.46\n\n\n45.42\n\n\n48.28\n\n\n50.99\n\n\n53.59\n\n\n56.89\n\n\n59.30\n\n\n\n\n29\n\n\n33.71\n\n\n35.14\n\n\n36.85\n\n\n39.09\n\n\n42.56\n\n\n45.72\n\n\n46.69\n\n\n49.59\n\n\n52.34\n\n\n54.97\n\n\n58.30\n\n\n60.73\n\n\n\n\n30\n\n\n34.80\n\n\n36.25\n\n\n37.99\n\n\n40.26\n\n\n43.77\n\n\n46.98\n\n\n47.96\n\n\n50.89\n\n\n53.67\n\n\n56.33\n\n\n59.70\n\n\n62.16\n\n\n\n\n40\n\n\n45.62\n\n\n47.27\n\n\n49.24\n\n\n51.81\n\n\n55.76\n\n\n59.34\n\n\n60.44\n\n\n63.69\n\n\n66.77\n\n\n69.70\n\n\n73.40\n\n\n76.09\n\n\n\n\n50\n\n\n56.33\n\n\n58.16\n\n\n60.35\n\n\n63.17\n\n\n67.50\n\n\n71.42\n\n\n72.61\n\n\n76.15\n\n\n79.49\n\n\n82.66\n\n\n86.66\n\n\n89.56\n\n\n\n\n60\n\n\n66.98\n\n\n68.97\n\n\n71.34\n\n\n74.40\n\n\n79.08\n\n\n83.30\n\n\n84.58\n\n\n88.38\n\n\n91.95\n\n\n95.34\n\n\n99.61\n\n\n102.69\n\n\n\n\n80\n\n\n88.13\n\n\n90.41\n\n\n93.11\n\n\n96.58\n\n\n101.9\n\n\n106.6\n\n\n108.1\n\n\n112.3\n\n\n116.3\n\n\n120.1\n\n\n124.8\n\n\n128.3\n\n\n\n\n100\n\n\n109.1\n\n\n111.7\n\n\n114.7\n\n\n118.5\n\n\n124.3\n\n\n129.6\n\n\n131.1\n\n\n135.8\n\n\n140.2\n\n\n144.3\n\n\n149.4\n\n\n153.2"
  },
  {
    "objectID": "statistic-to-pvalue.html#z-statisticsscore",
    "href": "statistic-to-pvalue.html#z-statisticsscore",
    "title": "Appendix B — From Value to Probability",
    "section": "B.1 z statistics/score",
    "text": "B.1 z statistics/score\nIf you are given a z-score of \\(z\\) and the range that you want a corresponding area (probability) that you want to find, there are normally two situations, one where you want the area to the left of \\(z\\) (i.e. \\(P(Z &lt; z)\\)) and the area to the right of \\(z\\) (i.e. \\(P(Z &gt; z)\\)).\n\nB.1.1 \\(P(Z &lt; z)\\)\nIf you want the area to the left of \\(z\\), you look up the z-score in Table A by finding the corresponding whole number and tenths place on the left margin and the corresponding hundreths place on the top margin of the table then right down the number that you find.\nFor example, \\(P(Z &lt; -1.01) = 0.1562\\)\nYou can also use the calculator and do normalcdf(-1000, -1.01), where -1000 represents \\(- \\infty\\) since you want the area from \\(- \\infty\\) to -1.01. The calculator needs the lower bound and upper bound of the area that you want to find.\n\n\nB.1.2 \\(P(Z &gt; z)\\)\nIf you want the area to the right of \\(z\\), you first look up the z-score in Table A by finding the corresponding whole number and tenths place on the left margin and the corresponding hundreths place on the top margin of the table. Take note of the probability that you found, then take the complement of that probability by subtracting from 1.\nFor example, \\(P(Z &gt; 1.01) = 1 - P(Z &lt; 1.01) = 1 - 0.8438 = 0.1562\\)\nThis is an application of the [complement rule][Complement Rule], where \\(P(Z &gt; z)^C = P(Z &lt; z)\\) and thus \\(P(Z &gt; z) = 1 - P(Z &lt; z)\\).\nYou can also use the calculator and do normalcdf(1.01, 1000), where 1000 represents \\(\\infty\\) since you want the area from 1.01 to \\(\\infty\\).\n\n\nB.1.3 \\(P(z_1 &lt; Z &lt; z_2)\\)\nSay you want the area between two z-scores/statistics, \\(z_1\\) and \\(z_2\\), then you do the same thing by looking up the z-scores on the tables as described above and subtract the probability that you find for \\(z_2\\) from the one that you find for \\(z_1\\).\nFor example, \\[P(-1.01 &lt; Z &lt; 1.01) = P(Z &lt; 1.01) - P(Z &lt; -1.01) = 0.8438 - 0.1562 = 0.6876\\]\nOn the calculator, you can do normalcdf(-1.01, 1.01)"
  },
  {
    "objectID": "statistic-to-pvalue.html#t-statisticsscore",
    "href": "statistic-to-pvalue.html#t-statisticsscore",
    "title": "Appendix B — From Value to Probability",
    "section": "B.2 t statistics/score",
    "text": "B.2 t statistics/score\nIf you are given a t-statistic of \\(t\\) and the range that you want a corresponding area (probability) that you want to find, there are normally two situations, one where you want the area to the left of \\(t\\) (\\(P(t_{n-1} &lt; t)\\)) and the area to the right of \\(t\\) (\\(P(t_{n-1} &gt; t)\\)). Where \\(t_{n-1}\\) represents the t-distribution with degrees of freedom \\(df = n-1\\). For all of these, calculate the degrees of freedom first.\nTo do this, take a look at Table B.\nTable B only provides the right tail of corresponding to \\(|t|\\).\nTable B’s left margin describes the degrees of freedom that you have. Since we know that the t-distribution is conservative, if your degrees of freedom is not on the margin, round your degrees of freedom down to the nearest one.\nThe top margin describes the probability that corresponds to the right of \\(|t|\\).\nThe table itself contains the various exact \\(t\\) statistics for the corresponding probability and degrees of freedom.\nWith this context, since we know that the t-distribution is symmetrical, all we need to do is look up our value of \\(|t|\\) in the middle of Table B and write down the corresponding probability\n\nB.2.1 Examples\n\n\\(P(t_{29} &lt; -1.2)\\)\n\nLook up the \\(|t| = |-1.2| = 1.2\\) since \\(P(t_{29} &lt; -1.2) = P(t_{29} &gt; 1.2)\\)\nAt the row with \\(df = 29\\), notice that \\(1.055 &lt; 1.2 &lt; 1.311\\)\nThese values correspond to probability of \\(0.15\\) and \\(0.10\\) respectively.\nThis means that \\(0.10 &lt; P(t_{29} &lt; -1.2) &lt; 0.15\\)\nThis is the only answer that we know from the table, the probability is between 0.10 and 0.15.\nAlternatively, use the calculator command tcdf(-1000, -1.2, 29), where we put the lower bound, upper bound, and the degrees of freedom respectively.\n\n\\(P(t_{57} &gt; 1.2)\\)\n\nRound your degrees of freedom down to 50, since it’s the next lowest one. \\(df = 57 \\sim 50\\).\nAt the row with \\(df = 50\\), notice that \\(1.047 &lt; 1.2 &lt; 1.299\\)\nThese values correspond to probability of \\(0.15\\) and \\(0.10\\) respectively.\nThis means that \\(0.10 &lt; P(t_{57} &gt; 1.2) &lt; 0.15\\)\nThis is the only answer that we know from the table, the probability is between 0.10 and 0.15.\nAlternatively, use the calculator command tcdf(1.2, 1000, 57), where we put the lower bound, upper bound, and the degrees of freedom respectively."
  },
  {
    "objectID": "critical-values.html#zast",
    "href": "critical-values.html#zast",
    "title": "Appendix C — Critical Values",
    "section": "C.1 \\(z^\\ast\\)",
    "text": "C.1 \\(z^\\ast\\)\nWhen constructing confidence intervals, we need the value \\(z^\\ast\\) such that \\(C\\)% of the estimated sampling distribution is between \\(-|z^\\ast|\\) and \\(|z^\\ast|\\). In other words, imagine:\n\n\n\n\n\nSo if we want to \\(z\\) that corresponds to the confidence level, we have to determine the area of the left/right tail that corresponds to the confidence level. We know that if there’s \\(C\\)% in the middle, then there’s \\(100\\% - C\\) left in the two tails. For one tail, that’ll be \\(\\frac{100\\% - C}{2}\\).\nLook up that proportion in Table A in the middle of the table, take the larger value if it’s not on there specifically, and back track to the margins to get your desired value of \\(z^\\ast\\).\nThe sign of any critical value does not matter, so just take the absolute value as your critical value.\n\nC.1.1 Examples\n\n90% confidence\n\n\nFind that the area that we want is: \\(\\frac{100\\% - 90%}{2} = 5\\% = 0.05\\)\nTry to find 0.05 in the middle of Table A and you notice that it is between 0.0505 and 0.0495.\nTake the larger value, 0.0505 and look at the margins for the \\(z\\) that corresponds to it, -1.64.\nJust take the positive value, \\(z^\\ast = 1.64\\).\n\nAlternatively, do the calculator command invNorm(0.05) to get a value of around -1.64. Take the positive value as your critical value, \\(z^\\ast = 1.64\\).\n\n99% confidence\n\n\nFind that the area that we want is: \\(\\frac{100\\% - 99%}{2} = 0.5\\% = 0.005\\)\nTry to find 0.005 in the middle of Table A and you notice that it is between 0.0051 and 0.0049.\nTake the larger value, 0.0051 and look at the margins for the \\(z\\) that corresponds to it, -2.57.\nJust take the positive value, \\(z^\\ast = 2.57\\).\n\nAlternatively, do the calculator command invNorm(0.005) to get a value of around -2.5758. Take the positive value as your critical value, \\(z^\\ast = 2.58\\)."
  },
  {
    "objectID": "critical-values.html#tast",
    "href": "critical-values.html#tast",
    "title": "Appendix C — Critical Values",
    "section": "C.2 \\(t^\\ast\\)",
    "text": "C.2 \\(t^\\ast\\)\nThe same principles for \\(z^\\ast\\) applies to \\(t^\\ast\\), except we have to use degrees of freedom too.\nTable B conveniently has a confidence level margin at the bottom of the table, which corresponds to the tail probability that’s given in the top margin of the table.\nThe left margin is where you should look for the degrees of freedom, calculated by \\(df = n-1\\).\nFind the critical value by: looking up the degrees of freedom and the confidence level in the left margin and bottom margin respectively. Round the degrees of freedom down. The critical value is the number that you pinpoint in the middle of the table.\n\n\nWhy round down?\n\nBy rounding the degrees of freedom down, you use a more conservative estimate.\nThe main reason that we are even using the \\(t\\)-distribution is because it is more conservative than the Normal distribution.\nBy rounding the degrees of freedom down, you use a more conservative \\(t\\)-distribution (more area in the tails), so you ensure that you do not underestimate your answer, rather, you purposely overestimate your answer.\n\n\nC.2.1 Examples\n\n90% confidence and 24 degrees of freedom\n\n\nLook at the \\(df = 24\\) row and the confidence level 90% column.\nTake the value in the middle of the table that you pinpoint from that row and column, \\(t^\\ast = 1.711\\)\n\n\n99% confidence and 96 degrees of freedom\n\n\nRound your df down to the next available value. \\(df = 96 \\sim 80\\).\nLook at the \\(df = 80\\) row and the confidence level 99% column.\nTake the value in the middle of the table that you pinpoint from that row and column, \\(t^\\ast = 2.639\\)"
  }
]