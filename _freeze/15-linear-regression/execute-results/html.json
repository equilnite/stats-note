{
  "hash": "4d1f89365d7a91c4e5ab6d10bb32cadc",
  "result": {
    "markdown": "# Linear Regression\n\n## Scatter plots\n\nA **scatter plot** is a graph that shows the relationship between two quantitative variables measured on the same individuals.  The values of one variable appear on the horizontal axis, and the values of the other variable appear on the vertical axis. Each individual in the data is represented as a point on the graph. We want to know if there is any relationship between the two quantitative variables. In other words, as the x-value increases, does the y-value tend to increase, decrease, or stay the same? Or do the two variables appear to be related at all?  Are there any outliers?  A scatter plot allows us to answer these questions visually.  In a scatter plot, the y-axis is always the response variable (which measures an outcome of a study) the x-axis is always the e**X**planatory variable (which may help explain or influence changes in the response variable).  \n\n::: {.highlight}\nWe can describe the relationships between two quantitative variables through their scatter plot's:\n\na. Strength: The degree to which the points follow a pattern\n\nb. Form: Linear or non-linear\n\nc. Direction: Positive or negative association\n\nd. Outliers: Individual observations that fall outside the overall pattern\n:::\n\n<details>\n<summary> \nMake sure to remember these small details when describing your scatterplots.\n</summary>\n\n- You should remember to describe your strength and direction of the relationships through the form of the relationship.\n    * Likewise, describe your outliers through the form of the relationship.\n\n- Make sure to include the context of the problem when you are describing the relationship.\n\n</details>\n\n### An example of a scatter plot\n\nHere's an example of a scatter plot, with the iris data set of just the *setosa* species:\n\n\n::: {.cell hash='15-linear-regression_cache/html/unnamed-chunk-1_9822491f2a219e5e4ec965dae5095a85'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Sepal.Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sepal.Width\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Length\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Width\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species\"],\"name\":[5],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"5.1\",\"2\":\"3.5\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.9\",\"2\":\"3.0\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.6\",\"2\":\"3.1\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.6\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"3\":\"1.7\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"4.6\",\"2\":\"3.4\",\"3\":\"1.4\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.4\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.4\",\"2\":\"2.9\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.9\",\"2\":\"3.1\",\"3\":\"1.5\",\"4\":\"0.1\",\"5\":\"setosa\"},{\"1\":\"5.4\",\"2\":\"3.7\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.8\",\"2\":\"3.4\",\"3\":\"1.6\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.8\",\"2\":\"3.0\",\"3\":\"1.4\",\"4\":\"0.1\",\"5\":\"setosa\"},{\"1\":\"4.3\",\"2\":\"3.0\",\"3\":\"1.1\",\"4\":\"0.1\",\"5\":\"setosa\"},{\"1\":\"5.8\",\"2\":\"4.0\",\"3\":\"1.2\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.7\",\"2\":\"4.4\",\"3\":\"1.5\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"3\":\"1.3\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.5\",\"3\":\"1.4\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"5.7\",\"2\":\"3.8\",\"3\":\"1.7\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.8\",\"3\":\"1.5\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"5.4\",\"2\":\"3.4\",\"3\":\"1.7\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.7\",\"3\":\"1.5\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"4.6\",\"2\":\"3.6\",\"3\":\"1.0\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.3\",\"3\":\"1.7\",\"4\":\"0.5\",\"5\":\"setosa\"},{\"1\":\"4.8\",\"2\":\"3.4\",\"3\":\"1.9\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.0\",\"3\":\"1.6\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.4\",\"3\":\"1.6\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"5.2\",\"2\":\"3.5\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.2\",\"2\":\"3.4\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"3\":\"1.6\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.8\",\"2\":\"3.1\",\"3\":\"1.6\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.4\",\"2\":\"3.4\",\"3\":\"1.5\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"5.2\",\"2\":\"4.1\",\"3\":\"1.5\",\"4\":\"0.1\",\"5\":\"setosa\"},{\"1\":\"5.5\",\"2\":\"4.2\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.9\",\"2\":\"3.1\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.2\",\"3\":\"1.2\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.5\",\"2\":\"3.5\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.9\",\"2\":\"3.6\",\"3\":\"1.4\",\"4\":\"0.1\",\"5\":\"setosa\"},{\"1\":\"4.4\",\"2\":\"3.0\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.4\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.5\",\"3\":\"1.3\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"4.5\",\"2\":\"2.3\",\"3\":\"1.3\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"4.4\",\"2\":\"3.2\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.5\",\"3\":\"1.6\",\"4\":\"0.6\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.8\",\"3\":\"1.9\",\"4\":\"0.4\",\"5\":\"setosa\"},{\"1\":\"4.8\",\"2\":\"3.0\",\"3\":\"1.4\",\"4\":\"0.3\",\"5\":\"setosa\"},{\"1\":\"5.1\",\"2\":\"3.8\",\"3\":\"1.6\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"4.6\",\"2\":\"3.2\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.3\",\"2\":\"3.7\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\"},{\"1\":\"5.0\",\"2\":\"3.3\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nLet's say that we are trying to see if sepal length and sepal width have a relationship, specifically, if sepal length can explain the differences in sepal width between irises. This makes our explanatory variable the sepal length and the response variable the sepal width.\n\n\n::: {.cell hash='15-linear-regression_cache/html/iris scatter plot_6d9df8ad5b35a715aa688a3c17d33526'}\n::: {.cell-output-display}\n![](15-linear-regression_files/figure-html/iris scatter plot-1.png){width=672}\n:::\n:::\n\n\nBased off this plot, I would say that there is a \nmoderately, positive, linear relationship between sepal width and sepal length. The point (4.5, 2.3) might be an outlier. \n\n\n## Least Squares Regression Model {#LSRL-model}\n\nSuppose that we have a response variable $y$, and an explanatory variable $x$. If the relationship between the two variables is linear, then the relationship can be modeled by the following model:\n\n$$y = \\alpha + \\beta x + \\epsilon$$\n\nWhere \n\n- $y$ is the value of the response variable for a given value of the explanatory variable $x$ and some underlying \"noise\" $\\epsilon$.\n\n- $\\alpha$ is the y-intercept, the value of $y$ when $x=0$\n\n- $\\beta$ is the slope, and \n\n- $\\epsilon$ is the underlying \"noise\"/error that we can expect out of the relationship.\n\n::: {.highlight}\n\nAn estimate of this model would then have the following form:\n\n$$\\hat y = a + bx$$\n\nWhere\n\n- $\\hat y$ is the predicted value of the response variable $y$ for a given value of the explanatory variable $x$\n\n- $a$ is a statistic that estimates $\\alpha$, the y-intercept, and provides the value of $\\hat y$ when $x=0$.\n\n- $b$ is a statistic that estimates $\\beta$, the slope.\n\n:::\n\n::: {.highlight}\n\nWe interpret $a$ and $b$ as:\n\nSample sentence frame for a **Y-INTERCEPT**\n: When the [*explanatory variable*] is zero, our model predicts the [*response variable*] would be [*y-intercept*].\n\n\nSample sentence frame for a **SLOPE**:\n: For every 1 [*unit*] increase in [*explanatory variable*], our model predicts an average change of [slope] in [*response variable*]\n:::\n\n### Correlation\n\nA scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. Linear relationships are important because a straight line is a simple pattern that is quite common. Unfortunately, our eyes are not good judges of how strong a linear relationship is.\nThe correlation $r$ measures the direction and strength of the linear relationship between two quantitative variables.\n\n$$r = \\frac{1}{n-1}\\sum \\left( \\frac{x_i - \\bar x}{s_x}\\right) \\left( \\frac{y_i -\\bar y}{s_y}\\right)$$\n\n::: {.highlight}\n\n- $r$ is always a number between -1 and 1\n\n- $r > 0$ indicates a positive association.\n\n- $r < 0$ indicates a negative association.\n\n- Values of $r$ near 0 indicate a very weak linear relationship.\n\n- The strength of the linear relationship increases as $r$ moves away from 0 towards -1 or 1.\n\n- The extreme values $r = -1$ and $r = 1$ occur only in the case of a perfect linear relationship.\n\n- Correlation makes no distinction between explanatory and response variables.\n\n- $r$ does not change when we change the units of measurement of $x$, $y$ or both.\n\n- The correlation $r$ itself has no unit of measurement.\n\n- CORRELATION DOES NOT IMPLY CAUSATION!!!\n\n- Correlation requires that both variables be quantitative. \n\n- Correlation does not describe curved relationships between variables.\n \n- A value of $r$ close to -1 or 1 does not guarantee a linear relationship between them.\n\n- Correlation is not resistant: $r$ is strongly affected by a few outlying observations\n\n- What would be the effect of removing the outlier\n on the correlation?\n \n- Correlation is not a complete summary of two-variable data. \n:::\n\nThe correlation $r$ only informs us how well the data can conform to a line (the strength of a linear relationship), and the direction of of the relationship. Otherwise, depending on $r$ is hard unless we look at a couple of other things, such as a [residual plot](#residual-plots).\n\nTo interpret values of $r$, follow this sentence frame:\n\n::: {.highlight}\n\nThere is a [*strength*], [*direction*] relationship between the [*explanatory variable*] and the [*response variable*]\n\n:::\n\n### Residuals\n\nIn most cases, no line will pass through all of the points in a scatter plot. Because we use the line to predict $y$ from $x$, the prediction errors we make are errors in $y$, the vertical direction in the scatter plot. A regression line using our least squares method minimizes the squared residuals.\n\n::: {.highlight}\n\nA **residual** is the difference between an observed value of the response variable and the value predicted by the regression line. That is,  \n$$r_i= \\text{residual for the ith value}=\\text{observed } i^{th} \\text{ y value}-\\text{predicted } i^{th} \\text{ y value}=y_i-\\hat y_i$$\n\nInterpreting RESIDUALS\n:The actual [**response variable**] for [**specific value of the explanatory variable**], is [**greater than/less than**] the predicted [response variable] by [**residual value**].\n:::\n\nAll the equations that we learn in this chapter are based off the fact that a least squares regression line minimizes the *squared* vertical deviations of the points from the line. In this course, you do not have to know the underlying proofs for the formulas.\n\n#### Residual Plots {#residual-plots}\n\nWe can plot the residuals versus the explanatory variable to obtain insight into the appropriateness of our regression model:\n\n::: {.highlight}\nLook for:\n\nCurved pattern\n: if there’s a curved pattern in our residuals, that means there’s a curved pattern in our observations that we didn’t notice before, probably because of the scale of our scatterplot.  A curved pattern means that a linear model (like an LSR line) is probably not appropriate to describe our data.  We should try to use a non-linear model (we’ll look at these in Chapter 4).\n\nHeteroskedasticity\n: In other words, is there a pattern in our plot? If the residuals are getting larger at one or both ends of the data, that means our model is not predicting as well in those areas (remember that a big residual = a big error in prediction) and shouldn’t be used to make predictions for those values of our explanatory (x) variable.  \n:::\n\n#### Standard Deviation of Residuals {#residual-sd}\n\nTo assess how well the line fits all the data, we need to consider the residuals for each observation, not just one. Using these residuals, we can estimate the “typical” prediction error when using the least-squares regression line.\n\n::: {.highlight}\nIf we use a least-squares regression line to predict the values of a response variable $y$ from an explanatory variable $x$, the standard deviation of the residuals ($s$) is given by\n\n$$s = \\sqrt{\\frac{\\sum (residuals ^2)}{n-2}}=\\sqrt{\\frac{\\sum (y_i - \\hat y_i)^2)}{n - 2}}$$\n\nThis value gives the approximate size of a “typical” prediction error (residual).\n\n\n**Sample Sentence Frame**\n\nWhen using the LSRL with [*explanatory variable*] to predict [*response variable*], we will typically be off by about [*the value of $s$ with units of the response variable ($y$)*].\n:::\n\n#### Coefficient of Determination\n\nWhen we do the least squares model, we minimize the squares of the residuals, but what can we compare this to? We compare it to another line (a \"baseline\" model), that always predicts the mean of $y$ for any value of $x$. In other words, we compare it to $\\hat y = \\bar y$. We do this since this baseline model practically means that are are saying that there is no relationship between $x$ and $y$. On the other hand, in the least squares model, we are trying to say that there is a relationship.\n\nThe coefficient of determination, $r^2$ is defined by the \\% reduction in squared error that using a least squares model has in comparison to the baseline model. \n\n$$r^2 = \\frac{\\text{size of reduction}}{\\text{original amount}} = \\frac{SST - SSE}{SST} = 1 -\\frac{SSE}{SST} = 1 - \\frac{\\sum (y_i-\\hat y)^2}{\\sum (y_i - \\bar y)^2}$$\n\n### Other definitions\n\nSometimes you’re just given a summary of the data instead of every single point.  In this case, you can still find a (the y-intercept) and b (the slope) of the LSR line, and write the equation.  To do this you need to know the following:\n\n::: {.highlight}\n\nRemembering the basic notation from [least squares regression model](#LSRL-model),\n\n$$b=r\\frac{s_y}{s_x}$$\n\nThe LSR line always passes through the mean of both the $x$ and $y$ variables, $$(\\bar x, \\bar y)$$. So when you apply slope-point formula and the using $b$ as calculated above, we can find that:\n\n$$a=\\bar y-b\\bar x$$\n\n:::\n\n## Doing it in your calculator:\n\n1. Enter your $x$ and $y$ into two lists in your calculator (access it by `STAT > EDIT`).\n\n2. Go to `STAT > CALC > LinReg(a+bx)`. The usage for this will be: `LinReg(a+bx) explanatory list, response list, desired equation destination`\n    - An example would be: `LinReg(a+bx) L1, L2, Y1`\n    - You assess the function variables by going to: `VARS > Y-VARS > Function` and you can choose any one that you want (normally `Y1`)\n    \n3. Enter the command.\n    - You'll see the value for `a`, `b`, `r2`, and `r`\n    - You'll also be able to observe a few other things\n        * Since you entered a destination for your equation, your equation will be saved (go to `Y=` and look at your equations there)\n        * The residuals for each observation will also be calculated, which you can see in the `RESID` list (go to `2nd > stat (list)` and scroll down to see it)\n    \nPlot the scatterplot (and regression line):\n\n1. Go to `2nd > Y= (stat plot)` and choose any of the plots.\n\n2. Choose `On` and the scatter plot (first option)\n\n3. Specify the list for the `Xlist` and `Ylist`\n\n4. Go to `Y=` and make sure the `=` for the regression equation is on (highlighted)\n\n5. Do `zoom > ZoomStat`. This ensures everything is in your graphing window and takes you to the graph.\n\nPlot the residual plot:\n\nRepeat the previous steps, except you don't need to plot your regression equation, you can can un-highlight it (just go the `=` and hit `enter`). Change the `Ylist` to `RESID` in your `stat plot` plot so that you are plotting the residuals against the x-values.\n\n\n{{< video https://www.youtube.com/embed/u1t8dtca7UU >}}\n\n\n\n## Miscellaneous Things to Remember\n\n1. *The distinction between explanatory and response variables is important in regression*. Least-squares regression makes the distances of the data points from the line small only in the y direction. If we reverse the roles of the two variables, we get a different least-squares regression line. This isn’t true for correlation: switching x and y doesn’t affect the value of r.\n\n2. *Correlation and regression lines describe only linear relationships*. You can calculate the correlation and least-squares line for any relationship between two quantitative variables, but the results are useful only if the scatterplot shows a linear pattern. ALWAYS PLOT YOUR DATA.\n    - [Here's an interesting thing to look at](https://blog.revolutionanalytics.com/2017/05/the-datasaurus-dozen.html)\n    \n3. *Correlation and least-squares regression lines are not resistant*.\n\n![A scatterplot of Gesell Adaptive Scores versus the age at the first word for 21 children. \n](graphics/influ-obs.png)\n\nRemember, an outlier is an observation that lies outside the overall pattern of other observations.  In bivariate (2-variable) data, an outlier can either be an outlier in the x-direction, the y-direction, both, or neither. \n\n\n\n::: {.highlight}\nAn influential observation in statistics is a point that has a large effect on a statistical calculation. In other words, an observation is considered influential if removing it would markedly change the regression line.\n:::\n\n<details>\n<summary>\nWhich appears to have the greatest influence on the position of the regression line?\n</summary>\n\nChild 18 is the most influential observation here. Notice how removing (and adding) child 18 caused the greatest change in the regression line. Child 19 is a close one, but Child 18 has the most influence. \n\nWe could say that Child 18 is an outlier in the x-direction and y-direction meanwhile child 19  is an outlier in the y-direction\n</details>\n\n\n<details>\n<summary>\nWill influential observations always have large residuals?  Why/why not?  Try to find some examples on your own first.\n</summary>\n\nNo, take the previous example for instance. Child 18 has an extremely small residual in the plot.\n\nSo, influential observations actually often have small residuals. This is because they have \"influence\" on the line; they \"pull\" the regression line towards them. So if the regression line is closer to them, then they have a small residual.\n</details>\n\n4. *Correlation does not imply causation*. When we study the relationship between two variables, we often hope to show that changes in the explanatory variable cause changes in the response variable. *A strong association between two variables is not enough to draw conclusions about cause and effect.* [We've learned how to draw cause and effect relationships and the scope of inference.](#experimental-design)\n\n<details>\n<summary>\nConsider the study that found people with two cars live longer than people who only own one car. Owning three cars is even better, and so on. There is a substantial positive association between the number of cars x and the length of life y. The basic meaning of causation is that by changing x, we can bring about a change in y. Could we lengthen our lives by buying more cars?\n</summary>\n\nNo. This does not even make sense to happen. What does make sense is looking between the lines and determining that there are other factors at play. A common variable here that would confound this relationship (confounding variable) would be the wealth of individuals. By having more wealth, you have more cars and are likely to live longer. Since this factor is in play, it's likely the real reason why we see this positive relationship between number of cars and length of life.\n</details>\n\n[See this website for more examples on weird correlations.](https://tylervigen.com/)\n\n## Interpreting Computer Output {#basic-linreg-output}\n\nYou are expected to know how to interpret computer out in MINITAB format (just what you normally see).\n\n[See the next section for the rest of the interpretation](#more-linreg-output)\n\n### Example \n\nBelow is the computer output from a basic regression analysis for the Ford-F-150 data relating the mileage and value of Ford F-150s. \n\n```\nPredictor     \tCoef  \t    SECoef      T      P\nConstant  \t\t38257\t\t2446  \t    15.64  0.000\nMiles Driven    -0.16292    0.03096   \t-5.26  0.000\n\nS = 5740.13   R-Sq = 66.4%   R-Sq(adj) = 64.0%\n```\n\nLet's first identify a few basic facts, when we have a LSRL, the line is of form $$\\hat y = a + b x$$\n\nFrom our basic knowledge of algebra, $b$ is the coefficient of $x$ in the equation and the only things that we really need values (estimates) for are $a$ and $b$. So the two rows of interest for now are looking at the `Predictor` and the `Coef` of the `Predictor`. \n\n<details>\n<summary> What is a predictor? </summary>\nWhat variable are we using to \"predict\"? Obviously, we are trying to predict $y$ based off $x$, so $x$ is our \"predictor\" and the constant term $a$ in the equation is actually included here too, since it's involved in our prediction.\n</details>\n\nWe can identify that our explanatory variable is mileage (`Miles Driven`) and our response variable is the value of the Ford F-150.\n\nSo what we can figure out from these math facts here are that each value correspond to the following:\n\n```\nPredictor     \tCoef  \t    SECoef      T      P\nConstant  \t\ta\t\t    2446  \t    15.64  0.000\nMiles Driven    b           0.03096   \t-5.26  0.000\n\nS = s          R-Sq = r^2     R-Sq(adj) = 64.0%\n```\n\n::: {.highlight}\nTo reiterate, we can figure out that for a LSRL with this data and knowing our explanatory variable is mileage (`Miles Driven`) and our response variable is the value of the Ford F-150,\n\n$$\n\\begin{aligned}\n&a = 38257\\\\\n&b = -0.16292\\\\\n&s = 5740.13\\\\\n&r^2 = .664\n\\end{aligned}\n$$\n:::\n\nThus our LSRL is $$\\hat{\\text{value of F-150}} = 38257 - 0.16292 (\\text{miles driven})$$\n\nWith $s = 5740.13$ and $r^2 = .664$. \n\n\n::: {.highlight}\nWe can also infer from this output that since $b$ is negative (negative slope, so negative relationship), that $$r = -\\sqrt{r^2} \\approx -\\sqrt{.664} \\approx -0.8149 $$\n:::\n\nOther parts are irrelevant to this chapter.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}